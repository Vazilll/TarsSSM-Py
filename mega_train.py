"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  mega_train.py â€” Ğ¢ĞĞ Ğ¡ v3: ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞĞ´Ğ¸Ğ½ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ’Ğ¡Ğ:

  Ğ¤Ğ°Ğ·Ğ° 0:  Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ (pip install)
  Ğ¤Ğ°Ğ·Ğ° 1:  Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Wikipedia + HuggingFace + LEANN)
  Ğ¤Ğ°Ğ·Ğ° 2:  ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¾Ğ² (MinGRU classifier, ~1 Ğ¼Ğ¸Ğ½)
  Ğ¤Ğ°Ğ·Ğ° 3:  ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MinGRU LM (System 1, ~30 Ğ¼Ğ¸Ğ½ GPU)
  Ğ¤Ğ°Ğ·Ğ° 4:  ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Mamba-2 (12 ÑĞ»Ğ¾Ñ‘Ğ² Ã— 768d, 4 Ñ„Ğ°Ğ·Ñ‹, ~2-4Ñ‡ GPU)
  Ğ¤Ğ°Ğ·Ğ° 5:  ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 1.58-bit + Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (~30 Ğ¼Ğ¸Ğ½)
  Ğ¤Ğ°Ğ·Ğ° 6:  Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ±Ğ¾Ñ€ĞºĞ° Ğ² models/tars_v3/
  Ğ¤Ğ°Ğ·Ğ° 7:  Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ (Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ)
  Ğ¤Ğ°Ğ·Ğ° 8:  Whisper Tiny LoRA â€” Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ STT Ğ´Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ (~2Ñ‡ GPU)
  Ğ¤Ğ°Ğ·Ğ° 9:  Piper TTS â€” Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ´Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ (~5Ñ‡ GPU)
  Ğ¤Ğ°Ğ·Ğ° 10: ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… ONNX-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (INT8, ~5 Ğ¼Ğ¸Ğ½)

ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ´Ğ»Ñ: Kaggle P100 (16GB) / Colab A100 / RTX 4090

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
  python mega_train.py              # ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ (~15Ñ‡)
  python mega_train.py --skip-download  # Ğ‘ĞµĞ· ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞµÑÑ‚ÑŒ)
  python mega_train.py --phase 4    # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Mamba-2
  python mega_train.py --phase 8    # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Whisper fine-tune
  python mega_train.py --quick      # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ (Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ)
  python mega_train.py --skip-voice # Ğ‘ĞµĞ· Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… Ñ„Ğ°Ğ· (8-10)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import re
import time
import json
import shutil
import subprocess
import logging
import argparse
from pathlib import Path
from datetime import datetime

# â•â•â• ĞŸÑƒÑ‚Ğ¸ â•â•â•
ROOT = Path(__file__).resolve().parent
TRAINING = ROOT / "training"
DATA = ROOT / "data"
MODELS = ROOT / "models"
TARS_V3 = MODELS / "tars_v3"
PYTHON = sys.executable

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(ROOT / "mega_train.log", encoding="utf-8"),
    ]
)
logger = logging.getLogger("MegaTrain")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ£Ğ¢Ğ˜Ğ›Ğ˜Ğ¢Ğ«
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def banner(phase: int, title: str, total: int = 10):
    """ĞŸĞµÑ‡Ğ°Ñ‚Ğ°ĞµÑ‚ Ğ±Ğ°Ğ½Ğ½ĞµÑ€ Ñ„Ğ°Ğ·Ñ‹."""
    logger.info("")
    logger.info("â•”" + "â•" * 62 + "â•—")
    logger.info(f"â•‘  Ğ¤Ğ°Ğ·Ğ° {phase}/{total}: {title:<50s}  â•‘")
    logger.info("â•š" + "â•" * 62 + "â•")
    logger.info("")


def run(cmd: list, cwd=None, retries=3, check=True) -> bool:
    """Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ Ñ retry Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ (Windows Defender / launcher bugs)."""
    cmd_str = " ".join(str(c) for c in cmd)
    logger.info(f"â–¶ {cmd_str}")
    
    for attempt in range(retries):
        try:
            result = subprocess.run(
                [str(c) for c in cmd],
                cwd=str(cwd or ROOT),
                timeout=7200,  # 2 Ñ‡Ğ°ÑĞ° Ğ¼Ğ°ĞºÑ
            )
            if result.returncode == 0:
                return True
            if result.returncode == 101 and attempt < retries - 1:
                logger.warning(f"  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ»Ğ°ÑƒĞ½Ñ‡ĞµÑ€Ğ° (ĞºĞ¾Ğ´ 101), Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ {attempt+2}/{retries}...")
                time.sleep(3)
                continue
            if check:
                logger.error(f"  âŒ ĞšĞ¾Ğ´ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ°: {result.returncode}")
            return False
        except PermissionError:
            if attempt < retries - 1:
                logger.warning(f"  PermissionError, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ {attempt+2}/{retries}...")
                time.sleep(5)
            else:
                logger.error("  âŒ PermissionError Ğ¿Ğ¾ÑĞ»Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
                return False
        except subprocess.TimeoutExpired:
            logger.error("  âŒ Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ (2 Ñ‡Ğ°ÑĞ°)")
            return False
    return False


def gpu_info():
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ GPU."""
    # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° 1: import torch Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ
    try:
        import torch
        if torch.cuda.is_available():
            name = torch.cuda.get_device_name(0)
            vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
            return name, vram
    except Exception as e:
        logger.debug(f"  gpu_info torch attempt failed: {e}")
    
    # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° 2: Ñ‡ĞµÑ€ĞµĞ· subprocess (PYTHON Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ venv)
    try:
        code = (
            "import torch; "
            "print(torch.cuda.is_available()); "
            "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else ''); "
            "print(torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0)"
        )
        r = subprocess.run([PYTHON, "-c", code], capture_output=True, text=True, timeout=30)
        if r.returncode == 0:
            lines = r.stdout.strip().split('\n')
            if len(lines) >= 3 and lines[0].strip() == "True":
                name = lines[1].strip()
                vram = float(lines[2].strip())
                return name, vram
        elif r.stderr:
            logger.debug(f"  gpu_info subprocess stderr: {r.stderr[:200]}")
    except Exception as e:
        logger.debug(f"  gpu_info subprocess failed: {e}")
    
    # ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° 3: Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· nvidia-smi (Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ PyTorch)
    try:
        r = subprocess.run(
            ["nvidia-smi", "--query-gpu=name,memory.total", "--format=csv,noheader,nounits"],
            capture_output=True, text=True, timeout=10
        )
        if r.returncode == 0 and r.stdout.strip():
            parts = r.stdout.strip().split(", ")
            if len(parts) >= 2:
                name = parts[0].strip()
                vram = float(parts[1].strip()) / 1024  # MiB â†’ GiB
                return name, vram
    except Exception as e:
        logger.debug(f"  gpu_info nvidia-smi failed: {e}")
    
    return None, 0


def get_ram_gb():
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼ RAM Ğ² GB."""
    try:
        import psutil
        return psutil.virtual_memory().total / 1024**3
    except ImportError:
        # Fallback Ğ´Ğ»Ñ Windows
        try:
            import ctypes
            kernel32 = ctypes.windll.kernel32
            class MEMORYSTATUSEX(ctypes.Structure):
                _fields_ = [("dwLength", ctypes.c_ulong),
                           ("dwMemoryLoad", ctypes.c_ulong),
                           ("ullTotalPhys", ctypes.c_ulonglong),
                           ("ullAvailPhys", ctypes.c_ulonglong),
                           ("ullTotalPageFile", ctypes.c_ulonglong),
                           ("ullAvailPageFile", ctypes.c_ulonglong),
                           ("ullTotalVirtual", ctypes.c_ulonglong),
                           ("ullAvailVirtual", ctypes.c_ulonglong)]
            stat = MEMORYSTATUSEX()
            stat.dwLength = ctypes.sizeof(stat)
            kernel32.GlobalMemoryStatusEx(ctypes.byref(stat))
            return stat.ullTotalPhys / 1024**3
        except Exception:
            return 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 0: Ğ£Ğ¡Ğ¢ĞĞĞĞ’ĞšĞ Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ•Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _detect_cuda_version():
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ĞµÑ€ÑĞ¸Ñ CUDA Ñ‡ĞµÑ€ĞµĞ· nvidia-smi."""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=driver_version", "--format=csv,noheader"],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            result2 = subprocess.run(
                ["nvidia-smi"], capture_output=True, text=True, timeout=10
            )
            output = result2.stdout

            match = re.search(r'CUDA Version:\s*(\d+)\.(\d+)', output)
            if match:
                major = int(match.group(1))
                minor = int(match.group(2))
                # PyTorch wheels: cu124 (latest), cu121, cu118
                # CUDA 13.x Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ cu124
                if major >= 13 or (major == 12 and minor >= 4):
                    return "cu124"
                elif major == 12:
                    return "cu121"
                elif major >= 11:
                    return "cu118"
            return "cu124"  # default Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ñ€Ğ°Ğ¹Ğ²ĞµÑ€Ğ¾Ğ²
    except Exception:
        pass
    return None


def _install_torch_cuda(pip_cmd, cuda_tag, extra_flags=None):
    """Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ PyTorch Ñ CUDA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹."""
    extra = extra_flags or []
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ â€” Ğ¼Ğ¾Ğ¶ĞµÑ‚ torch ÑƒĞ¶Ğµ Ñ CUDA?
    try:
        import torch
        if torch.cuda.is_available():
            logger.info(f"  âœ… torch â€” ÑƒĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ (CUDA: True)")
            return
    except ImportError:
        pass
    
    if cuda_tag:
        logger.info(f"  ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° PyTorch Ñ CUDA ({cuda_tag})...")
        torch_url = f"https://download.pytorch.org/whl/{cuda_tag}"
        run(pip_cmd + ["install", "torch", "--index-url", torch_url, "--quiet", "--force-reinstall"] + extra, check=False)
    else:
        logger.info("  ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° PyTorch (CPU)...")
        run(pip_cmd + ["install", "torch", "--quiet"] + extra, check=False)


def phase_0_install():
    """Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹."""
    banner(0, "Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹")
    
    global PYTHON
    
    cuda_tag = _detect_cuda_version()
    if cuda_tag:
        logger.info(f"  ğŸ® NVIDIA GPU Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½, CUDA tag: {cuda_tag}")
    
    # â•â•â• ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ venv (PEP 668 fix) â•â•â•
    venv_dir = ROOT / "venv"
    if sys.platform == "win32":
        venv_python = venv_dir / "Scripts" / "python.exe"
        venv_pip = venv_dir / "Scripts" / "pip.exe"
    else:
        venv_python = venv_dir / "bin" / "python"
        venv_pip = venv_dir / "bin" / "pip"
    
    use_venv = False
    if not venv_python.exists():
        logger.info("  ğŸ”§ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (venv)...")
        try:
            subprocess.run([sys.executable, "-m", "venv", str(venv_dir)], check=True)
            logger.info(f"  âœ… venv ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {venv_dir}")
            use_venv = True
        except (subprocess.CalledProcessError, Exception) as e:
            logger.warning(f"  âš  venv Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ: {e}")
            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ÑĞ»Ğ¾Ğ¼Ğ°Ğ½Ğ½Ñ‹Ğ¹ venv ĞµÑĞ»Ğ¸ Ğ¾Ğ½ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ»ÑÑ
            if venv_dir.exists():
                shutil.rmtree(str(venv_dir), ignore_errors=True)
                logger.info("  ğŸ—‘ Ğ¡Ğ»Ğ¾Ğ¼Ğ°Ğ½Ğ½Ñ‹Ğ¹ venv ÑƒĞ´Ğ°Ğ»Ñ‘Ğ½")
            logger.info("  Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ pip + --break-system-packages")
    elif venv_pip.exists():
        use_venv = True
    else:
        # venv_python ĞµÑÑ‚ÑŒ, Ğ½Ğ¾ pip Ğ½ĞµÑ‚ â€” ÑĞ»Ğ¾Ğ¼Ğ°Ğ½Ğ½Ñ‹Ğ¹ venv
        logger.warning("  âš  venv ÑĞ»Ğ¾Ğ¼Ğ°Ğ½ (Ğ½ĞµÑ‚ pip), ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼...")
        shutil.rmtree(str(venv_dir), ignore_errors=True)
        logger.info("  Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ pip + --break-system-packages")
    
    if use_venv:
        PYTHON = str(venv_python)
        pip_cmd = [str(venv_pip)]
        extra_flags = []
    else:
        PYTHON = sys.executable
        pip_cmd = [PYTHON, "-m", "pip"]
        extra_flags = ["--break-system-packages"]
    
    logger.info(f"  ğŸ Python: {PYTHON}")
    
    # â•â•â• Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° PyTorch (Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾, Ñ CUDA) â•â•â•
    _install_torch_cuda(pip_cmd, cuda_tag, extra_flags)
    
    # ĞÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹
    packages = [
        # â•â•â• Core (Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ·Ğ³Ğ°) â•â•â•
        "numpy", "einops", "tqdm",
        "sentencepiece", "tokenizers",
        "sentence-transformers",
        "datasets",
        "psutil",
        # â•â•â• Voice (Ñ„Ğ°Ğ·Ñ‹ 8-10) â•â•â•
        "transformers",          # Whisper model
        "peft",                  # LoRA adapters
        "jiwer",                 # WER Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ´Ğ»Ñ Whisper
        "onnxruntime",           # INT8 ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ONNX
        "faster-whisper",        # STT runtime
        "sounddevice",           # Ğ—Ğ°Ñ…Ğ²Ğ°Ñ‚ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½Ğ°
        # â•â•â• Hub (API ÑĞµÑ€Ğ²ĞµÑ€) â•â•â•
        "fastapi",               # REST API
        "uvicorn",               # ASGI ÑĞµÑ€Ğ²ĞµÑ€
        "websockets",            # WebSocket support
    ]
    
    logger.info("ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ²...")
    
    for pkg in packages:
        check_cmd = [PYTHON, "-c", f"import {pkg.replace('-', '_')}"]
        try:
            result_ok = subprocess.run(check_cmd, capture_output=True, timeout=10).returncode == 0
        except Exception:
            result_ok = False
        
        if result_ok:
            logger.info(f"  âœ… {pkg} â€” ÑƒĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        else:
            logger.info(f"  ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° {pkg}...")
            run(pip_cmd + ["install", pkg, "--quiet"] + extra_flags, check=False)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° CUDA
    gpu_name, vram = gpu_info()
    ram = get_ram_gb()
    
    logger.info("")
    logger.info(f"  ğŸ–¥ï¸  GPU: {gpu_name or 'ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½'}")
    if vram > 0:
        logger.info(f"  ğŸ’¾ VRAM: {vram:.1f} GB")
    logger.info(f"  ğŸ§  RAM: {ram:.0f} GB")
    
    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 1: Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ• Ğ”ĞĞĞĞ«Ğ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_1_download(quick: bool = False):
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."""
    banner(1, "Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    
    success = True
    # 1.1 Wikipedia
    wiki_path = DATA / "wiki_ru.txt"
    if quick:
        logger.info("  ğŸ“š Wikipedia: Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº (quick mode, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ)")
    elif wiki_path.exists() and wiki_path.stat().st_size > 1_000_000:
        wiki_mb = wiki_path.stat().st_size / 1024 / 1024
        logger.info(f"  ğŸ“š Wikipedia: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({wiki_mb:.1f} MB)")
    else:
        logger.info("  ğŸ“š Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Wikipedia (100000 ÑÑ‚Ğ°Ñ‚ĞµĞ¹)...")
        if not run([PYTHON, TRAINING / "download_wiki.py", "--count", "100000"], check=False):
            logger.warning("  âš  Wikipedia Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ° â€” Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼")
            success = False
    
    # 1.2 HuggingFace Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹
    if quick:
        logger.info("  ğŸ¤— HuggingFace: Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº (quick mode)")
    else:
        hf_files = list(DATA.glob("hf_*.txt"))
        if len(hf_files) >= 3:
            total_mb = sum(f.stat().st_size for f in hf_files) / 1024 / 1024
            logger.info(f"  ğŸ¤— HuggingFace: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({len(hf_files)} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ², {total_mb:.0f} MB)")
        else:
            logger.info("  ğŸ¤— Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ HuggingFace Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² (ĞºĞ¾Ğ´ + Ñ‡Ğ°Ñ‚ + Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹)...")
            if not run([PYTHON, TRAINING / "download_hf_dataset.py", "--preset", "all"], check=False):
                logger.warning("  âš  HuggingFace Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½ â€” Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼")
                success = False
    
    # 1.3 LEANN embedding model
    emb_path = MODELS / "embeddings"
    if emb_path.exists():
        logger.info(f"  ğŸ§  LEANN embeddings: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({emb_path})")
    else:
        logger.info("  ğŸ§  Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² (all-MiniLM-L6-v2)...")
        try:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('all-MiniLM-L6-v2')
            model.save(str(emb_path))
            logger.info(f"  âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ² {emb_path}")
        except Exception as e:
            logger.warning(f"  âš  Embeddings: {e}")
    
    # 1.4 Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LEANN Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
    logger.info("  ğŸ§  Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² LEANN...")
    try:
        sys.path.insert(0, str(TRAINING))
        from ingest_to_leann import ingest_all
        ingest_all()
        logger.info("  âœ… LEANN Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ°")
    except Exception as e:
        logger.info(f"  â„¹ LEANN: {e} (Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾)")
    
    # 1.5 Ğ“Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Whisper CTranslate2 + Piper ONNX + Silero VAD)
    voice_dir = MODELS / "voice"
    voice_dir.mkdir(parents=True, exist_ok=True)
    
    # 1.5.1 faster-whisper CTranslate2 (Ğ´Ğ»Ñ runtime STT)
    whisper_ct2 = voice_dir / "whisper_tiny"
    if (whisper_ct2 / "model.bin").exists():
        logger.info(f"  ğŸ™ Whisper CTranslate2: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({whisper_ct2})")
    else:
        logger.info("  ğŸ™ Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Whisper Tiny (CTranslate2) Ğ´Ğ»Ñ STT...")
        try:
            from faster_whisper import WhisperModel
            _m = WhisperModel("tiny", device="cpu", compute_type="int8",
                              download_root=str(voice_dir))
            # faster-whisper ĞºĞµÑˆĞ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² Ğ½ÑƒĞ¶Ğ½Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾
            import huggingface_hub
            cached = huggingface_hub.snapshot_download("guillaumekln/faster-whisper-tiny")
            if not (whisper_ct2 / "model.bin").exists():
                shutil.copytree(cached, str(whisper_ct2), dirs_exist_ok=True)
            del _m
            logger.info(f"  âœ… Whisper Tiny CTranslate2 ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {whisper_ct2}")
        except Exception as e:
            logger.warning(f"  âš  Whisper CTranslate2: {e}")
    
    # 1.5.2 Piper TTS ONNX (Ñ€ÑƒÑÑĞºĞ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ñ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸)
    piper_models = [
        ("ru_RU-irina-medium.onnx",
         "https://huggingface.co/rhasspy/piper-voices/resolve/main/ru/ru_RU/irina/medium/ru_RU-irina-medium.onnx"),
        ("ru_RU-irina-medium.onnx.json",
         "https://huggingface.co/rhasspy/piper-voices/resolve/main/ru/ru_RU/irina/medium/ru_RU-irina-medium.onnx.json"),
    ]
    for fname, url in piper_models:
        dst = voice_dir / fname
        if dst.exists():
            size_mb = dst.stat().st_size / 1024 / 1024
            logger.info(f"  ğŸ—£ Piper {fname}: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({size_mb:.1f} MB)")
        else:
            logger.info(f"  ğŸ—£ Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Piper {fname}...")
            try:
                import urllib.request
                urllib.request.urlretrieve(url, str(dst))
                size_mb = dst.stat().st_size / 1024 / 1024
                logger.info(f"  âœ… {fname}: {size_mb:.1f} MB")
            except Exception as e:
                logger.warning(f"  âš  Piper {fname}: {e}")
    
    # 1.5.3 Silero VAD ONNX (Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°)
    vad_path = voice_dir / "silero_vad.onnx"
    if vad_path.exists():
        logger.info(f"  ğŸ‘‚ Silero VAD: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ")
    else:
        logger.info("  ğŸ‘‚ Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Silero VAD...")
        try:
            vad_url = "https://github.com/snakers4/silero-vad/raw/master/src/silero_vad/data/silero_vad.onnx"
            import urllib.request
            urllib.request.urlretrieve(vad_url, str(vad_path))
            size_mb = vad_path.stat().st_size / 1024 / 1024
            logger.info(f"  âœ… Silero VAD: {size_mb:.1f} MB")
        except Exception as e:
            logger.warning(f"  âš  Silero VAD: {e}")
    
    # Ğ¡Ğ²Ğ¾Ğ´ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    total_data = 0
    for f in DATA.glob("*.txt"):
        total_data += f.stat().st_size
    for f in DATA.glob("*.json"):
        total_data += f.stat().st_size
    
    logger.info(f"\n  ğŸ“Š Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {total_data / 1024 / 1024:.0f} MB")
    return success


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 2: Ğ Ğ•Ğ¤Ğ›Ğ•ĞšĞ¡Ğ« (TIER 1)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_2_reflex(quick: bool = False):
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°."""
    banner(2, "Ğ ĞµÑ„Ğ»ĞµĞºÑÑ‹ (MinGRU Classifier)")
    
    epochs = "10" if quick else "100"
    return run([PYTHON, TRAINING / "train_reflex.py",
        "--epochs", epochs,
        "--lr", "0.002",
    ])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 3: MinGRU LANGUAGE MODEL (TIER 1 / System 1)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_3_mingru(device: str, quick: bool = False):
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MinGRU LM â€” Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€."""
    banner(3, "MinGRU Language Model (System 1)")
    
    if quick:
        logger.info("  âš¡ Quick mode: dim=256, layers=4, 3 ÑĞ¿Ğ¾Ñ…Ğ¸")
        return run([PYTHON, TRAINING / "train_mingru.py",
            "--epochs", "3",
            "--lr", "3e-3",
            "--dim", "256",
            "--layers", "4",
            "--batch", "16",
            "--seq_len", "128",
        ])
    
    return run([PYTHON, TRAINING / "train_mingru.py",
        "--epochs", "25",           # 25 ÑĞ¿Ğ¾Ñ… (Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ½Ğ° GPU)
        "--lr", "3e-3",
        "--dim", "512",             # ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ
        "--layers", "6",            # 6 ÑĞ»Ğ¾Ñ‘Ğ² MinGRU
        "--batch", "32",            # Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ±Ğ°Ñ‚Ñ‡ (64GB RAM)
        "--seq_len", "256",
        "--augment",                # + HuggingFace Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    ])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 4: MAMBA-2 BRAIN (TIER 2 / System 2) â€” ĞĞ¡ĞĞĞ’ĞĞĞ• ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_4_mamba2(device: str, resume: bool = False, quick: bool = False):
    """
    ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Mamba-2 brain â€” Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°.
    
    4 Ğ¿Ğ¾Ğ´-Ñ„Ğ°Ğ·Ñ‹:
      Phase 1: Full pretrain (Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, 3 ÑĞ¿Ğ¾Ñ…Ğ¸)
      Phase 2: Fine-tune WKV + Fusion (SSD frozen, 2 ÑĞ¿Ğ¾Ñ…Ğ¸)
      Phase 3: Fine-tune MoLE + MatrixPool (1 ÑĞ¿Ğ¾Ñ…Ğ°)
      Phase 4: Fine-tune WKV Fusion + RAG (1 ÑĞ¿Ğ¾Ñ…Ğ°)
    """
    banner(4, "Mamba-2 Brain (12Ã—768d, Full Architecture)")
    
    # â•â•â• Transfer embedding MinGRU â†’ Mamba-2 â•â•â•
    emb_path = None
    mingru_weights = MODELS / "mingru_weights.pt"
    if mingru_weights.exists():
        logger.info("ğŸ”— ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° MinGRU â†’ Mamba-2...")
        try:
            import torch
            cp = torch.load(str(mingru_weights), map_location='cpu', weights_only=False)
            state = cp.get('model_state_dict', cp)
            for k in state:
                if 'shared_embedding' in k or 'emb.weight' in k:
                    emb_tensor = state[k]
                    TARS_V3.mkdir(parents=True, exist_ok=True)
                    emb_path = str(TARS_V3 / "_transfer_embedding.pt")
                    torch.save(emb_tensor, emb_path)
                    logger.info(f"  âœ… Embedding ({emb_tensor.shape}) â†’ {emb_path}")
                    break
        except Exception as e:
            logger.warning(f"  âš  Transfer failed: {e}")
    
    # â•â•â• Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ RTX 4090 â•â•â•
    if quick:
        logger.info("  âš¡ Quick mode: d_model=256, n_layers=4, 1 ÑĞ¿Ğ¾Ñ…Ğ° Ğ½Ğ° Ñ„Ğ°Ğ·Ñƒ")
        base = [
            "--d_model", "256",
            "--n_layers", "4",
            "--vocab_size", "256",
            "--batch", "8",
            "--accum_steps", "2",
            "--device", device,
            "--curriculum",
            "--label_smoothing", "0.1",
            "--max_samples", "500",
            "--no_compile",  # T4 Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ linalg.solve Ğ² FP16 Ñ‡ĞµÑ€ĞµĞ· compile
        ]
    else:
        base = [
            "--d_model", "2048",        # TARS 1B
            "--n_layers", "24",         # 24 rich blocks
            "--vocab_size", "256",      # cp1251 Ğ±Ğ°Ğ¹Ñ‚Ñ‹
            "--batch", "4",             # 4 Ã— seq â†’ fits 16GB VRAM (bf16+ckpt)
            "--accum_steps", "16",      # Effective batch = 64
            "--device", device,
            "--curriculum",             # Curriculum learning (64â†’128â†’256)
            "--label_smoothing", "0.1",
        ]
    # â•â•â• BitMamba-2 optimizations: bf16 + grad checkpoint â•â•â•
    if device != "cpu" and not quick:
        base += ["--bf16", "--grad_ckpt"]
    
    if emb_path:
        base += ["--pretrained_emb", emb_path]
    
    results = {}
    
    # â”€â”€ Phase 1: Full pretrain (Ğ²ÑĞµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹) â”€â”€
    logger.info("â”€â”€ Phase 1/4: Full pretrain (SSD + WKV + Î©-SSM + MoLE + WaveMerge) â”€â”€")
    quick_epochs = "1"
    results["p1"] = run([PYTHON, TRAINING / "train_mamba2.py"] + base + [
        "--epochs", quick_epochs if quick else "5",  # Tars.txt Â§7.3
        "--lr", "3e-4",
        "--phase", "1",
        "--seq_len", "128" if quick else "256",
    ])
    
    # â”€â”€ Phase 2: Fine-tune WKV + Fusion (SSD frozen) â”€â”€
    logger.info("â”€â”€ Phase 2/4: Fine-tune WKV + Fusion (SSD frozen) â”€â”€")
    results["p2"] = run([PYTHON, TRAINING / "train_mamba2.py"] + base + [
        "--epochs", quick_epochs if quick else "3",  # Tars.txt Â§7.3
        "--lr", "1e-4",
        "--phase", "2",
        "--seq_len", "128" if quick else "512",
        "--resume",
    ])
    
    # â”€â”€ Phase 3: Fine-tune MoLE + MatrixPool + WaveMerge â”€â”€
    logger.info("â”€â”€ Phase 3/4: Fine-tune MoLE + MatrixPool + WaveMerge â”€â”€")
    results["p3"] = run([PYTHON, TRAINING / "train_mamba2.py"] + base + [
        "--epochs", quick_epochs if quick else "2",  # Tars.txt Â§7.3
        "--lr", "3e-5",
        "--phase", "3",
        "--seq_len", "128" if quick else "512",
        "--resume",
    ])
    
    # â”€â”€ Phase 4: Fine-tune WKV RAG State + Memory Integration â”€â”€
    logger.info("â”€â”€ Phase 4/4: Fine-tune WKV + RAG + Memory Injection â”€â”€")
    results["p4"] = run([PYTHON, TRAINING / "train_mamba2.py"] + base + [
        "--epochs", quick_epochs if quick else "2",  # Tars.txt Â§7.3
        "--lr", "1.5e-5",
        "--phase", "4",
        "--seq_len", "128" if quick else "512",
        "--resume",
    ])
    
    all_ok = all(results.values())
    if all_ok:
        logger.info("âœ… Ğ’ÑĞµ 4 Ñ„Ğ°Ğ·Ñ‹ Mamba-2 Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ñ‹")
    else:
        failed = [k for k, v in results.items() if not v]
        logger.warning(f"âš  Ğ¤Ğ°Ğ·Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸: {failed}")
    
    return all_ok


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 5: ĞšĞ’ĞĞĞ¢Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ 1.58-bit
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_5_quantize(device: str, quick: bool = False):
    """ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ FP16 â†’ 1.58-bit + Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."""
    banner(5, "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ BitNet 1.58-bit")
    
    # Ğ˜Ñ‰ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ FP16 Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    fp16_path = MODELS / "mamba2" / "mamba2_omega.pt"
    
    if not fp16_path.exists():
        logger.info("  ğŸ” FP16 Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾, ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ· HuggingFace...")
        fp16_path.parent.mkdir(parents=True, exist_ok=True)
        
        downloaded = False
        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ ÑĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ pre-trained Mamba-2 Ğ¸Ğ· state-spaces
        hf_models = [
            "state-spaces/mamba2-130m",
            "state-spaces/mamba2-370m",
            "state-spaces/mamba-130m",
        ]
        
        for hf_model in hf_models:
            try:
                logger.info(f"  ğŸ“¥ Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ {hf_model}...")
                from transformers import AutoModelForCausalLM
                import torch
                
                hf_m = AutoModelForCausalLM.from_pretrained(
                    hf_model, torch_dtype=torch.float16, trust_remote_code=True
                )
                
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾Ğ¼ Ñ TarsMamba2LM
                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¸Ğ· HF Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                hf_cfg = hf_m.config
                d_model = getattr(hf_cfg, 'd_model', getattr(hf_cfg, 'hidden_size', 768))
                n_layers = getattr(hf_cfg, 'n_layer', getattr(hf_cfg, 'num_hidden_layers', 24))
                vocab_size = getattr(hf_cfg, 'vocab_size', 50280)
                
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ TarsMamba2LM Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸
                from brain.mamba2.model import TarsMamba2LM
                tars_model = TarsMamba2LM(
                    d_model=d_model, n_layers=n_layers,
                    vocab_size=vocab_size, quant_mode="fp16",
                )
                
                # ĞŸĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ Ğ²ĞµÑĞ°
                hf_state = hf_m.state_dict()
                tars_state = tars_model.state_dict()
                loaded = 0
                for key in tars_state:
                    # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸ Ğ¸Ğ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğµ
                    for hf_key, hf_val in hf_state.items():
                        if hf_val.shape == tars_state[key].shape:
                            if key.split('.')[-1] == hf_key.split('.')[-1]:
                                tars_state[key] = hf_val
                                loaded += 1
                                break
                
                tars_model.load_state_dict(tars_state, strict=False)
                
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼
                checkpoint = {
                    "model_state_dict": tars_model.state_dict(),
                    "config": {
                        "d_model": d_model,
                        "n_layers": n_layers,
                        "vocab_size": vocab_size,
                        "quant_mode": "fp16",
                    },
                    "d_model": d_model,
                    "n_layers": n_layers,
                    "vocab_size": vocab_size,
                    "source": hf_model,
                }
                torch.save(checkpoint, str(fp16_path))
                
                size_mb = fp16_path.stat().st_size / 1024 / 1024
                logger.info(f"  âœ… {hf_model} ÑĞºĞ°Ñ‡Ğ°Ğ½ Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½: {size_mb:.1f} MB")
                logger.info(f"  ğŸ“Š d_model={d_model}, n_layers={n_layers}, vocab={vocab_size}")
                logger.info(f"  ğŸ”— ĞŸĞµÑ€ĞµĞ½ĞµÑĞµĞ½Ğ¾ {loaded} Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ¾Ğ²")
                
                del hf_m, tars_model
                downloaded = True
                break
                
            except Exception as e:
                logger.warning(f"  âš  {hf_model}: {e}")
                continue
        
        if not downloaded:
            logger.warning(f"  âš  ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Mamba-2 Ğ¸Ğ· HuggingFace")
            logger.warning("  ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ")
            return False
    
    logger.info(f"  Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {fp16_path}")
    fp16_size = fp16_path.stat().st_size / 1024 / 1024
    logger.info(f"  Ğ Ğ°Ğ·Ğ¼ĞµÑ€ FP16: {fp16_size:.1f} MB")
    
    # ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ + Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ 5 ÑĞ¿Ğ¾Ñ…
    return run([PYTHON, TRAINING / "train_mamba2.py",
        "--d_model", "768",
        "--n_layers", "12",
        "--batch", "16",
        "--accum_steps", "4",
        "--epochs", "1" if quick else "3",  # Tars.txt Â§7.5
        "--lr", "5e-5",
        "--phase", "1",
        "--quant",                  # 1.58-bit Ñ€ĞµĞ¶Ğ¸Ğ¼ (STE)
        "--resume",
        "--device", device,
        "--seq_len", "256",
        "--label_smoothing", "0.1",
    ])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 6: Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ¯ Ğ¡Ğ‘ĞĞ ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_6_consolidate(results: dict, total_time: float):
    """Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² models/tars_v3/ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ config.json."""
    banner(6, "Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ±Ğ¾Ñ€ĞºĞ°")
    
    TARS_V3.mkdir(parents=True, exist_ok=True)
    
    copies = {
        "reflex": (MODELS / "reflex" / "reflex_classifier.pt", TARS_V3 / "reflex.pt"),
        "mingru": (MODELS / "mingru_weights.pt", TARS_V3 / "mingru.pt"),
        "mamba2_fp16": (MODELS / "mamba2" / "mamba2_omega.pt", TARS_V3 / "mamba2.pt"),
        "mamba2_158bit": (MODELS / "mamba2" / "mamba2_omega_158bit.pt", TARS_V3 / "mamba2_158bit.pt"),
    }
    
    consolidated = []
    for name, (src, dst) in copies.items():
        if src.exists():
            shutil.copy2(str(src), str(dst))
            size_mb = dst.stat().st_size / 1024 / 1024
            logger.info(f"  ğŸ“¦ {name}: {src.name} â†’ tars_v3/{dst.name} ({size_mb:.1f} MB)")
            consolidated.append(name)
        else:
            logger.info(f"  â­ {name}: Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ ({src.name})")
    
    # â•â•â• Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ config.json (Ğ½ÑƒĞ¶ĞµĞ½ Ğ´Ğ»Ñ load_pretrained) â•â•â•
    # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ¸Ğ· checkpoint, Ğ¸Ğ½Ğ°Ñ‡Ğµ â€” Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚
    model_config = {
        "d_model": 768, "n_layers": 12, "vocab_size": 256,
        "d_state": 64, "headdim": 64, "omega_dim": 32,
        "pool_size": 48, "n_experts": 8,
    }
    # Ğ˜Ñ‰ĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ Ğ² .pt Ñ„Ğ°Ğ¹Ğ»Ğµ
    for pt_name in ["mamba2.pt", "mamba2_158bit.pt"]:
        pt_path = TARS_V3 / pt_name
        if pt_path.exists():
            try:
                import torch
                ckpt = torch.load(str(pt_path), map_location="cpu", weights_only=False)
                if "config" in ckpt and isinstance(ckpt["config"], dict):
                    cfg = ckpt["config"]
                    model_config.update({
                        "d_model": cfg.get("d_model", model_config["d_model"]),
                        "n_layers": cfg.get("n_layers", model_config["n_layers"]),
                        "vocab_size": cfg.get("vocab_size", model_config["vocab_size"]),
                    })
                    logger.info(f"  ğŸ“„ config Ğ¸Ğ· {pt_name}: d={model_config['d_model']}, L={model_config['n_layers']}")
                elif "d_model" in ckpt:
                    model_config["d_model"] = ckpt["d_model"]
                    model_config["n_layers"] = ckpt.get("n_layers", model_config["n_layers"])
                    model_config["vocab_size"] = ckpt.get("vocab_size", model_config["vocab_size"])
                    logger.info(f"  ğŸ“„ config Ğ¸Ğ· {pt_name}: d={model_config['d_model']}, L={model_config['n_layers']}")
                del ckpt
                break
            except Exception as e:
                logger.warning(f"  âš  ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ config Ğ¸Ğ· {pt_name}: {e}")
    
    config_json = {
        "name": "tars_v3",
        "version": "3.0",
        "encoding": "cp1251",
        "models": {
            "mamba2": {
                "params": model_config
            }
        }
    }
    config_path = TARS_V3 / "config.json"
    config_path.write_text(json.dumps(config_json, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info(f"  ğŸ“„ config.json ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {config_path}")
    
    # Ğ›Ğ¾Ğ³ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "total_time_sec": round(total_time, 1),
        "total_time_human": f"{total_time/3600:.1f} Ñ‡Ğ°ÑĞ¾Ğ²",
        "results": {k: ("ok" if v else "failed") for k, v in results.items()},
        "models_consolidated": consolidated,
        "config": model_config,
    }
    
    log_path = TARS_V3 / "training_log.json"
    logs = []
    if log_path.exists():
        try:
            logs = json.loads(log_path.read_text(encoding="utf-8"))
        except Exception:
            pass
    if not isinstance(logs, list):
        logs = []
    logs.append(log_entry)
    log_path.write_text(json.dumps(logs, ensure_ascii=False, indent=2), encoding="utf-8")
    
    logger.info(f"\n  ğŸ“‹ Training log: {log_path}")
    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 7: Ğ’ĞĞ›Ğ˜Ğ”ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_7_validate():
    """Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."""
    banner(7, "Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ")
    
    try:
        sys.path.insert(0, str(ROOT))
        import torch
        from brain.tokenizer import TarsTokenizer
        from brain.mamba2.model import TarsMamba2LM
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = TarsTokenizer()
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        model, ckpt = TarsMamba2LM.load_pretrained(device=device)
        model.eval()
        
        if ckpt is None:
            logger.warning("  âš  ĞĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ")
            return False
        
        # Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹
        test_prompts = ["Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚", "ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°", "Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ"]
        
        logger.info(f"  ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {ckpt}")
        params = sum(p.numel() for p in model.parameters())
        logger.info(f"  ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: {params:,}")
        
        for prompt in test_prompts:
            tokens = tokenizer.encode(prompt)
            input_ids = torch.tensor([tokens], dtype=torch.long, device=device)
            
            with torch.no_grad():
                logits = model(input_ids)
            
            # Ğ‘ĞµÑ€Ñ‘Ğ¼ top-5 Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°
            probs = torch.softmax(logits[0, -1, :], dim=-1)
            top5 = torch.topk(probs, 5)
            
            predictions = []
            for idx, prob in zip(top5.indices.tolist(), top5.values.tolist()):
                char = tokenizer.decode([idx])
                predictions.append(f"'{char}'({prob:.2%})")
            
            logger.info(f"  \"{prompt}\" â†’ {', '.join(predictions)}")
        
        logger.info("  âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚!")
        return True
        
    except Exception as e:
        logger.error(f"  âŒ Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ»Ğ°ÑÑŒ: {e}")
        import traceback
        traceback.print_exc()
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 8: WHISPER TINY FINE-TUNE (Ğ Ğ£Ğ¡Ğ¡ĞšĞ˜Ğ™)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_8_whisper(device: str, quick: bool = False):
    """Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Whisper Tiny Ğ´Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ (LoRA)."""
    banner(8, "Whisper Tiny LoRA (Russian STT)")

    # Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
    logger.info("  ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° peft, jiwer...")
    run([PYTHON, "-m", "pip", "install", "peft", "jiwer", "-q"], check=False)

    args_list = [
        PYTHON, TRAINING / "train_whisper.py",
        "--device", device,
    ]
    if quick:
        args_list += ["--samples", "500", "--epochs", "1", "--batch", "8"]
    else:
        args_list += ["--samples", "5000", "--epochs", "3", "--batch", "16"]

    return run(args_list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 9: PIPER TTS FINE-TUNE (Ğ Ğ£Ğ¡Ğ¡ĞšĞ˜Ğ™)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_9_piper(quick: bool = False):
    """Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Piper TTS Ğ´Ğ»Ñ Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑĞ°."""
    banner(9, "Piper TTS (Russian Voice)")

    # Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
    logger.info("  ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° piper-tts, piper-phonemize...")
    run([PYTHON, "-m", "pip", "install", "piper-tts", "piper-phonemize", "-q"], check=False)

    args_list = [
        PYTHON, TRAINING / "train_piper.py",
    ]
    if quick:
        args_list += ["--epochs", "100", "--max_samples", "200", "--batch", "8"]
    else:
        args_list += ["--epochs", "1000", "--max_samples", "3000", "--batch", "16"]

    return run(args_list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 10: ĞšĞ’ĞĞĞ¢Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ“ĞĞ›ĞĞ¡ĞĞ’Ğ«Ğ¥ ĞœĞĞ”Ğ•Ğ›Ğ•Ğ™ (INT8)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_10_quantize_voice():
    """INT8 ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Whisper ONNX + Piper ONNX."""
    banner(10, "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (INT8)")

    # Whisper Vocabulary Boost
    logger.info("  ğŸ“ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Whisper hotwords...")
    run([PYTHON, TRAINING / "whisper_boost.py"], check=False)

    # ONNX ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    return run([PYTHON, TRAINING / "quantize_voice.py"])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="Ğ¢ĞĞ Ğ¡ v3 â€” ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:
  python mega_train.py                        # ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ (~15 Ñ‡Ğ°ÑĞ¾Ğ²)
  python mega_train.py --skip-download        # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ¶Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½Ñ‹
  python mega_train.py --phase 4              # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Mamba-2
  python mega_train.py --phase 8              # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Whisper
  python mega_train.py --phase 9              # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Piper
  python mega_train.py --quick                # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ (256d, 4 ÑĞ»Ğ¾Ñ)
  python mega_train.py --skip-quantize        # Ğ‘ĞµĞ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
  python mega_train.py --skip-voice           # Ğ‘ĞµĞ· Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… Ñ„Ğ°Ğ· (8-10)
        """
    )
    
    parser.add_argument("--skip-download", action="store_true",
                        help="ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    parser.add_argument("--skip-quantize", action="store_true",
                        help="ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 1.58-bit")
    parser.add_argument("--skip-voice", action="store_true",
                        help="ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ğµ Ñ„Ğ°Ğ·Ñ‹ (Whisper, Piper, ĞºĞ²Ğ°Ğ½Ñ‚.)")
    parser.add_argument("--phase", type=int, choices=[0,1,2,3,4,5,6,7,8,9,10],
                        help="Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ñ„Ğ°Ğ·Ñƒ")
    parser.add_argument("--quick", action="store_true",
                        help="Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ (Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, 1 ÑĞ¿Ğ¾Ñ…Ğ°)")
    
    args = parser.parse_args()
    
    # â•â•â• ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¶ĞµĞ»ĞµĞ·Ğ° â•â•â•
    gpu_name, vram = gpu_info()
    ram = get_ram_gb()
    device = "cuda" if gpu_name else "cpu"
    
    logger.info("")
    logger.info("â•”" + "â•" * 62 + "â•—")
    logger.info("â•‘                                                              â•‘")
    logger.info("â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â•‘")
    logger.info("â•‘      â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•—    â•‘")
    logger.info("â•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â•‘")
    logger.info("â•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â•â•â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â• â•šâ•â•â•â–ˆâ–ˆâ•—    â•‘")
    logger.info("â•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â•‘")
    logger.info("â•‘      â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•      â•šâ•â•â•â•  â•šâ•â•â•â•â•â•     â•‘")
    logger.info("â•‘                                                              â•‘")
    logger.info("â•‘           MEGA TRAIN â€” Autonomous Pipeline                   â•‘")
    logger.info("â•‘                                                              â•‘")
    logger.info("â•š" + "â•" * 62 + "â•")
    logger.info("")
    logger.info(f"  ğŸ–¥ï¸  GPU:    {gpu_name or 'CPU only'}")
    logger.info(f"  ğŸ’¾ VRAM:   {vram:.1f} GB" if vram > 0 else "  ğŸ’¾ VRAM:   N/A")
    logger.info(f"  ğŸ§  RAM:    {ram:.0f} GB")
    logger.info(f"  ğŸ Python: {sys.version.split()[0]}")
    logger.info(f"  ğŸ“ Root:   {ROOT}")
    logger.info(f"  â° Start:  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    if args.quick:
        logger.info("  âš¡ QUICK MODE: ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (256d, 4 ÑĞ»Ğ¾Ñ, 1 ÑĞ¿Ğ¾Ñ…Ğ°)")
    
    if device == "cuda" and vram < 8:
        logger.warning("âš  VRAM < 8GB â€” Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ batch size")
    
    t0 = time.time()
    results = {}
    
    # â•â•â• Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ· â•â•â•
    phases = {
        0: ("install", lambda: phase_0_install()),
        1: ("download", lambda: phase_1_download(quick=args.quick)),
        2: ("reflex", lambda: phase_2_reflex(quick=args.quick)),
        3: ("mingru", lambda: phase_3_mingru(device, quick=args.quick)),
        4: ("mamba2", lambda: phase_4_mamba2(device, quick=args.quick)),
        5: ("quantize", lambda: phase_5_quantize(device, quick=args.quick)),
        7: ("validate", lambda: phase_7_validate()),
        8: ("whisper", lambda: phase_8_whisper(device, quick=args.quick)),
        9: ("piper", lambda: phase_9_piper(quick=args.quick)),
        10: ("voice_quant", lambda: phase_10_quantize_voice()),
    }
    
    # Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ñ„Ğ°Ğ·Ğ°
    if args.phase is not None:
        if args.phase in phases:
            name, func = phases[args.phase]
            results[name] = func()
        elif args.phase == 6:
            # ĞŸĞ¾Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ°Ğ·Ñ‹ ĞºĞ°Ğº "skipped"
            for pn in ["install", "download", "reflex", "mingru", "mamba2", "quantize", "validate"]:
                results[pn] = "skipped"
            phase_6_consolidate(results, time.time() - t0)
        else:
            logger.error(f"ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ°Ñ Ñ„Ğ°Ğ·Ğ°: {args.phase}")
        total = time.time() - t0
        logger.info(f"\n  Ğ¤Ğ°Ğ·Ğ° {args.phase} Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° Ğ·Ğ° {total:.0f}s")
        return
    
    # â•â•â• ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ â•â•â•
    
    # Ğ¤Ğ°Ğ·Ğ° 0: Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
    results["install"] = phase_0_install()
    
    # ĞŸĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ device Ğ¿Ğ¾ÑĞ»Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ torch Ñ CUDA Ğ² venv
    gpu_name, vram = gpu_info()
    device = "cuda" if gpu_name else "cpu"
    if gpu_name:
        logger.info(f"  ğŸ–¥ï¸  GPU Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½: {gpu_name} ({vram:.1f} GB VRAM)")
    
    # Ğ¤Ğ°Ğ·Ğ° 1: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ
    if not args.skip_download:
        results["download"] = phase_1_download(quick=args.quick)
    else:
        logger.info("â­ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (--skip-download)")
        results["download"] = True
    
    # Ğ¤Ğ°Ğ·Ğ° 2: Ğ ĞµÑ„Ğ»ĞµĞºÑÑ‹
    results["reflex"] = phase_2_reflex(quick=args.quick)
    
    # Ğ¤Ğ°Ğ·Ğ° 3: MinGRU
    results["mingru"] = phase_3_mingru(device, quick=args.quick)
    
    # Ğ¤Ğ°Ğ·Ğ° 4: Mamba-2 (Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ)
    results["mamba2"] = phase_4_mamba2(device, quick=args.quick)
    
    # Ğ¤Ğ°Ğ·Ğ° 5: ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    if not args.skip_quantize:
        results["quantize"] = phase_5_quantize(device, quick=args.quick)
    else:
        logger.info("â­ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (--skip-quantize)")
        results["quantize"] = True
    
    # Ğ¤Ğ°Ğ·Ğ° 6: Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° (Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ°Ñ)
    phase_6_consolidate(results, time.time() - t0)
    
    # Ğ¤Ğ°Ğ·Ğ° 7: Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ·Ğ³Ğ°
    results["validate"] = phase_7_validate()
    
    # â•â•â• Ğ“ĞĞ›ĞĞ¡ĞĞ’ĞĞ™ ĞŸĞĞ™ĞŸĞ›ĞĞ™Ğ (Ğ¤Ğ°Ğ·Ñ‹ 8-10) â•â•â•
    if not args.skip_voice:
        # Ğ¤Ğ°Ğ·Ğ° 8: Whisper STT fine-tune
        results["whisper"] = phase_8_whisper(device, quick=args.quick)
        
        # Ğ¤Ğ°Ğ·Ğ° 9: Piper TTS fine-tune
        results["piper"] = phase_9_piper(quick=args.quick)
        
        # Ğ¤Ğ°Ğ·Ğ° 10: ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… ONNX
        results["voice_quant"] = phase_10_quantize_voice()
    else:
        logger.info("â­ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… Ñ„Ğ°Ğ· (--skip-voice)")
        results["whisper"] = True
        results["piper"] = True
        results["voice_quant"] = True
    
    # â•â•â• INSTRUCTION TUNING (Ğ¤Ğ°Ğ·Ğ° 11) â•â•â•
    banner(11, "Instruction Tuning (â†’ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº)", total=11)
    try:
        from training.train_instruct import (
            load_instruction_dataset, train_instruct, download_instruct_datasets
        )
        
        # Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ datasets ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
        download_instruct_datasets()
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        instruct_texts = load_instruction_dataset()
        
        if instruct_texts and results.get("mamba2"):
            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            from brain.mamba2.model import TarsMamba2LM
            model_path = ROOT / "models" / "tars_v3" / "mamba2.pt"
            if model_path.exists():
                import torch
                state = torch.load(model_path, map_location=device, weights_only=False)
                config = state.get("config", {})
                model = TarsMamba2LM(**config).to(device)
                model.load_state_dict(state["model_state_dict"], strict=False)
                
                def tokenize_cp1251(text):
                    tokens = list(text.encode('cp1251', errors='replace')[:1024])
                    return torch.tensor([tokens], dtype=torch.long)
                
                model = train_instruct(
                    model, tokenize_cp1251, instruct_texts,
                    epochs=2 if args.quick else 3,
                    lr=5e-5, batch_size=4
                )
                
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ
                state["model_state_dict"] = model.state_dict()
                torch.save(state, model_path)
                logger.info(f"âœ… Instruction-tuned Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°: {model_path}")
                results["instruct"] = True
            else:
                logger.warning("âš  ĞœĞ¾Ğ´ĞµĞ»ÑŒ brain_mamba2.pt Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° â€” Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº instruction tuning")
                results["instruct"] = False
        else:
            logger.info("â­ ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ instruction tuning")
            results["instruct"] = len(instruct_texts) == 0
    except Exception as e:
        logger.error(f"âŒ Instruction tuning failed: {e}")
        import traceback; traceback.print_exc()
        results["instruct"] = False
    
    # â•â•â• Ğ˜Ğ¢ĞĞ“Ğ˜ â•â•â•
    total_time = time.time() - t0
    hours = total_time / 3600
    
    logger.info("")
    logger.info("â•”" + "â•" * 62 + "â•—")
    logger.info("â•‘              Ğ˜Ğ¢ĞĞ“Ğ˜ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯                                  â•‘")
    logger.info("â• " + "â•" * 62 + "â•£")
    for name, ok in results.items():
        icon = "âœ…" if ok else "âŒ"
        logger.info(f"â•‘  {icon} {name:<20s}                                    â•‘")
    logger.info("â• " + "â•" * 62 + "â•£")
    logger.info(f"â•‘  â±  Ğ’Ñ€ĞµĞ¼Ñ: {hours:.1f} Ñ‡Ğ°ÑĞ¾Ğ² ({total_time:.0f} ÑĞµĞº)                         â•‘")
    logger.info("â•š" + "â•" * 62 + "â•")
    
    if all(results.values()):
        logger.info("")
        logger.info("  ğŸ¯ Ğ’Ğ¡Ğ• Ğ¤ĞĞ—Ğ« Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ« Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞ!")
        logger.info("  ğŸ“ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ñ‹: models/tars_v3/")
        logger.info("  ğŸ¤ Ğ“Ğ¾Ğ»Ğ¾Ñ: Whisper (RU) + Piper (RU) + INT8")
        logger.info("  ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº: python launch_tars.py")
        logger.info("")
    else:
        failed = [k for k, v in results.items() if not v]
        logger.info("")
        logger.info(f"  âš  Ğ¤Ğ°Ğ·Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸: {', '.join(failed)}")
        logger.info("  ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ mega_train.log Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹")
        logger.info("")


if __name__ == "__main__":
    main()
