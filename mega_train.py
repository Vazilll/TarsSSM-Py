"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  mega_train.py â€” Ğ¢ĞĞ Ğ¡ v3: ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞĞ´Ğ¸Ğ½ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ’Ğ¡Ğ:

  Ğ¤Ğ°Ğ·Ğ° 0: Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ (pip install)
  Ğ¤Ğ°Ğ·Ğ° 1: Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Wikipedia + HuggingFace + LEANN)
  Ğ¤Ğ°Ğ·Ğ° 2: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¾Ğ² (MinGRU classifier, ~1 Ğ¼Ğ¸Ğ½)
  Ğ¤Ğ°Ğ·Ğ° 3: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MinGRU LM (System 1, ~30 Ğ¼Ğ¸Ğ½ GPU)
  Ğ¤Ğ°Ğ·Ğ° 4: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Mamba-2 (12 ÑĞ»Ğ¾Ñ‘Ğ² Ã— 768d, 4 Ñ„Ğ°Ğ·Ñ‹, ~2-4Ñ‡ GPU)
  Ğ¤Ğ°Ğ·Ğ° 5: ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 1.58-bit + Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (~30 Ğ¼Ğ¸Ğ½)
  Ğ¤Ğ°Ğ·Ğ° 6: Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ±Ğ¾Ñ€ĞºĞ° Ğ² models/tars_v3/
  Ğ¤Ğ°Ğ·Ğ° 7: Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ (Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ)

ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ´Ğ»Ñ: RTX 4090 (24GB VRAM) + 64GB RAM

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
  python mega_train.py              # ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½
  python mega_train.py --skip-download  # Ğ‘ĞµĞ· ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞµÑÑ‚ÑŒ)
  python mega_train.py --phase 4    # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Mamba-2
  python mega_train.py --quick      # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ (Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import time
import json
import shutil
import subprocess
import logging
import argparse
from pathlib import Path
from datetime import datetime

# â•â•â• ĞŸÑƒÑ‚Ğ¸ â•â•â•
ROOT = Path(__file__).resolve().parent
TRAINING = ROOT / "training"
DATA = ROOT / "data"
MODELS = ROOT / "models"
TARS_V3 = MODELS / "tars_v3"
PYTHON = sys.executable

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(ROOT / "mega_train.log", encoding="utf-8"),
    ]
)
logger = logging.getLogger("MegaTrain")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ£Ğ¢Ğ˜Ğ›Ğ˜Ğ¢Ğ«
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def banner(phase: int, title: str, total: int = 7):
    """ĞŸĞµÑ‡Ğ°Ñ‚Ğ°ĞµÑ‚ Ğ±Ğ°Ğ½Ğ½ĞµÑ€ Ñ„Ğ°Ğ·Ñ‹."""
    logger.info("")
    logger.info("â•”" + "â•" * 62 + "â•—")
    logger.info(f"â•‘  Ğ¤Ğ°Ğ·Ğ° {phase}/{total}: {title:<50s}  â•‘")
    logger.info("â•š" + "â•" * 62 + "â•")
    logger.info("")


def run(cmd: list, cwd=None, retries=3, check=True) -> bool:
    """Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ Ñ retry Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ (Windows Defender / launcher bugs)."""
    cmd_str = " ".join(str(c) for c in cmd)
    logger.info(f"â–¶ {cmd_str}")
    
    for attempt in range(retries):
        try:
            result = subprocess.run(
                [str(c) for c in cmd],
                cwd=str(cwd or ROOT),
                timeout=7200,  # 2 Ñ‡Ğ°ÑĞ° Ğ¼Ğ°ĞºÑ
            )
            if result.returncode == 0:
                return True
            if result.returncode == 101 and attempt < retries - 1:
                logger.warning(f"  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ»Ğ°ÑƒĞ½Ñ‡ĞµÑ€Ğ° (ĞºĞ¾Ğ´ 101), Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ {attempt+2}/{retries}...")
                time.sleep(3)
                continue
            if check:
                logger.error(f"  âŒ ĞšĞ¾Ğ´ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ°: {result.returncode}")
            return False
        except PermissionError:
            if attempt < retries - 1:
                logger.warning(f"  PermissionError, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ {attempt+2}/{retries}...")
                time.sleep(5)
            else:
                logger.error("  âŒ PermissionError Ğ¿Ğ¾ÑĞ»Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº")
                return False
        except subprocess.TimeoutExpired:
            logger.error("  âŒ Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ (2 Ñ‡Ğ°ÑĞ°)")
            return False
    return False


def gpu_info():
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ GPU."""
    try:
        import torch
        if torch.cuda.is_available():
            name = torch.cuda.get_device_name(0)
            vram = torch.cuda.get_device_properties(0).total_mem / 1024**3
            return name, vram
    except Exception:
        pass
    return None, 0


def get_ram_gb():
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼ RAM Ğ² GB."""
    try:
        import psutil
        return psutil.virtual_memory().total / 1024**3
    except ImportError:
        # Fallback Ğ´Ğ»Ñ Windows
        try:
            import ctypes
            kernel32 = ctypes.windll.kernel32
            class MEMORYSTATUSEX(ctypes.Structure):
                _fields_ = [("dwLength", ctypes.c_ulong),
                           ("dwMemoryLoad", ctypes.c_ulong),
                           ("ullTotalPhys", ctypes.c_ulonglong)]
            stat = MEMORYSTATUSEX()
            stat.dwLength = ctypes.sizeof(stat)
            kernel32.GlobalMemoryStatusEx(ctypes.byref(stat))
            return stat.ullTotalPhys / 1024**3
        except Exception:
            return 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 0: Ğ£Ğ¡Ğ¢ĞĞĞĞ’ĞšĞ Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ•Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_0_install():
    """Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹."""
    banner(0, "Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹")
    
    global PYTHON
    
    # â•â•â• ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ venv (PEP 668 fix) â•â•â•
    venv_dir = ROOT / "venv"
    if sys.platform == "win32":
        venv_python = venv_dir / "Scripts" / "python.exe"
        venv_pip = venv_dir / "Scripts" / "pip.exe"
    else:
        venv_python = venv_dir / "bin" / "python"
        venv_pip = venv_dir / "bin" / "pip"
    
    if not venv_python.exists():
        logger.info("  ğŸ”§ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (venv)...")
        try:
            subprocess.run([sys.executable, "-m", "venv", str(venv_dir)], check=True)
            logger.info(f"  âœ… venv ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {venv_dir}")
        except subprocess.CalledProcessError as e:
            logger.error(f"  âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ venv: {e}")
            logger.info("  ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· --break-system-packages...")
            # Fallback: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ pip Ñ --break-system-packages
            PYTHON = sys.executable
            pip_cmd = [PYTHON, "-m", "pip"]
            packages = [
                "torch", "numpy", "einops", "tqdm",
                "sentencepiece", "tokenizers",
                "sentence-transformers", "datasets", "psutil",
            ]
            logger.info("ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ²...")
            for pkg in packages:
                try:
                    __import__(pkg.replace("-", "_"))
                    logger.info(f"  âœ… {pkg} â€” ÑƒĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
                except ImportError:
                    logger.info(f"  ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° {pkg}...")
                    run(pip_cmd + ["install", pkg, "--quiet", "--break-system-packages"], check=False)
            return True
    
    # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ PYTHON Ğ´Ğ»Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°
    PYTHON = str(venv_python)
    pip_cmd = [str(venv_pip)]
    logger.info(f"  ğŸ Python: {PYTHON}")
    
    # ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹
    packages = [
        # Ğ¯Ğ´Ñ€Ğ¾
        "torch", "numpy", "einops", "tqdm",
        # Ğ¢Ğ¾ĞºĞµĞ½Ğ°Ğ¹Ğ·ĞµÑ€Ñ‹
        "sentencepiece", "tokenizers",
        # ĞŸĞ°Ğ¼ÑÑ‚ÑŒ (LEANN)
        "sentence-transformers",
        # Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹
        "datasets",
        # ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³
        "psutil",
    ]
    
    logger.info("ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ²...")
    
    for pkg in packages:
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· venv python
        check_cmd = [PYTHON, "-c", f"import {pkg.replace('-', '_')}"]
        try:
            subprocess.run(check_cmd, capture_output=True, timeout=10)
            result_ok = subprocess.run(check_cmd, capture_output=True, timeout=10).returncode == 0
        except Exception:
            result_ok = False
        
        if result_ok:
            logger.info(f"  âœ… {pkg} â€” ÑƒĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
        else:
            logger.info(f"  ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° {pkg}...")
            run(pip_cmd + ["install", pkg, "--quiet"], check=False)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° CUDA
    gpu_name, vram = gpu_info()
    ram = get_ram_gb()
    
    logger.info("")
    logger.info(f"  ğŸ–¥ï¸  GPU: {gpu_name or 'ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½'}")
    if vram > 0:
        logger.info(f"  ğŸ’¾ VRAM: {vram:.1f} GB")
    logger.info(f"  ğŸ§  RAM: {ram:.0f} GB")
    
    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 1: Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ• Ğ”ĞĞĞĞ«Ğ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_1_download():
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."""
    banner(1, "Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    
    success = True
    
    # 1.1 Wikipedia
    wiki_path = DATA / "wiki_ru.txt"
    if wiki_path.exists() and wiki_path.stat().st_size > 1_000_000:
        wiki_mb = wiki_path.stat().st_size / 1024 / 1024
        logger.info(f"  ğŸ“š Wikipedia: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({wiki_mb:.1f} MB)")
    else:
        logger.info("  ğŸ“š Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Wikipedia (100 000 ÑÑ‚Ğ°Ñ‚ĞµĞ¹)...")
        if not run([PYTHON, TRAINING / "download_wiki.py"], check=False):
            logger.warning("  âš  Wikipedia Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ° â€” Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼")
            success = False
    
    # 1.2 HuggingFace Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹
    hf_files = list(DATA.glob("hf_*.txt"))
    if len(hf_files) >= 3:
        total_mb = sum(f.stat().st_size for f in hf_files) / 1024 / 1024
        logger.info(f"  ğŸ¤— HuggingFace: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({len(hf_files)} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ², {total_mb:.0f} MB)")
    else:
        logger.info("  ğŸ¤— Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ HuggingFace Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² (ĞºĞ¾Ğ´ + Ñ‡Ğ°Ñ‚ + Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹)...")
        if not run([PYTHON, TRAINING / "download_hf_dataset.py", "--preset", "all"], check=False):
            logger.warning("  âš  HuggingFace Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½ â€” Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼")
            success = False
    
    # 1.3 LEANN embedding model
    emb_path = MODELS / "embeddings"
    if emb_path.exists():
        logger.info(f"  ğŸ§  LEANN embeddings: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({emb_path})")
    else:
        logger.info("  ğŸ§  Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² (all-MiniLM-L6-v2)...")
        try:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('all-MiniLM-L6-v2')
            model.save(str(emb_path))
            logger.info(f"  âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ² {emb_path}")
        except Exception as e:
            logger.warning(f"  âš  Embeddings: {e}")
    
    # 1.4 Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LEANN Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
    logger.info("  ğŸ§  Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² LEANN...")
    try:
        sys.path.insert(0, str(TRAINING))
        from ingest_to_leann import ingest_all
        ingest_all()
        logger.info("  âœ… LEANN Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ°")
    except Exception as e:
        logger.info(f"  â„¹ LEANN: {e} (Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾)")
    
    # Ğ¡Ğ²Ğ¾Ğ´ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    total_data = 0
    for f in DATA.glob("*.txt"):
        total_data += f.stat().st_size
    for f in DATA.glob("*.json"):
        total_data += f.stat().st_size
    
    logger.info(f"\n  ğŸ“Š Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {total_data / 1024 / 1024:.0f} MB")
    return success


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 2: Ğ Ğ•Ğ¤Ğ›Ğ•ĞšĞ¡Ğ« (TIER 1)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_2_reflex():
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°."""
    banner(2, "Ğ ĞµÑ„Ğ»ĞµĞºÑÑ‹ (MinGRU Classifier)")
    
    return run([PYTHON, TRAINING / "train_reflex.py",
        "--epochs", "100",          # 100 ÑĞ¿Ğ¾Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸
        "--lr", "0.002",
    ])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 3: MinGRU LANGUAGE MODEL (TIER 1 / System 1)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_3_mingru(device: str):
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MinGRU LM â€” Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€."""
    banner(3, "MinGRU Language Model (System 1)")
    
    return run([PYTHON, TRAINING / "train_mingru.py",
        "--epochs", "25",           # 25 ÑĞ¿Ğ¾Ñ… (Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ½Ğ° GPU)
        "--lr", "3e-3",
        "--dim", "512",             # ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ
        "--layers", "6",            # 6 ÑĞ»Ğ¾Ñ‘Ğ² MinGRU
        "--batch", "32",            # Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ±Ğ°Ñ‚Ñ‡ (64GB RAM)
        "--seq_len", "256",
        "--augment",                # + HuggingFace Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    ])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 4: MAMBA-2 BRAIN (TIER 2 / System 2) â€” ĞĞ¡ĞĞĞ’ĞĞĞ• ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_4_mamba2(device: str, resume: bool = False):
    """
    ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Mamba-2 brain â€” Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°.
    
    4 Ğ¿Ğ¾Ğ´-Ñ„Ğ°Ğ·Ñ‹:
      Phase 1: Full pretrain (Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, 3 ÑĞ¿Ğ¾Ñ…Ğ¸)
      Phase 2: Fine-tune WKV + Fusion (SSD frozen, 2 ÑĞ¿Ğ¾Ñ…Ğ¸)
      Phase 3: Fine-tune MoLE + MatrixPool (1 ÑĞ¿Ğ¾Ñ…Ğ°)
      Phase 4: Fine-tune WKV Fusion + RAG (1 ÑĞ¿Ğ¾Ñ…Ğ°)
    """
    banner(4, "Mamba-2 Brain (12Ã—768d, Full Architecture)")
    
    # â•â•â• Transfer embedding MinGRU â†’ Mamba-2 â•â•â•
    emb_path = None
    mingru_weights = MODELS / "mingru_weights.pt"
    if mingru_weights.exists():
        logger.info("ğŸ”— ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ° MinGRU â†’ Mamba-2...")
        try:
            import torch
            cp = torch.load(str(mingru_weights), map_location='cpu', weights_only=False)
            state = cp.get('model_state_dict', cp)
            for k in state:
                if 'shared_embedding' in k or 'emb.weight' in k:
                    emb_tensor = state[k]
                    TARS_V3.mkdir(parents=True, exist_ok=True)
                    emb_path = str(TARS_V3 / "_transfer_embedding.pt")
                    torch.save(emb_tensor, emb_path)
                    logger.info(f"  âœ… Embedding ({emb_tensor.shape}) â†’ {emb_path}")
                    break
        except Exception as e:
            logger.warning(f"  âš  Transfer failed: {e}")
    
    # â•â•â• Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ RTX 4090 â•â•â•
    base = [
        "--d_model", "768",         # ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        "--n_layers", "12",         # 12 Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² TarsBlock
        "--vocab_size", "256",      # cp1251 Ğ±Ğ°Ğ¹Ñ‚Ñ‹
        "--batch", "16",            # 16 Ã— seq â†’ 24GB VRAM OK
        "--accum_steps", "4",       # Effective batch = 64
        "--device", device,
        "--curriculum",             # Curriculum learning (64â†’128â†’256)
        "--label_smoothing", "0.1",
    ]
    if emb_path:
        base += ["--pretrained_emb", emb_path]
    
    results = {}
    
    # â”€â”€ Phase 1: Full pretrain (Ğ²ÑĞµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹) â”€â”€
    logger.info("â”€â”€ Phase 1/4: Full pretrain (SSD + WKV + Î©-SSM + MoLE + WaveMerge) â”€â”€")
    results["p1"] = run([PYTHON, TRAINING / "train_mamba2.py"] + base + [
        "--epochs", "8",            # 8 ÑĞ¿Ğ¾Ñ… (24Ñ‡ ÑĞµÑÑĞ¸Ñ â†’ ~6Ñ‡ Ğ½Ğ° Phase 1)
        "--lr", "3e-4",
        "--phase", "1",
        "--seq_len", "256",
    ])
    
    # â”€â”€ Phase 2: Fine-tune WKV + Fusion (SSD frozen) â”€â”€
    logger.info("â”€â”€ Phase 2/4: Fine-tune WKV + Fusion (SSD frozen) â”€â”€")
    results["p2"] = run([PYTHON, TRAINING / "train_mamba2.py"] + base + [
        "--epochs", "5",            # 5 ÑĞ¿Ğ¾Ñ… (24Ñ‡ â†’ ~5Ñ‡ Ğ½Ğ° Phase 2)
        "--lr", "1e-4",
        "--phase", "2",
        "--seq_len", "512",         # Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
        "--resume",
    ])
    
    # â”€â”€ Phase 3: Fine-tune MoLE + MatrixPool + WaveMerge â”€â”€
    logger.info("â”€â”€ Phase 3/4: Fine-tune MoLE + MatrixPool + WaveMerge â”€â”€")
    results["p3"] = run([PYTHON, TRAINING / "train_mamba2.py"] + base + [
        "--epochs", "3",            # 3 ÑĞ¿Ğ¾Ñ…Ğ¸ (24Ñ‡ â†’ ~4Ñ‡ Ğ½Ğ° Phase 3)
        "--lr", "3e-5",
        "--phase", "3",
        "--seq_len", "512",
        "--resume",
    ])
    
    # â”€â”€ Phase 4: Fine-tune WKV RAG State + Memory Integration â”€â”€
    logger.info("â”€â”€ Phase 4/4: Fine-tune WKV + RAG + Memory Injection â”€â”€")
    results["p4"] = run([PYTHON, TRAINING / "train_mamba2.py"] + base + [
        "--epochs", "3",            # 3 ÑĞ¿Ğ¾Ñ…Ğ¸ (24Ñ‡ â†’ ~4Ñ‡ Ğ½Ğ° Phase 4)
        "--lr", "1.5e-5",
        "--phase", "4",
        "--seq_len", "512",
        "--resume",
    ])
    
    all_ok = all(results.values())
    if all_ok:
        logger.info("âœ… Ğ’ÑĞµ 4 Ñ„Ğ°Ğ·Ñ‹ Mamba-2 Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ñ‹")
    else:
        failed = [k for k, v in results.items() if not v]
        logger.warning(f"âš  Ğ¤Ğ°Ğ·Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸: {failed}")
    
    return all_ok


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 5: ĞšĞ’ĞĞĞ¢Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ 1.58-bit
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_5_quantize(device: str):
    """ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ FP16 â†’ 1.58-bit + Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."""
    banner(5, "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ BitNet 1.58-bit")
    
    # Ğ˜Ñ‰ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ FP16 Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    fp16_path = MODELS / "mamba2" / "mamba2_omega.pt"
    if not fp16_path.exists():
        logger.warning(f"  âš  FP16 Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {fp16_path}")
        logger.warning("  ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ")
        return False
    
    logger.info(f"  Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {fp16_path}")
    fp16_size = fp16_path.stat().st_size / 1024 / 1024
    logger.info(f"  Ğ Ğ°Ğ·Ğ¼ĞµÑ€ FP16: {fp16_size:.1f} MB")
    
    # ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ + Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ 5 ÑĞ¿Ğ¾Ñ…
    return run([PYTHON, TRAINING / "train_mamba2.py",
        "--d_model", "768",
        "--n_layers", "12",
        "--batch", "16",
        "--accum_steps", "4",
        "--epochs", "5",            # 5 ÑĞ¿Ğ¾Ñ… STE Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (~5Ñ‡ Ğ½Ğ° 4090)
        "--lr", "5e-5",
        "--phase", "1",
        "--quant",                  # 1.58-bit Ñ€ĞµĞ¶Ğ¸Ğ¼ (STE)
        "--resume",
        "--device", device,
        "--seq_len", "256",
        "--label_smoothing", "0.1",
    ])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 6: Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ¯ Ğ¡Ğ‘ĞĞ ĞšĞ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_6_consolidate(results: dict, total_time: float):
    """Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² models/tars_v3/."""
    banner(6, "Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ±Ğ¾Ñ€ĞºĞ°")
    
    TARS_V3.mkdir(parents=True, exist_ok=True)
    
    copies = {
        "reflex": (MODELS / "reflex" / "reflex_classifier.pt", TARS_V3 / "reflex.pt"),
        "mingru": (MODELS / "mingru_weights.pt", TARS_V3 / "mingru.pt"),
        "mamba2_fp16": (MODELS / "mamba2" / "mamba2_omega.pt", TARS_V3 / "mamba2.pt"),
        "mamba2_158bit": (MODELS / "mamba2" / "mamba2_omega_158bit.pt", TARS_V3 / "mamba2_158bit.pt"),
    }
    
    consolidated = []
    for name, (src, dst) in copies.items():
        if src.exists():
            shutil.copy2(str(src), str(dst))
            size_mb = dst.stat().st_size / 1024 / 1024
            logger.info(f"  ğŸ“¦ {name}: {src.name} â†’ tars_v3/{dst.name} ({size_mb:.1f} MB)")
            consolidated.append(name)
        else:
            logger.info(f"  â­ {name}: Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ ({src.name})")
    
    # Ğ›Ğ¾Ğ³ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "total_time_sec": round(total_time, 1),
        "total_time_human": f"{total_time/3600:.1f} Ñ‡Ğ°ÑĞ¾Ğ²",
        "results": {k: ("ok" if v else "failed") for k, v in results.items()},
        "models_consolidated": consolidated,
        "config": {
            "encoding": "cp1251",
            "vocab_size": 256,
            "d_model": 768,
            "n_layers": 12,
            "n_experts": 8,
            "omega_dim": 32,
            "pool_size": 48,
        },
    }
    
    log_path = TARS_V3 / "training_log.json"
    logs = []
    if log_path.exists():
        try:
            logs = json.loads(log_path.read_text(encoding="utf-8"))
        except Exception:
            pass
    if not isinstance(logs, list):
        logs = []
    logs.append(log_entry)
    log_path.write_text(json.dumps(logs, ensure_ascii=False, indent=2), encoding="utf-8")
    
    logger.info(f"\n  ğŸ“‹ Training log: {log_path}")
    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ğ¤ĞĞ—Ğ 7: Ğ’ĞĞ›Ğ˜Ğ”ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def phase_7_validate():
    """Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."""
    banner(7, "Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ")
    
    try:
        sys.path.insert(0, str(ROOT))
        import torch
        from brain.tokenizer import TarsTokenizer
        from brain.mamba2.model import TarsMamba2LM
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        tokenizer = TarsTokenizer()
        
        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        model, ckpt = TarsMamba2LM.load_pretrained(device=device)
        model.eval()
        
        if ckpt is None:
            logger.warning("  âš  ĞĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ")
            return False
        
        # Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹
        test_prompts = ["Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚", "ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°", "Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ"]
        
        logger.info(f"  ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {ckpt}")
        params = sum(p.numel() for p in model.parameters())
        logger.info(f"  ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: {params:,}")
        
        for prompt in test_prompts:
            tokens = tokenizer.encode(prompt)
            input_ids = torch.tensor([tokens], dtype=torch.long, device=device)
            
            with torch.no_grad():
                logits = model(input_ids)
            
            # Ğ‘ĞµÑ€Ñ‘Ğ¼ top-5 Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°
            probs = torch.softmax(logits[0, -1, :], dim=-1)
            top5 = torch.topk(probs, 5)
            
            predictions = []
            for idx, prob in zip(top5.indices.tolist(), top5.values.tolist()):
                char = tokenizer.decode([idx])
                predictions.append(f"'{char}'({prob:.2%})")
            
            logger.info(f"  \"{prompt}\" â†’ {', '.join(predictions)}")
        
        logger.info("  âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚!")
        return True
        
    except Exception as e:
        logger.error(f"  âŒ Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ»Ğ°ÑÑŒ: {e}")
        import traceback
        traceback.print_exc()
        return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="Ğ¢ĞĞ Ğ¡ v3 â€” ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:
  python mega_train.py                        # ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ (~4-6 Ñ‡Ğ°ÑĞ¾Ğ²)
  python mega_train.py --skip-download        # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ¶Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½Ñ‹
  python mega_train.py --phase 4              # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Mamba-2
  python mega_train.py --quick                # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ (256d, 4 ÑĞ»Ğ¾Ñ)
  python mega_train.py --skip-quantize        # Ğ‘ĞµĞ· ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        """
    )
    
    parser.add_argument("--skip-download", action="store_true",
                        help="ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    parser.add_argument("--skip-quantize", action="store_true",
                        help="ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 1.58-bit")
    parser.add_argument("--phase", type=int, choices=[0,1,2,3,4,5,6,7],
                        help="Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ñ„Ğ°Ğ·Ñƒ")
    parser.add_argument("--quick", action="store_true",
                        help="Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ (Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, 1 ÑĞ¿Ğ¾Ñ…Ğ°)")
    
    args = parser.parse_args()
    
    # â•â•â• ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¶ĞµĞ»ĞµĞ·Ğ° â•â•â•
    gpu_name, vram = gpu_info()
    ram = get_ram_gb()
    device = "cuda" if gpu_name else "cpu"
    
    logger.info("")
    logger.info("â•”" + "â•" * 62 + "â•—")
    logger.info("â•‘                                                              â•‘")
    logger.info("â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â•‘")
    logger.info("â•‘      â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•—    â•‘")
    logger.info("â•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â•‘")
    logger.info("â•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â•â•â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â• â•šâ•â•â•â–ˆâ–ˆâ•—    â•‘")
    logger.info("â•‘      â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â•‘")
    logger.info("â•‘      â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•      â•šâ•â•â•â•  â•šâ•â•â•â•â•â•     â•‘")
    logger.info("â•‘                                                              â•‘")
    logger.info("â•‘           MEGA TRAIN â€” Autonomous Pipeline                   â•‘")
    logger.info("â•‘                                                              â•‘")
    logger.info("â•š" + "â•" * 62 + "â•")
    logger.info("")
    logger.info(f"  ğŸ–¥ï¸  GPU:    {gpu_name or 'CPU only'}")
    logger.info(f"  ğŸ’¾ VRAM:   {vram:.1f} GB" if vram > 0 else "  ğŸ’¾ VRAM:   N/A")
    logger.info(f"  ğŸ§  RAM:    {ram:.0f} GB")
    logger.info(f"  ğŸ Python: {sys.version.split()[0]}")
    logger.info(f"  ğŸ“ Root:   {ROOT}")
    logger.info(f"  â° Start:  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    if device == "cuda" and vram < 8:
        logger.warning("âš  VRAM < 8GB â€” Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ batch size")
    
    t0 = time.time()
    results = {}
    
    # â•â•â• Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ· â•â•â•
    phases = {
        0: ("install", lambda: phase_0_install()),
        1: ("download", lambda: phase_1_download()),
        2: ("reflex", lambda: phase_2_reflex()),
        3: ("mingru", lambda: phase_3_mingru(device)),
        4: ("mamba2", lambda: phase_4_mamba2(device)),
        5: ("quantize", lambda: phase_5_quantize(device)),
        7: ("validate", lambda: phase_7_validate()),
    }
    
    # Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ñ„Ğ°Ğ·Ğ°
    if args.phase is not None:
        if args.phase in phases:
            name, func = phases[args.phase]
            results[name] = func()
        elif args.phase == 6:
            phase_6_consolidate(results, time.time() - t0)
        else:
            logger.error(f"ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ°Ñ Ñ„Ğ°Ğ·Ğ°: {args.phase}")
        total = time.time() - t0
        logger.info(f"\n  Ğ¤Ğ°Ğ·Ğ° {args.phase} Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° Ğ·Ğ° {total:.0f}s")
        return
    
    # â•â•â• ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ â•â•â•
    
    # Ğ¤Ğ°Ğ·Ğ° 0: Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
    results["install"] = phase_0_install()
    
    # Ğ¤Ğ°Ğ·Ğ° 1: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ
    if not args.skip_download:
        results["download"] = phase_1_download()
    else:
        logger.info("â­ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (--skip-download)")
        results["download"] = True
    
    # Ğ¤Ğ°Ğ·Ğ° 2: Ğ ĞµÑ„Ğ»ĞµĞºÑÑ‹
    results["reflex"] = phase_2_reflex()
    
    # Ğ¤Ğ°Ğ·Ğ° 3: MinGRU
    results["mingru"] = phase_3_mingru(device)
    
    # Ğ¤Ğ°Ğ·Ğ° 4: Mamba-2 (Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ)
    results["mamba2"] = phase_4_mamba2(device)
    
    # Ğ¤Ğ°Ğ·Ğ° 5: ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    if not args.skip_quantize:
        results["quantize"] = phase_5_quantize(device)
    else:
        logger.info("â­ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (--skip-quantize)")
        results["quantize"] = True
    
    # Ğ¤Ğ°Ğ·Ğ° 6: Ğ¡Ğ±Ğ¾Ñ€ĞºĞ°
    total_time = time.time() - t0
    phase_6_consolidate(results, total_time)
    
    # Ğ¤Ğ°Ğ·Ğ° 7: Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
    results["validate"] = phase_7_validate()
    
    # â•â•â• Ğ˜Ğ¢ĞĞ“Ğ˜ â•â•â•
    total_time = time.time() - t0
    hours = total_time / 3600
    
    logger.info("")
    logger.info("â•”" + "â•" * 62 + "â•—")
    logger.info("â•‘              Ğ˜Ğ¢ĞĞ“Ğ˜ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯                                  â•‘")
    logger.info("â• " + "â•" * 62 + "â•£")
    for name, ok in results.items():
        icon = "âœ…" if ok else "âŒ"
        logger.info(f"â•‘  {icon} {name:<20s}                                    â•‘")
    logger.info("â• " + "â•" * 62 + "â•£")
    logger.info(f"â•‘  â±  Ğ’Ñ€ĞµĞ¼Ñ: {hours:.1f} Ñ‡Ğ°ÑĞ¾Ğ² ({total_time:.0f} ÑĞµĞº)                         â•‘")
    logger.info("â•š" + "â•" * 62 + "â•")
    
    if all(results.values()):
        logger.info("")
        logger.info("  ğŸ¯ Ğ’Ğ¡Ğ• Ğ¤ĞĞ—Ğ« Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ« Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞ!")
        logger.info("  ğŸ“ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ñ‹: models/tars_v3/")
        logger.info("  ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº: python launch_tars.py")
        logger.info("")
    else:
        failed = [k for k, v in results.items() if not v]
        logger.info("")
        logger.info(f"  âš  Ğ¤Ğ°Ğ·Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸: {', '.join(failed)}")
        logger.info("  ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ mega_train.log Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹")
        logger.info("")


if __name__ == "__main__":
    main()
