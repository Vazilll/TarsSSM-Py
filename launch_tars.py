"""
launch_tars.py â€” TARS v3 Auto-Setup & Verification.

1. ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ²ÑĞµ pip-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ
3. Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ CLI Ğ¿Ñ€Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğµ

Usage:
    python launch_tars.py          # Setup + Verify + CLI
    python launch_tars.py --check  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° (Ğ±ĞµĞ· CLI)
"""
import sys
import os
import subprocess
import logging
import argparse
import time

# ĞšĞ¾Ñ€ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, ROOT)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("Tars.Launcher")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 0: Auto-Install Dependencies
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REQUIRED_PACKAGES = {
    # import_name: pip_package_name
    "torch": "torch",
    "numpy": "numpy",
    "einops": "einops",
    "tqdm": "tqdm",
}

# ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ (Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ¿ÑƒÑĞº)
OPTIONAL_PACKAGES = {
    "sentencepiece": "sentencepiece",
    "tokenizers": "tokenizers",
    "sounddevice": "sounddevice",
    "duckduckgo_search": "duckduckgo-search",
}


def auto_install():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ pip-Ğ¿Ğ°ĞºĞµÑ‚Ñ‹."""
    missing = []
    for imp_name, pkg_name in REQUIRED_PACKAGES.items():
        try:
            __import__(imp_name)
        except ImportError:
            missing.append((imp_name, pkg_name))

    if not missing:
        logger.info("âœ… Ğ’ÑĞµ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹")
        return True

    logger.info(f"ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° {len(missing)} Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ²...")
    for imp_name, pkg_name in missing:
        logger.info(f"   pip install {pkg_name}...")
        try:
            subprocess.check_call(
                [sys.executable, "-m", "pip", "install", pkg_name, "-q"],
                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
            )
            logger.info(f"   âœ… {pkg_name}")
        except Exception as e:
            logger.error(f"   âŒ {pkg_name}: {e}")
            return False

    # ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ â€” ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ Ğ¼Ğ¾Ğ»Ñ‡Ğ°, Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµĞ¼
    for imp_name, pkg_name in OPTIONAL_PACKAGES.items():
        try:
            __import__(imp_name)
        except ImportError:
            try:
                subprocess.check_call(
                    [sys.executable, "-m", "pip", "install", pkg_name, "-q"],
                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
                )
            except Exception:
                pass  # ĞĞµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾

    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 1: Verify All Modules
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def verify():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ²ÑĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹."""
    results = {}

    # 1. OmegaCore C++ Kernel
    try:
        from brain.omega_core import get_omega_core
        core = get_omega_core()
        if core.available:
            results["OmegaCore"] = f"âœ… C++ v{core.version}"
        else:
            results["OmegaCore"] = "âš ï¸  Python fallback (DLL Ğ½Ğµ ÑĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½)"
    except Exception as e:
        results["OmegaCore"] = f"âš ï¸  {e}"

    # 2. MinGRU (Tier 1)
    try:
        from brain.min_gru.mingru import MinGRU
        results["MinGRU"] = "âœ… Tier 1 Reflex"
    except Exception as e:
        results["MinGRU"] = f"âŒ {e}"

    # 3. Reflex Classifier
    try:
        from brain.reflex_classifier import ReflexClassifier
        rc = ReflexClassifier(vocab_size=256, embed_dim=64, hidden_dim=64)
        results["ReflexClassifier"] = f"âœ… {rc.count_parameters():,} params"
    except Exception as e:
        results["ReflexClassifier"] = f"âŒ {e}"

    # 4. Mamba-2 (Tier 2)
    try:
        from brain.mamba2.model import TarsMamba2LM
        m = TarsMamba2LM(d_model=128, n_layers=2, vocab_size=256, mingru_dim=64)
        info = m.count_parameters()
        total = info["total"] if isinstance(info, dict) else info
        results["Mamba-2 LM"] = f"âœ… {total:,} params"
    except Exception as e:
        results["Mamba-2 LM"] = f"âŒ {e}"

    # 5. Generator
    try:
        from brain.mamba2.generate_mamba import TarsGenerator
        results["Generator"] = "âœ… <thought>/<tool> parser"
    except Exception as e:
        results["Generator"] = f"âŒ {e}"

    # 6. RRN
    try:
        from brain.rrn import RrnCore
        results["RRN"] = "âœ… Tier 1.5 Relational"
    except Exception as e:
        results["RRN"] = f"âš ï¸  {e}"

    # 7. MoLE
    try:
        from brain.mole import MoleManager
        results["MoLE"] = "âœ… 8 experts"
    except Exception as e:
        results["MoLE"] = f"âš ï¸  {e}"

    # 8. Knowledge Injector (RAG)
    try:
        from agent.knowledge_injector import KnowledgeInjector
        results["RAG Injector"] = "âœ… web/file/recall"
    except Exception as e:
        results["RAG Injector"] = f"âŒ {e}"

    # Print results
    print("\n" + "=" * 55)
    print("   TARS v3.0 â€” System Verification")
    print("=" * 55)
    all_ok = True
    for name, status in results.items():
        icon = "â”‚"
        print(f"  {icon} {name:20s} {status}")
        if "âŒ" in status:
            all_ok = False
    print("=" * 55)

    if all_ok:
        print("  ğŸ¯ ALL CORE SYSTEMS OPERATIONAL\n")
    else:
        print("  âš ï¸  Some modules failed\n")

    return all_ok


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 2: Launch CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Ğ ÑƒÑÑĞºĞ¸Ğ¹ Ğ°Ğ»Ñ„Ğ°Ğ²Ğ¸Ñ‚ + ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹ (Ğ´Ğ»Ñ Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ)
CHARSET = (
    " Ğ°Ğ±Ğ²Ğ³Ğ´ĞµÑ‘Ğ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑÑ"
    "ĞĞ‘Ğ’Ğ“Ğ”Ğ•ĞĞ–Ğ—Ğ˜Ğ™ĞšĞ›ĞœĞĞĞŸĞ Ğ¡Ğ¢Ğ£Ğ¤Ğ¥Ğ¦Ğ§Ğ¨Ğ©ĞªĞ«Ğ¬Ğ­Ğ®Ğ¯"
    "abcdefghijklmnopqrstuvwxyz"
    "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    "0123456789.,!?;:()-â€”\"'Â«Â»\n"
)

class CharTokenizer:
    """
    ĞŸĞ¾ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ (char-level).
    ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ» = 1 Ñ‚Ğ¾ĞºĞµĞ½. Vocab = len(charset) + 1.
    """
    def __init__(self, charset=CHARSET):
        self.char2id = {ch: i + 1 for i, ch in enumerate(charset)}
        self.id2char = {i + 1: ch for i, ch in enumerate(charset)}
        self.eos_token_id = 0
        self.vocab_size = len(charset) + 1  # +1 Ğ´Ğ»Ñ EOS/PAD (id=0)

    def encode(self, text: str) -> list:
        return [self.char2id.get(ch, 1) for ch in text]

    def decode(self, ids: list) -> str:
        return "".join(self.id2char.get(i, "") for i in ids if i != 0)


def run_cli():
    """Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ CLI Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¼Ğ¾Ğ·Ğ³Ğ°."""
    print("\n" + "=" * 60)
    print("   TARS v3.0 â€” Interactive Console (Verbose Brain Mode)")
    print("   Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¸Ğ»Ğ¸ 'Ğ²Ñ‹Ñ…Ğ¾Ğ´' Ğ´Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ")
    print("=" * 60 + "\n")

    try:
        from brain.mamba2.model import TarsMamba2LM
        from brain.mamba2.generate_mamba import TarsGenerator, GenerationConfig
        from brain.omega_core import get_omega_core
        from brain.reflexes.reflex_dispatcher import ReflexDispatcher
        import torch

        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"  Device: {device}")

        # Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€
        tokenizer = CharTokenizer()
        print(f"  Tokenizer: CharTokenizer (vocab={tokenizer.vocab_size}, chars={tokenizer.vocab_size - 1})")

        # ĞœĞ¾Ğ´ĞµĞ»ÑŒ â€” vocab_size Ğ¢ĞĞ§ĞĞ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼
        model = TarsMamba2LM(
            d_model=256, n_layers=4, vocab_size=tokenizer.vocab_size, mingru_dim=128
        ).to(device)
        model.eval()
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  Model: Mamba-2 LM ({total_params:,} params, UNTRAINED)")

        core = get_omega_core()
        print(f"  OmegaCore: {'C++ DLL' if core.available else 'Python fallback'}")
        
        # â•â•â• Reflex Dispatcher (Ğ¡Ğ¿Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ·Ğ³) â•â•â•
        dispatcher = ReflexDispatcher(memory=None, max_workers=6)
        print(f"  Reflexes: {len(dispatcher.sensors)} sensors (parallel ThreadPool)")
        
        print(f"\n  âš ï¸  ĞœĞ¾Ğ´ĞµĞ»ÑŒ ĞĞ• Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° â€” Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹!")
        print(f"  ĞĞ±ÑƒÑ‡Ğ¸Ñ‚Ğµ: python training/train_mamba2.py\n")

        gen = TarsGenerator(model, tokenizer, omega_core=core)
        config = GenerationConfig(max_tokens=64, temperature=0.9, top_k=40, top_p=0.92)

        while True:
            try:
                user_input = input("Ğ’Ñ‹: ").strip()
            except (EOFError, KeyboardInterrupt):
                break

            if not user_input:
                continue
            if user_input.lower() in ["Ğ²Ñ‹Ñ…Ğ¾Ğ´", "exit", "quit", "ÑÑ‚Ğ¾Ğ¿"]:
                print("\nTARS: Ğ”Ğ¾ ÑĞ²ÑĞ·Ğ¸.")
                break

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 1: Reflex Dispatch (6 sensors Ã— parallel)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            reflex_ctx = dispatcher.dispatch(user_input)
            
            print(f"\n{'â”€' * 60}")
            print(f"  âš¡ â•â•â• Reflexes ({reflex_ctx.dispatch_time_ms:.0f}ms) â•â•â•")
            print(f"  â”‚ {reflex_ctx.summary_line()}")
            
            # Sensor timing breakdown
            timings = " ".join(
                f"{name}:{ms:.0f}ms"
                for name, ms in sorted(reflex_ctx.sensor_times.items(), key=lambda x: -x[1])
            )
            print(f"  â”‚ Sensors: {timings}")
            
            if reflex_ctx.dominant_emotion != "neutral":
                print(f"  â”‚ Emotion: {reflex_ctx.dominant_emotion} (urgency={reflex_ctx.urgency:.0%})")
            print(f"  â”‚ System:  CPU {reflex_ctx.cpu_percent:.0f}%, RAM {reflex_ctx.ram_free_gb:.1f}GB, GPU {'âœ…' if reflex_ctx.gpu_available else 'âŒ'}")
            if reflex_ctx.is_followup:
                print(f"  â”‚ Context: â†©ï¸ Follow-up (session #{reflex_ctx.session_length})")
            if reflex_ctx.rag_found:
                print(f"  â”‚ RAG:     {len(reflex_ctx.rag_snippets)} docs found")
            print(f"  â•°{'â”€' * 45}")
            
            # â•â•â• Fast response (no brain needed) â•â•â•
            if reflex_ctx.can_handle_fast and reflex_ctx.fast_response:
                print(f"\n  ğŸ’¬ TARS (Ñ€ĞµÑ„Ğ»ĞµĞºÑ): {reflex_ctx.fast_response}")
                dispatcher.add_to_history(user_input, reflex_ctx.fast_response, reflex_ctx.intent)
                print(f"\n  ğŸ“Š Reflex handled ({reflex_ctx.dispatch_time_ms:.0f}ms, brain NOT invoked)")
                print(f"{'â”€' * 60}")
                continue

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STEP 2: Full Brain Think (with enriched context)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            input_ids = tokenizer.encode(user_input)
            input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)
            print(f"\n  ğŸ“ Input: \"{user_input}\"")
            print(f"  ğŸ”¢ Tokens: {input_ids[:20]}{'...' if len(input_ids) > 20 else ''} (len={len(input_ids)})")

            # â”€â”€ Think (IDME deep reasoning + adaptive depth) â”€â”€
            t0 = time.time()
            think_result = model.think(
                input_tensor,
                query_text=user_input,
                reflex_ctx=reflex_ctx,
            )

            if isinstance(think_result, tuple):
                logits, stats = think_result
            else:
                logits = think_result
                stats = {}

            think_time = (time.time() - t0) * 1000

            # â”€â”€ ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¼Ğ¾Ğ·Ğ³Ğ° â”€â”€
            print(f"\n  ğŸ§  â•â•â• Brain Think (Deep WuNeng Core: Mamba-2 + RWKV-7) â•â•â•")
            print(f"  â”‚ Task Type:     {stats.get('task_type', '?')}")
            be = stats.get('blocks_executed', '?')
            ed = stats.get('estimated_depth', '?')
            tb = stats.get('total_blocks', '?')
            print(f"  â”‚ Depth:         {be}/{tb} blocks (target: {ed})")
            print(f"  â”‚ p-convergence: {stats.get('final_p', 0):.4f}  (Ğ¿Ğ¾Ñ€Ğ¾Ğ³: {stats.get('p_threshold', 1.2):.1f})")
            print(f"  â”‚ Converged:     {'âœ… Ğ”Ğ°' if stats.get('converged', False) else 'âŒ ĞĞµÑ‚'}")
            print(f"  â”‚ IDME Rounds:   {stats.get('expansion_rounds', 0)}")
            bt = stats.get('branches_tested', 0)
            bw = stats.get('branches_won', 0)
            print(f"  â”‚ Branches:      {bw}/{bt} Ğ¿Ğ¾Ğ±ĞµĞ´  (3 ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ°/Ñ€Ğ°ÑƒĞ½Ğ´)")
            print(f"  â”‚ Matrices Used: {stats.get('total_matrices', 0)} ({stats.get('matrices_recruited', 0)} Ñ€ĞµĞºÑ€ÑƒÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾)")
            rwkv_mb = stats.get('rwkv_state_size_mb', 0)
            print(f"  â”‚ RWKV State:    {rwkv_mb:.2f} MB (O(1) memory)")
            print(f"  â”‚ Hankel Collapses: {stats.get('hankel_collapses', 0)}")

            # Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹
            experts = stats.get('active_experts', [])
            if experts:
                expert_str = ", ".join(experts) if isinstance(experts[0], str) else str(experts)
                print(f"  â”‚ MoLE Experts:  {expert_str}")

            print(f"  â”‚ Think Time:    {stats.get('total_ms', think_time):.0f}ms")
            print(f"  â”‚ Logits Shape:  {list(logits.shape)}")

            # Top-5 Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹
            probs = torch.softmax(logits[0, -1, :], dim=-1)
            top5_probs, top5_ids = probs.topk(5)
            print(f"  â”‚")
            print(f"  â”‚ Top-5 Next Tokens:")
            for prob, tid in zip(top5_probs, top5_ids):
                char = tokenizer.decode([tid.item()])
                char_display = repr(char) if char in ['\n', ' '] else char
                print(f"  â”‚   '{char_display}' (id={tid.item():3d})  p={prob.item():.4f}")

            print(f"  â•°{'â”€' * 45}")

            # â”€â”€ Generate â”€â”€
            print(f"\n  ğŸ’¬ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ:")
            print(f"  ", end="")

            t1 = time.time()
            result = gen.generate(
                user_input, config=config,
                on_token=lambda t: print(t, end="", flush=True)
            )
            gen_time = time.time() - t1

            # â”€â”€ Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° â”€â”€
            tps = result.tokens_generated / gen_time if gen_time > 0 else 0
            print(f"\n\n  ğŸ“Š â•â•â• Generation Stats â•â•â•")
            print(f"  â”‚ Tokens:     {result.tokens_generated}")
            print(f"  â”‚ Time:       {gen_time:.2f}s ({tps:.1f} tok/s)")
            print(f"  â”‚ p-final:    {result.p_convergence:.4f}")
            print(f"  â”‚ IDME rounds:{result.idme_rounds}")
            if result.tool_calls:
                print(f"  â”‚ Tool calls: {result.tool_calls}")
            if result.thought:
                print(f"  â”‚ Thought:    {result.thought[:80]}...")
            print(f"  â•°{'â”€' * 45}\n")
            
            # Update session history
            dispatcher.add_to_history(user_input, result.text, reflex_ctx.intent)

    except Exception as e:
        logger.error(f"CLI Error: {e}")
        import traceback
        traceback.print_exc()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="TARS v3 Launcher")
    parser.add_argument("--check", action="store_true", help="Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° (Ğ±ĞµĞ· CLI)")
    parser.add_argument("--no-install", action="store_true", help="ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹")
    args = parser.parse_args()

    # Phase 0: Auto-install
    if not args.no_install:
        if not auto_install():
            logger.error("Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ. Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ:")
            logger.error("  pip install torch numpy einops tqdm")
            sys.exit(1)

    # Phase 1: Verify
    ok = verify()

    # Phase 2: CLI (ĞµÑĞ»Ğ¸ Ğ²ÑÑ‘ Ğ¾Ğº Ğ¸ Ğ½Ğµ --check)
    if not args.check and ok:
        run_cli()
