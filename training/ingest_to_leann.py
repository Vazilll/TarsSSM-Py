"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  LEANN Ingestion ‚Äî –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ø–∞–º—è—Ç—å –¢–ê–†–°
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã (Wikipedia, –¥–∞—Ç–∞—Å–µ—Ç—ã) –Ω–∞ —á–∞–Ω–∫–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç
–≤ LEANN –∏–Ω–¥–µ–∫—Å –¥–ª—è RAG (Retrieval-Augmented Generation).

–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Colab (54 GB RAM):
  - –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ float16 (–≤–º–µ—Å—Ç–æ float64 JSON) ‚Üí ~8x —ç–∫–æ–Ω–æ–º–∏—è
  - –ë–∞—Ç—á–µ–≤–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–Ω–µ –¥–µ—Ä–∂–∏–º –≤—Å—ë –≤ RAM)
  - Numpy binary —Ñ–æ—Ä–º–∞—Ç (.npz) –≤–º–µ—Å—Ç–æ JSON

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  python training/ingest_to_leann.py                      # –í—Å—ë –∏–∑ data/
  python training/ingest_to_leann.py --file data/wiki_ru.txt
  python training/ingest_to_leann.py --chunk-size 500
"""

import os
import sys
import re
import gc
import logging
import json
import numpy as np
from pathlib import Path

ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT))

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("LEANN.Ingest")


def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.
    """
    paragraphs = re.split(r'\n{2,}', text)
    
    chunks = []
    current = ""
    
    for para in paragraphs:
        para = para.strip()
        if not para or len(para) < 30:
            continue
            
        if len(current) + len(para) < chunk_size:
            current += " " + para if current else para
        else:
            if current:
                chunks.append(current.strip())
            if len(para) > chunk_size:
                words = para.split()
                sub = ""
                for w in words:
                    if len(sub) + len(w) + 1 > chunk_size:
                        if sub:
                            chunks.append(sub.strip())
                        sub = w
                    else:
                        sub += " " + w if sub else w
                current = sub
            else:
                current = para
    
    if current and len(current) > 30:
        chunks.append(current.strip())
    
    return chunks


def batch_embeddings(texts: list, model, batch_size: int = 256):
    """
    –ë–∞—Ç—á–µ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (int8 array, float32 scales).
    """
    all_int8 = []
    all_scales = []
    total = len(texts)
    
    for i in range(0, total, batch_size):
        batch = texts[i:i + batch_size]
        embs = model.encode(batch, show_progress_bar=False, batch_size=batch_size)
        embs_f32 = embs.astype(np.float32)
        
        # Per-vector int8 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
        scales = np.abs(embs_f32).max(axis=1) / 127.0
        scales = np.maximum(scales, 1e-8)
        embs_q = np.clip(np.round(embs_f32 / scales[:, np.newaxis]), -128, 127).astype(np.int8)
        
        all_int8.append(embs_q)
        all_scales.append(scales.astype(np.float32))
        
        if (i // batch_size) % 10 == 0 and i > 0:
            logger.info(f"[LEANN]   –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{total} ({100*i//total}%)")
    
    return np.vstack(all_int8), np.concatenate(all_scales)


def fallback_embedding(text: str):
    """TF-IDF fallback. Returns (int8_vec, scale)."""
    words = text.lower().split()
    vec = np.zeros(384, dtype=np.float32)
    for w in words:
        idx = hash(w) % 384
        vec[idx] += 1.0
    norm = np.linalg.norm(vec)
    if norm > 0:
        vec = vec / norm
    scale = np.abs(vec).max() / 127.0
    if scale < 1e-8:
        scale = 1e-8
    vec_q = np.clip(np.round(vec / scale), -128, 127).astype(np.int8)
    return vec_q, np.float32(scale)


def load_index(index_path: str):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã:
    - int8 + scales (.npz)
    - float16 (.npz, —Å—Ç–∞—Ä—ã–π)
    - JSON (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
    
    Returns: (texts, embeddings_int8, scales)
    """
    npz_path = index_path.replace(".index", ".npz")
    texts_path = index_path.replace(".index", ".texts.json")
    
    if os.path.exists(npz_path) and os.path.exists(texts_path):
        try:
            data = np.load(npz_path)
            embeddings = data["embeddings"]
            with open(texts_path, 'r', encoding='utf-8') as f:
                texts = json.load(f)
            
            if "scales" in data:
                scales = data["scales"]
            else:
                # –ú–∏–≥—Ä–∞—Ü–∏—è float16 ‚Üí int8
                embs_f32 = embeddings.astype(np.float32)
                scales = np.abs(embs_f32).max(axis=1) / 127.0
                scales = np.maximum(scales, 1e-8).astype(np.float32)
                embeddings = np.clip(np.round(embs_f32 / scales[:, np.newaxis]), -128, 127).astype(np.int8)
                save_index(index_path, texts, embeddings, scales)
                logger.info(f"[LEANN] –ú–∏–≥—Ä–∞—Ü–∏—è float16 ‚Üí int8")
            
            logger.info(f"[LEANN] –ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å: {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {embeddings.nbytes / 1024 / 1024:.1f} MB int8")
            return texts, embeddings, scales
        except Exception as e:
            logger.warning(f"[LEANN] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è npz: {e}")
    
    if os.path.exists(index_path):
        try:
            file_size = os.path.getsize(index_path)
            if file_size < 100:
                return [], None, None
            logger.info(f"[LEANN] –ß—Ç–µ–Ω–∏–µ JSON –∏–Ω–¥–µ–∫—Å–∞ ({file_size / 1024 / 1024:.1f} MB)...")
            with open(index_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            texts = data.get("texts", [])
            raw_embs = data.get("embeddings", [])
            if raw_embs:
                embs_f32 = np.array(raw_embs, dtype=np.float32)
                scales = np.abs(embs_f32).max(axis=1) / 127.0
                scales = np.maximum(scales, 1e-8).astype(np.float32)
                embeddings = np.clip(np.round(embs_f32 / scales[:, np.newaxis]), -128, 127).astype(np.int8)
                save_index(index_path, texts, embeddings, scales)
                os.remove(index_path)
                logger.info(f"[LEANN] –ú–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–æ JSON ‚Üí int8")
                return texts, embeddings, scales
            return texts, None, None
        except Exception as e:
            logger.warning(f"[LEANN] –û—à–∏–±–∫–∞ JSON: {e}")
    
    return [], None, None


def save_index(index_path: str, texts: list, embeddings: np.ndarray, scales: np.ndarray = None):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ int8 —Ñ–æ—Ä–º–∞—Ç–µ:
    - .npz: int8 —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ + float32 scales
    - .texts.json: —Ç–µ–∫—Å—Ç—ã
    
    RAM: 1M docs √ó 384 dim √ó int8 = ~375 MB (vs 8.9 GB JSON)
    """
    npz_path = index_path.replace(".index", ".npz")
    texts_path = index_path.replace(".index", ".texts.json")
    
    os.makedirs(os.path.dirname(index_path) or ".", exist_ok=True)
    
    if scales is not None:
        np.savez_compressed(npz_path, embeddings=embeddings, scales=scales)
    else:
        np.savez_compressed(npz_path, embeddings=embeddings)
    
    with open(texts_path, 'w', encoding='utf-8') as f:
        json.dump(texts, f, ensure_ascii=False)
    
    npz_mb = os.path.getsize(npz_path) / (1024 * 1024)
    texts_mb = os.path.getsize(texts_path) / (1024 * 1024)
    logger.info(f"[LEANN] ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(texts)} docs, "
                f"embeddings={npz_mb:.1f} MB (int8), texts={texts_mb:.1f} MB")


def ingest_file(file_path: str, index_path: str = None,
                chunk_size: int = 500, max_chunks: int = None,
                existing_texts: list = None, existing_embeddings=None,
                model=None):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –≤ LEANN –∏–Ω–¥–µ–∫—Å.
    
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ–∫—Å—Ç—ã/—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å
    –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞.
    
    Returns: (texts, embeddings) ‚Äî –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ
    """
    if index_path is None:
        index_path = str(ROOT / "memory" / "leann.index")
    
    if not os.path.exists(file_path):
        logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return existing_texts or [], existing_embeddings
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    file_mb = len(text.encode('utf-8')) / (1024 * 1024)
    logger.info(f"[LEANN] –§–∞–π–ª: {file_path} ({file_mb:.1f} MB)")
    
    # –ß–∞–Ω–∫—É–µ–º
    chunks = chunk_text(text, chunk_size=chunk_size)
    if max_chunks:
        chunks = chunks[:max_chunks]
    logger.info(f"[LEANN] –ß–∞–Ω–∫–æ–≤: {len(chunks)} (–ø–æ ~{chunk_size} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ RAM
    del text
    gc.collect()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
    if existing_texts is None:
        existing_texts, existing_embeddings, existing_scales = load_index(index_path)
    else:
        existing_scales = None  # –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω–æ –∏–∑–≤–Ω–µ
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
    existing_set = set(existing_texts)
    new_chunks = [c for c in chunks if c not in existing_set]
    del chunks
    gc.collect()
    
    if not new_chunks:
        logger.info("[LEANN] ‚úì –í—Å–µ —á–∞–Ω–∫–∏ —É–∂–µ –≤ –∏–Ω–¥–µ–∫—Å–µ. –ü—Ä–æ–ø—É—Å–∫.")
        return existing_texts, existing_embeddings, existing_scales
    
    logger.info(f"[LEANN] –ù–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤: {len(new_chunks)}")
    
    # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    if model is None:
        try:
            from sentence_transformers import SentenceTransformer
            model_path = str(ROOT / "models" / "embeddings")
            if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "config.json")):
                model = SentenceTransformer(model_path)
            else:
                model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("[LEANN] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SentenceTransformer")
        except ImportError:
            logger.warning("[LEANN] ‚ö† TF-IDF fallback")
    
    logger.info(f"[LEANN] –°–æ–∑–¥–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(new_chunks)} —á–∞–Ω–∫–æ–≤ (int8)...")
    
    if model:
        new_embs, new_scales = batch_embeddings(new_chunks, model, batch_size=256)
    else:
        results = [fallback_embedding(c) for c in new_chunks]
        new_embs = np.array([r[0] for r in results], dtype=np.int8)
        new_scales = np.array([r[1] for r in results], dtype=np.float32)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
    all_texts = existing_texts + new_chunks
    if existing_embeddings is not None and len(existing_embeddings) > 0:
        all_embeddings = np.vstack([existing_embeddings, new_embs])
        all_scales = np.concatenate([existing_scales, new_scales]) if existing_scales is not None else new_scales
    else:
        all_embeddings = new_embs
        all_scales = new_scales
    
    del new_chunks, new_embs, new_scales, existing_texts, existing_embeddings
    gc.collect()
    
    return all_texts, all_embeddings, all_scales


def ingest_all(data_dir: str = None, index_path: str = None,
               chunk_size: int = 500):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –í–°–ï —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏–∑ data/ –≤ LEANN.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥, –æ–±—â–∏–π –∏–Ω–¥–µ–∫—Å.
    """
    if data_dir is None:
        data_dir = str(ROOT / "data")
    if index_path is None:
        index_path = str(ROOT / "memory" / "leann.index")
    
    if not os.path.exists(data_dir):
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
        return
    
    logger.info("‚ïê" * 60)
    logger.info("  LEANN ‚Äî –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ø–∞–º—è—Ç—å")
    logger.info("‚ïê" * 60)
    
    txt_files = list(sorted(Path(data_dir).glob("*.txt")))
    
    if not txt_files:
        logger.info("‚ö† –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ data/")
        return
    
    total_mb = sum(f.stat().st_size for f in txt_files) / (1024 * 1024)
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(txt_files)} —Ñ–∞–π–ª–æ–≤ ({total_mb:.0f} MB)")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –û–î–ò–ù —Ä–∞–∑
    all_texts, all_embeddings, all_scales = load_index(index_path)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –û–î–ò–ù —Ä–∞–∑
    model = None
    try:
        from sentence_transformers import SentenceTransformer
        model_path = str(ROOT / "models" / "embeddings")
        if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "config.json")):
            model = SentenceTransformer(model_path)
        else:
            model = SentenceTransformer('all-MiniLM-L6-v2')
        logger.info("[LEANN] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SentenceTransformer")
    except ImportError:
        logger.warning("[LEANN] ‚ö† TF-IDF fallback")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã —Å –æ–±—â–∏–º –∏–Ω–¥–µ–∫—Å–æ–º
    for i, f in enumerate(txt_files):
        logger.info(f"\n[{i+1}/{len(txt_files)}] {f.name} ({f.stat().st_size / 1024 / 1024:.1f} MB)")
        all_texts, all_embeddings, all_scales = ingest_file(
            str(f), index_path=index_path,
            chunk_size=chunk_size,
            existing_texts=all_texts,
            existing_embeddings=all_embeddings,
            model=model
        )
        
        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 5 —Ñ–∞–π–ª–æ–≤
        if (i + 1) % 5 == 0 and all_embeddings is not None:
            save_index(index_path, all_texts, all_embeddings, all_scales)
            logger.info(f"[LEANN] –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ({len(all_texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    if all_embeddings is not None and len(all_texts) > 0:
        save_index(index_path, all_texts, all_embeddings, all_scales)
    
    # RAM –æ—Ç—á—ë—Ç
    if all_embeddings is not None:
        ram_mb = all_embeddings.nbytes / (1024 * 1024)
        logger.info(f"\n{'‚ïê' * 60}")
        logger.info(f"  ‚úÖ LEANN –≥–æ—Ç–æ–≤: {len(all_texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        logger.info(f"  üíæ RAM —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {ram_mb:.0f} MB (int8)")
        logger.info(f"  –¢–ê–†–° —Ç–µ–ø–µ—Ä—å –ø–æ–º–Ω–∏—Ç –≤—Å—ë –∏–∑ data/")
        logger.info(f"{'‚ïê' * 60}")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ LEANN –ø–∞–º—è—Ç—å –¢–ê–†–°")
    parser.add_argument("--file", type=str, default=None)
    parser.add_argument("--chunk-size", type=int, default=500)
    parser.add_argument("--index", type=str, default=None)
    parser.add_argument("--max-chunks", type=int, default=None)
    args = parser.parse_args()
    
    if args.file:
        texts, embs, scales = ingest_file(args.file, index_path=args.index,
                    chunk_size=args.chunk_size, max_chunks=args.max_chunks)
        if embs is not None:
            save_index(args.index or str(ROOT / "memory" / "leann.index"), texts, embs, scales)
    else:
        ingest_all(index_path=args.index, chunk_size=args.chunk_size)


if __name__ == "__main__":
    main()
