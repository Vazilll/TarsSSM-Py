"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Mamba-2 + RWKV-7 Training Script (Deep WuNeng Core)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–û–±—É—á–∞–µ—Ç TarsMamba2LM (TarsCoreBlock: SSD + WKV + WuNeng Fusion).

–§–∞–∑—ã:
  1. Pre-train –í–°–Å (TarsCoreBlock + Œ©-SSM + MoLE)
  2. Fine-tune WKV + Fusion (–∑–∞–º–æ—Ä–æ–∑–∏—Ç—å SSD params)
  3. Fine-tune MoLE + MatrixPool
  4. Fine-tune WKV fusion + RAG State Tracking

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
  python training/train_mamba2.py --phase 1 --epochs 3
  python training/train_mamba2.py --phase 4 --epochs 5 --data data/wiki_ru.txt
  python training/train_mamba2.py --phase 2 --pretrained models/mamba2/best.pt
"""

import os
import sys
import time
import json
import argparse
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path

ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT))

from brain.mamba2.model import TarsMamba2LM


def parse_args():
    p = argparse.ArgumentParser(description="–¢–ê–†–° Mamba-2 + RWKV-7 Training (Deep WuNeng Core)")
    # ‚ïê‚ïê‚ïê Model params ‚ïê‚ïê‚ïê
    p.add_argument('--d_model', type=int, default=2048)   # TARS 1B
    p.add_argument('--n_layers', type=int, default=24)    # 24 rich blocks
    p.add_argument('--vocab_size', type=int, default=256)  # cp1251 bytes
    # ‚ïê‚ïê‚ïê Training params ‚ïê‚ïê‚ïê
    p.add_argument('--seq_len', type=int, default=256)
    p.add_argument('--batch', type=int, default=8)
    p.add_argument('--accum_steps', type=int, default=4,
                   help="Gradient accumulation steps (effective batch = batch √ó accum)")
    p.add_argument('--epochs', type=int, default=5)
    p.add_argument('--lr', type=float, default=3e-4)
    p.add_argument('--label_smoothing', type=float, default=0.1)
    p.add_argument('--phase', type=int, default=1, 
                   help="1=pretrain all, 2=WKV+Fusion (freeze SSD), 3=MoLE+Pool, 4=WKV RAG")
    p.add_argument('--curriculum', action='store_true', default=True,
                   help="Enable curriculum learning (short‚Üílong sequences)")
    p.add_argument('--resume', action='store_true')
    p.add_argument('--pretrained', type=str, default=None, help="–ü—É—Ç—å –∫ –≤–µ—Å–∞–º –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è")
    p.add_argument('--data', type=str, default=None, help="–ü—É—Ç—å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∫–æ—Ä–ø—É—Å—É (.txt)")
    p.add_argument('--device', type=str, default='auto', help="cpu/cuda/auto")
    p.add_argument('--quant', action='store_true', help="–û–±—É—á–∞—Ç—å –≤ 1.58-bit —Ä–µ–∂–∏–º–µ (BitNet STE)")
    p.add_argument('--save_dir', type=str, default='models/mamba2')
    p.add_argument('--max_samples', type=int, default=0,
                   help="–ú–∞–∫—Å–∏–º—É–º –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (0 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)")
    p.add_argument('--pretrained_emb', type=str, default=None,
                   help="–ü—É—Ç—å –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–º—É —ç–º–±–µ–¥–¥–∏–Ω–≥—É (–∏–∑ MinGRU)")
    p.add_argument('--no_compile', action='store_true',
                   help="–û—Ç–∫–ª—é—á–∏—Ç—å torch.compile")
    # ‚ïê‚ïê‚ïê BitMamba-2 optimizations ‚ïê‚ïê‚ïê
    p.add_argument('--bf16', action='store_true',
                   help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bfloat16 AMP (–ª—É—á—à–µ fp16: —à–∏—Ä–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω)")
    p.add_argument('--grad_ckpt', action='store_true',
                   help="Gradient checkpointing (50-70%% —ç–∫–æ–Ω–æ–º–∏—è VRAM, —á—É—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
    p.add_argument('--muon', action='store_true',
                   help="Use Muon optimizer (2x faster than AdamW, 50%% less VRAM)")
    p.add_argument('--wsd', action='store_true',
                   help="Warmup-Stable-Decay scheduler (SmolLM2, better for long training)")
    p.add_argument('--mod', action='store_true',
                   help="Mixture of Depths: skip layers for easy tokens (-30%% compute)")
    p.add_argument('--no_wiki', action='store_true',
                   help="–ù–µ —Å–∫–∞—á–∏–≤–∞—Ç—å Wikipedia (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å)")
    return p.parse_args()


def load_corpus(data_path=None, download_wiki=True):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å.
    
    –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (–ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É):
      1. --data (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ñ–∞–π–ª)
      2. –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å (train_corpus.py: –¥–∏–∞–ª–æ–≥–∏ + —Ç–µ–∫—Å—Ç—ã)
      3. Wikipedia (auto-download 500 —Å—Ç–∞—Ç–µ–π –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
      4. TARS memories (data/tars_memories.json)
    """
    parts = []
    
    # 1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ—Ä–ø—É—Å (--data)
    if data_path and os.path.exists(data_path):
        with open(data_path, 'r', encoding='utf-8') as f:
            text = f.read()
        parts.append(text)
        print(f"[Data] Custom corpus: {len(text):,} —Å–∏–º–≤–æ–ª–æ–≤ ({data_path})")
    
    # 2. –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å (–¥–∏–∞–ª–æ–≥–∏ + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã)
    try:
        from training.train_corpus import get_training_text
        built_in = get_training_text()
        parts.append(built_in)
        print(f"[Data] –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å: {len(built_in):,} —Å–∏–º–≤–æ–ª–æ–≤")
    except ImportError:
        pass
    
    # 3. Wikipedia (auto-download)
    wiki_path = ROOT / "data" / "wiki_ru.txt"
    if wiki_path.exists():
        with open(wiki_path, 'r', encoding='utf-8') as f:
            wiki_text = f.read()
        if len(wiki_text) > 1000:
            parts.append(wiki_text)
            wiki_mb = len(wiki_text.encode('utf-8')) / (1024 * 1024)
            print(f"[Data] Wikipedia: {len(wiki_text):,} —Å–∏–º–≤–æ–ª–æ–≤ ({wiki_mb:.1f} MB, –∫–µ—à)")
    elif download_wiki:
        print("[Data] Wikipedia –∫–æ—Ä–ø—É—Å –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî —Å–∫–∞—á–∏–≤–∞—é 100 000 —Å—Ç–∞—Ç–µ–π...")
        print("[Data] –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 10-30 –º–∏–Ω—É—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ.")
        try:
            sys.path.insert(0, str(ROOT / "training"))
            from download_wiki import download_corpus
            wiki_text = download_corpus(count=10000)
            if wiki_text:
                parts.append(wiki_text)
                wiki_mb = len(wiki_text.encode('utf-8')) / (1024 * 1024)
                print(f"[Data] Wikipedia: {len(wiki_text):,} —Å–∏–º–≤–æ–ª–æ–≤ ({wiki_mb:.1f} MB)")
        except Exception as e:
            print(f"[Data] ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å Wikipedia: {e}")
            print("[Data]   –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Ä—É—á–Ω—É—é: python training/download_wiki.py")
    
    # 4. HuggingFace –¥–∞—Ç–∞—Å–µ—Ç—ã (data/hf_*.txt)
    hf_dir = ROOT / "data"
    if hf_dir.exists():
        hf_files = sorted(hf_dir.glob("hf_*.txt"))
        for hf_file in hf_files:
            try:
                with open(hf_file, 'r', encoding='utf-8') as f:
                    hf_text = f.read()
                if len(hf_text) > 1000:
                    parts.append(hf_text)
                    hf_mb = len(hf_text.encode('utf-8')) / (1024 * 1024)
                    print(f"[Data] HF {hf_file.stem}: {len(hf_text):,} —Å–∏–º–≤–æ–ª–æ–≤ ({hf_mb:.1f} MB)")
            except Exception:
                pass
    
    # 5. TARS memories
    memories_path = ROOT / "data" / "tars_memories.json"
    if memories_path.exists():
        try:
            with open(memories_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            texts = [e.get("text", e.get("memory", "")) for e in data if isinstance(e, dict)]
            parts.append("\n".join(texts))
            print(f"[Data] Memories: {len(texts)} –∑–∞–ø–∏—Å–µ–π")
        except Exception:
            pass
    
    corpus = "\n\n".join(parts)
    
    # 6. TARS Identity (self-description ‚Äî –º–æ–¥–µ–ª—å —É—á–∏—Ç —Å–≤–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
    identity_path = ROOT / "data" / "tars_identity.txt"
    if identity_path.exists():
        with open(identity_path, 'r', encoding='utf-8') as f:
            identity_text = f.read()
        if len(identity_text) > 100:
            # –ü–æ–≤—Ç–æ—Ä—è–µ–º 5x –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è ‚Äî –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∑–Ω–∞—Ç—å —Å–≤–æ—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
            identity_repeated = ("\n\n" + identity_text) * 5
            corpus = identity_repeated + "\n\n" + corpus
            print(f"[Data] –¢–ê–†–° Identity: {len(identity_text):,} —Å–∏–º–≤–æ–ª–æ–≤ (√ó5 –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è)")
    
    
    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –º–∞–ª–µ–Ω—å–∫–∏–π –∫–æ—Ä–ø—É—Å
    corpus_bytes = len(corpus.encode('utf-8'))
    if corpus_bytes < 200_000:
        repeat = max(1, 200_000 // corpus_bytes)
        corpus = ("\n\n" + corpus) * repeat
        print(f"[Data] –ö–æ—Ä–ø—É—Å –ø–æ–≤—Ç–æ—Ä—ë–Ω {repeat}x")
    
    print(f"[Data] –ò—Ç–æ–≥–æ: {len(corpus):,} —Å–∏–º–≤–æ–ª–æ–≤ ({corpus_bytes / 1024:.0f} KB)")
    return corpus


def prepare_byte_data(text: str, seq_len: int, vocab_size: int = 32000, max_samples: int = 0):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è byte-level –æ–±—É—á–µ–Ω–∏—è.
    –î–ª—è –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω—É–∂–µ–Ω SentencePiece tokenizer,
    –Ω–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º byte-level (vocab=256, –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ —á–µ—Ä–µ–∑ cp1251).
    """
    tokens = list(text.encode('cp1251', errors='replace'))
    
    # –ï—Å–ª–∏ max_samples –∑–∞–¥–∞–Ω, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
    if max_samples > 0:
        max_chars = max_samples * seq_len
        if len(tokens) > max_chars:
            tokens = tokens[:max_chars]
    
    inputs = []
    targets = []
    stride = seq_len // 2
    
    for i in range(0, len(tokens) - seq_len - 1, stride):
        inp = tokens[i:i + seq_len]
        tgt = tokens[i + 1:i + seq_len + 1]
        if len(inp) == seq_len and len(tgt) == seq_len:
            inputs.append(inp)
            targets.append(tgt)
        if max_samples > 0 and len(inputs) >= max_samples:
            break
    
    return (
        torch.tensor(inputs, dtype=torch.long),
        torch.tensor(targets, dtype=torch.long)
    )

def load_hf_weights(model, pretrained_dir):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ—ë–≤ –∏–∑ HuggingFace safetensors/bin"""
    pretrained_path = Path(pretrained_dir)
    found_weights = list(pretrained_path.glob("*.safetensors")) or list(pretrained_path.glob("*.bin"))
    if not found_weights:
        print(f"[Warning] –ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–µ—Å–∞ –≤ {pretrained_dir}")
        return

    print(f"[Model] –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤—ã—Ö –≤–µ—Å–æ–≤ Mamba-2 –∏–∑: {pretrained_dir}")
    try:
        from safetensors.torch import load_file
        state_dict = {}
        for f in found_weights:
            if f.suffix == '.safetensors':
                state_dict.update(load_file(f))
            else:
                state_dict.update(torch.load(f, map_location='cpu'))

        # –û—á–∏—Å—Ç–∫–∞ –∫–ª—é—á–µ–π HF (–ø—Ä–∏–º–µ—Ä–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è)
        mapped_dict = {}
        for k, v in state_dict.items():
            if 'embeddings' in k or 'backbone.embedding' in k:
                mapped_dict['embedding.weight'] = v
            elif 'layers' in k:
                # –ú–∞–ø–ø–∏–Ω–≥ —Å–ª–æ–µ–≤: model.layers.0 -> blocks.0.core
                new_k = k.replace('backbone.layers.', 'blocks.').replace('mixer.', 'core.')
                mapped_dict[new_k] = v
        
        info = model.load_state_dict(mapped_dict, strict=False)
        print(f"[Model] –ü–µ—Ä–µ–Ω–æ—Å—ã —É—Å–ø–µ—à–Ω—ã. –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å–ª–æ—ë–≤ TARS (IDME/MoLE/RWKV): {len(info.missing_keys)}")
    except Exception as e:
        print(f"[Error] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ pretrained: {e}")

def train(args):
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è."""
    
    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    if args.device == 'auto':
        device_str = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device_str = args.device
    device = torch.device(device_str)
    
    if device_str == 'cuda':
        print(f"[GPU] {torch.cuda.get_device_name(0)} "
              f"({torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB)")
    else:
        print("[CPU] GPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è")
    
    # –î–∞–Ω–Ω—ã–µ (–∫–æ—Ä–ø—É—Å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è 1 —Ä–∞–∑, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Äî –ø–æ curriculum)
    corpus = load_corpus(data_path=args.data, download_wiki=not args.no_wiki)
    print(f"[Data] –ö–æ—Ä–ø—É—Å –∑–∞–≥—Ä—É–∂–µ–Ω ({len(corpus):,} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –ú–æ–¥–µ–ª—å: 3-—à–∞–≥–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω
    #   Step A: –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ FP16 (–Ω–æ—Ä–º–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
    #   Step B: –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤ 1.58-bit (–µ—Å–ª–∏ --quant)
    #   Step C: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö (Wiki + HF + –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    actual_vocab = 256  # byte level
    
    # ‚îÄ‚îÄ Step A: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ FP16 ‚îÄ‚îÄ
    print(f"\n[Step A] –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ FP16...")
    model = TarsMamba2LM(
        d_model=args.d_model,
        n_layers=args.n_layers,
        vocab_size=actual_vocab,
        omega_dim=32,
        pool_size=48,
        n_experts=8,
        quant_mode="fp16",  # –í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–µ–º –≤ FP16
    )
    
    # Load Pretrained or Resume
    save_suffix = "_158bit" if args.quant else ""
    save_path = Path(args.save_dir) / f"mamba2_omega{save_suffix}.pt"
    if args.pretrained:
        load_hf_weights(model, args.pretrained)
        args.phase = 2

    if args.resume and save_path.exists():
        print(f"[Model] Loading checkpoint: {save_path}")
        checkpoint = torch.load(str(save_path), map_location='cpu', weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    elif args.resume and args.quant:
        # Quant mode but no 158bit checkpoint ‚Üí load FP16 as fallback
        fp16_path = Path(args.save_dir) / "mamba2_omega.pt"
        if fp16_path.exists():
            print(f"[Model] Quant: loading FP16 base ‚Üí {fp16_path}")
            checkpoint = torch.load(str(fp16_path), map_location='cpu', weights_only=False)
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            print(f"[!] No checkpoint found ‚Äî training from scratch")
    
    # ‚îÄ‚îÄ Gradient Checkpointing (BitMamba-2 style remat) ‚îÄ‚îÄ
    if args.grad_ckpt:
        model.use_checkpointing = True
        print("  ‚ö° Gradient checkpointing enabled (50-70% VRAM savings)")
    
    model.to(device)
    
    # ‚îÄ‚îÄ Transfer pre-trained embedding from MinGRU ‚îÄ‚îÄ
    if args.pretrained_emb and os.path.exists(args.pretrained_emb):
        print(f"\n[üîó Transfer] –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏–∑ MinGRU...")
        try:
            mingru_emb = torch.load(args.pretrained_emb, map_location=device)
            if mingru_emb.shape[0] == actual_vocab:
                # MinGRU dim might differ from Mamba-2 d_model
                if mingru_emb.shape[1] == args.d_model:
                    # Perfect match ‚Äî direct copy
                    model.embedding.weight.data.copy_(mingru_emb)
                    print(f"  ‚úî –ü—Ä—è–º–æ–π –ø–µ—Ä–µ–Ω–æ—Å ({mingru_emb.shape})")
                else:
                    # Dimension mismatch ‚Äî use projection (truncate or pad)
                    min_dim = min(mingru_emb.shape[1], args.d_model)
                    model.embedding.weight.data[:, :min_dim] = mingru_emb[:, :min_dim]
                    print(f"  ‚úî –ß–∞—Å—Ç–∏—á–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å ({mingru_emb.shape} ‚Üí {model.embedding.weight.shape})")
        except Exception as e:
            print(f"  ‚ö† Embedding transfer failed: {e}")
    
    params = model.count_parameters()
    total = params["total"]
    print(f"[Model] TarsMamba2LM: {total:,} params ({total/1e6:.1f}M), mode=FP16")
    
    # ‚îÄ‚îÄ Step B: –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤ 1.58-bit (–µ—Å–ª–∏ --quant) ‚îÄ‚îÄ
    quant_mode = "fp16"
    if args.quant:
        print(f"\n[Step B] –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è FP16 -> 1.58-bit...")
        try:
            from brain.mamba2.bitnet import convert_model_to_158bit, model_stats
            convert_model_to_158bit(model)
            stats = model_stats(model)
            quant_mode = "158bit"
            print(f"  –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {stats['total_layers']} —Å–ª–æ—ë–≤ UniversalLinear")
            print(f"  –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å: {stats['avg_sparsity']:.1%}")
            print(f"  –û–±—É—á–µ–Ω–∏–µ —Å STE (Straight-Through Estimator)")
        except Exception as e:
            print(f"  [!] –ù–µ —É–¥–∞–ª–æ—Å—å –∫–≤–∞–Ω—Ç–æ–≤–∞—Ç—å: {e}")
            print(f"  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è –≤ FP16")
    else:
        print(f"\n[Step B] –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ (FP16 —Ä–µ–∂–∏–º)")
    
    print(f"\n[Step C] –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö ({quant_mode} —Ä–µ–∂–∏–º)...")
    print(f"  –î–∞–Ω–Ω—ã–µ: Wikipedia + HuggingFace + –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å")
    for k, v in params.items():
        if k != "total" and isinstance(v, int) and v > 0:
            print(f"  {k}: {v:,}")
        elif k == "block_detail (√ó1)" and isinstance(v, dict):
            for bk, bv in v.items():
                print(f"    {bk}: {bv:,}")
    
    # ‚ïê‚ïê‚ïê –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è ‚ïê‚ïê‚ïê
    if args.phase == 1:
        # Phase 1: Pre-train ALL (TarsCoreBlock + Œ©-SSM + MoLE)
        print("[Phase 1] Full pre-training: ALL components")
        if getattr(args, 'muon', False):
            from training.muon import Muon
            optimizer = Muon(model.parameters(), lr=0.02, weight_decay=0.01)
            print("  ‚ö° Optimizer: Muon (2x faster, NS orthogonalization)")
        else:
            optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)
    elif args.phase == 2:
        # Phase 2: Freeze SSD-specific params. Train: WKV + Fusion + Œ©-SSM + MoLE + NoveltyGate
        print("[Phase 2] Fine-tune: WKV + Fusion + Œ©-SSM + MoLE + NoveltyGate (SSD params frozen)")
        for block in model.blocks:
            core = block.core
            # Freeze SSD-specific parameters
            core.A_log.requires_grad = False
            core.D.requires_grad = False
            core.dt_bias.requires_grad = False
            for p in core.conv1d.parameters():
                p.requires_grad = False
        trainable = [p for p in model.parameters() if p.requires_grad]
        print(f"  Trainable: {sum(p.numel() for p in trainable):,} params")
        optimizer = torch.optim.AdamW(trainable, lr=args.lr * 0.1, weight_decay=0.01)
    elif args.phase == 3:
        # Phase 3: MoLE + MatrixPool + WaveMerge + WaveGate + NoveltyGate
        print("[Phase 3] Fine-tune: MoLE + MatrixPool + WaveMerge + WaveGate + NoveltyGate")
        for p in model.parameters():
            p.requires_grad = False
        for block in model.blocks:
            for p in block.mole.parameters():
                p.requires_grad = True
            # NoveltyGate ‚Äî –æ–±—É—á–∞–µ–º—ã–π –ø–æ—Ä–æ–≥ (Tars.txt ¬ß6.1)
            for p in block.novelty_gate.parameters():
                p.requires_grad = True
        for p in model.matrix_pool.parameters():
            p.requires_grad = True
        # WaveConsolidation ‚Äî –æ–±—É—á–∞–µ–º—ã–µ (Tars.txt ¬ß1.4)
        for wc in model.wave_consolidations:
            for p in wc.parameters():
                p.requires_grad = True
        # Model-level NoveltyGate (IDME branch-and-bound –≤ think())
        for p in model.novelty_gate.parameters():
            p.requires_grad = True
        # norm_f ‚Äî –¥–æ–ª–∂–µ–Ω –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø—Ä–∏ —Å–º–µ–Ω–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
        for p in model.norm_f.parameters():
            p.requires_grad = True
        trainable = [p for p in model.parameters() if p.requires_grad]
        print(f"  Trainable: {sum(p.numel() for p in trainable):,} params")
        optimizer = torch.optim.AdamW(trainable, lr=args.lr * 0.01, weight_decay=0.01)
    elif args.phase == 4:
        # Phase 4: WKV fusion + RAG + Dynamic Memory Injection + NoveltyGate + Œ©-SSM
        print("[Phase 4] Fine-tune: WKV + RAG + Dynamic Memory + NoveltyGate + Œ©-SSM")
        for p in model.parameters():
            p.requires_grad = False
        for block in model.blocks:
            core = block.core
            # WKV-related: fusion gate, wkv_up, time_mix
            for p in core.fusion_gate.parameters():
                p.requires_grad = True
            for p in core.wkv_up.parameters():
                p.requires_grad = True
            core.time_mix.requires_grad = True
            # RAG projections
            for p in block.rag_query.parameters():
                p.requires_grad = True
            for p in block.rag_out.parameters():
                p.requires_grad = True
            # Dynamic Memory Injection (Tars.txt ¬ß4.3): mem_query, mem_proj, mem_gate
            for p in block.mem_query_proj.parameters():
                p.requires_grad = True
            for p in block.mem_proj.parameters():
                p.requires_grad = True
            for p in block.mem_gate.parameters():
                p.requires_grad = True
            # NoveltyGate
            for p in block.novelty_gate.parameters():
                p.requires_grad = True
            # Œ©-SSM (Cayley transform parameters)
            for p in block.omega.parameters():
                p.requires_grad = True
        # ‚ïê‚ïê‚ïê Model-level spine projections (Tars.txt ¬ß2.4) ‚ïê‚ïê‚ïê
        # –ë–µ–∑ —ç—Ç–æ–≥–æ to_memory_space/from_memory_space –Ω–µ —É—á–∞—Ç—Å—è –≤ Phase 4
        for p in model.to_memory_space.parameters():
            p.requires_grad = True
        for p in model.from_memory_space.parameters():
            p.requires_grad = True
        # WaveConsolidation ‚Äî —Å—É–º–º–∏—Ä—É—é—â–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –º–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏
        for wc in model.wave_consolidations:
            for p in wc.parameters():
                p.requires_grad = True
        # Model-level NoveltyGate (IDME) + norm_f
        for p in model.novelty_gate.parameters():
            p.requires_grad = True
        for p in model.norm_f.parameters():
            p.requires_grad = True
        trainable = [p for p in model.parameters() if p.requires_grad]
        print(f"  Trainable: {sum(p.numel() for p in trainable):,} params")
        optimizer = torch.optim.AdamW(trainable, lr=args.lr * 0.05, weight_decay=0.01)
    
    # Scheduler (estimate total steps from corpus size)
    estimated_samples = max(100, len(corpus.encode('cp1251', errors='replace')) // max(args.seq_len // 2, 1))
    total_steps = (estimated_samples // args.batch + 1) * args.epochs
    warmup = total_steps // 10
    
    if getattr(args, 'wsd', False):
        # ‚ïê‚ïê‚ïê WSD: Warmup-Stable-Decay (SmolLM2/MiniCPM) ‚ïê‚ïê‚ïê
        stable_end = int(total_steps * 0.7)  # 70% at stable LR
        def lr_fn(step):
            if step < warmup:
                return step / max(warmup, 1)
            elif step < stable_end:
                return 1.0  # stable phase ‚Äî full LR
            else:
                # decay phase: linear decay to 0.1
                progress = (step - stable_end) / max(total_steps - stable_end, 1)
                return max(0.1, 1.0 - 0.9 * progress)
        print("  ‚ö° Scheduler: WSD (warmup ‚Üí stable ‚Üí decay)")
    else:
        # Standard cosine annealing
        def lr_fn(step):
            if step < warmup:
                return step / max(warmup, 1)
            progress = (step - warmup) / max(total_steps - warmup, 1)
            return 0.5 * (1.0 + np.cos(np.pi * progress))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_fn)
    
    # ‚ïê‚ïê‚ïê Mixture of Depths (-30% compute) ‚ïê‚ïê‚ïê
    if getattr(args, 'mod', False):
        from brain.mamba2.mixture_of_depths import add_mod_to_model
        model = add_mod_to_model(model, capacity_factor=0.5)
    
    # ‚ïê‚ïê‚ïê torch.compile for 30% speedup ‚ïê‚ïê‚ïê
    if not args.no_compile and hasattr(torch, 'compile'):
        try:
            model = torch.compile(model, mode='reduce-overhead')
            print("  ‚ö° torch.compile: enabled (reduce-overhead mode)")
        except Exception as e:
            print(f"  ‚ö† torch.compile failed: {e} ‚Äî continuing without")
    
    # ‚ïê‚ïê‚ïê AMP (BitMamba-2: bfloat16 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–∞ TPU/GPU) ‚ïê‚ïê‚ïê
    use_amp = device.type == 'cuda'
    if use_amp and args.bf16:
        amp_dtype = torch.bfloat16
        scaler = None  # bfloat16 –Ω–µ –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ GradScaler
        print("  ‚ö° AMP: bfloat16 (no scaler needed, wider dynamic range)")
    elif use_amp:
        amp_dtype = torch.float16
        scaler = torch.amp.GradScaler('cuda')
        print("  ‚ö° AMP: float16 + GradScaler")
    else:
        amp_dtype = torch.float32
        scaler = None
    
    best_loss = float('inf')
    
    print(f"\n{'‚ïê'*60}")
    print(f"  –¢–ê–†–° Mamba-2 Training ‚Äî Phase {args.phase}")
    print(f"  Epochs: {args.epochs} | Batch: {args.batch} | LR: {args.lr}")
    print(f"{'‚ïê'*60}\n")
    
    # ‚ïê‚ïê‚ïê torch.compile (30-50% —É—Å–∫–æ—Ä–µ–Ω–∏–µ) ‚ïê‚ïê‚ïê
    compiled_model = None
    if hasattr(torch, 'compile') and not args.no_compile and device.type == 'cuda':
        try:
            compiled_model = torch.compile(model, mode="reduce-overhead")
            print("  ‚ö° torch.compile enabled (reduce-overhead mode)")
        except Exception as e:
            print(f"  ‚ö† torch.compile failed: {e}")
    forward_model = compiled_model if compiled_model is not None else model
    
    # ‚ïê‚ïê‚ïê Curriculum Learning ‚ïê‚ïê‚ïê
    curriculum_schedule = None
    if args.curriculum and args.epochs >= 3:
        # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º seq_len: 64 ‚Üí 128 ‚Üí 256 ‚Üí target
        max_sl = args.seq_len
        steps = [max(64, max_sl // 4), max(128, max_sl // 2), max_sl]
        epoch_per_step = max(1, args.epochs // len(steps))
        curriculum_schedule = []
        for sl in steps:
            curriculum_schedule.extend([sl] * epoch_per_step)
        while len(curriculum_schedule) < args.epochs:
            curriculum_schedule.append(max_sl)
        print(f"  üìö Curriculum: {' ‚Üí '.join(str(s) for s in dict.fromkeys(curriculum_schedule))}")
    
    accum_steps = max(1, args.accum_steps)
    print(f"  Effective batch: {args.batch} √ó {accum_steps} = {args.batch * accum_steps}")
    
    for epoch in range(args.epochs):
        t0 = time.time()
        model.train()
        total_loss = 0
        n_batches = 0
        tokens_processed = 0
        
        # Curriculum: –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å –Ω–æ–≤—ã–º seq_len
        cur_seq_len = curriculum_schedule[epoch] if curriculum_schedule else args.seq_len
        if epoch == 0 or (curriculum_schedule and cur_seq_len != curriculum_schedule[max(0, epoch-1)]):
            c_inputs, c_targets = prepare_byte_data(corpus, cur_seq_len, actual_vocab, max_samples=args.max_samples)
            n_test_c = max(2, len(c_inputs) // 10)
            c_train_in, c_test_in = c_inputs[:-n_test_c], c_inputs[-n_test_c:]
            c_train_tgt, c_test_tgt = c_targets[:-n_test_c], c_targets[-n_test_c:]
            print(f"  seq_len={cur_seq_len}, samples={len(c_train_in)}")
        
        perm = torch.randperm(len(c_train_in))
        train_in_s = c_train_in[perm]
        train_tgt_s = c_train_tgt[perm]
        
        optimizer.zero_grad()
        
        for i in range(0, len(train_in_s), args.batch):
            batch_in = train_in_s[i:i+args.batch].to(device)
            batch_tgt = train_tgt_s[i:i+args.batch].to(device)
            
            if use_amp:
                with torch.amp.autocast('cuda', dtype=amp_dtype):
                    logits = forward_model(batch_in)
                    lm_loss = F.cross_entropy(
                        logits.view(-1, actual_vocab),
                        batch_tgt.view(-1),
                        label_smoothing=args.label_smoothing,
                    )
                    # MoLE auxiliary loss (load balancing + z-loss)
                    loss = (lm_loss + model.mole_aux_loss) / accum_steps
                if scaler is not None:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()  # bfloat16: no scaler needed
            else:
                logits = forward_model(batch_in)
                lm_loss = F.cross_entropy(
                    logits.view(-1, actual_vocab),
                    batch_tgt.view(-1),
                    label_smoothing=args.label_smoothing,
                )
                # MoLE auxiliary loss (load balancing + z-loss)
                loss = (lm_loss + model.mole_aux_loss) / accum_steps
                loss.backward()
            
            total_loss += loss.item() * accum_steps
            n_batches += 1
            tokens_processed += batch_in.numel()
            
            # Gradient accumulation step
            if n_batches % accum_steps == 0 or i + args.batch >= len(train_in_s):
                if scaler is not None:
                    scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                if scaler is not None:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad()
                scheduler.step()
        
        # Eval
        model.eval()
        with torch.no_grad():
            eval_losses = []
            for j in range(0, len(c_test_in), args.batch):
                test_batch = c_test_in[j:j+args.batch].to(device)
                test_tgt_batch = c_test_tgt[j:j+args.batch].to(device)
                logits = forward_model(test_batch)
                el = F.cross_entropy(
                    logits.view(-1, actual_vocab),
                    test_tgt_batch.view(-1)
                ).item()
                eval_losses.append(el)
            eval_loss = np.mean(eval_losses)
        
        elapsed = time.time() - t0
        ppl = np.exp(min(eval_loss, 20))
        tok_per_sec = tokens_processed / max(elapsed, 0.01)
        
        print(f"Epoch {epoch+1}/{args.epochs} | "
              f"Train: {total_loss/max(n_batches,1):.4f} | "
              f"Eval: {eval_loss:.4f} | PPL: {ppl:.1f} | "
              f"{tok_per_sec:.0f} tok/s | {elapsed:.1f}s")
        
        # Save checkpoint
        os.makedirs(args.save_dir, exist_ok=True)
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'config': {
                'd_model': args.d_model,
                'n_layers': args.n_layers,
                'vocab_size': actual_vocab,
                'quant_mode': quant_mode,
            },
            'd_model': args.d_model,
            'n_layers': args.n_layers,
            'vocab_size': actual_vocab,
            'epoch': epoch,
            'eval_loss': eval_loss,
            'phase': args.phase,
            'quant_mode': quant_mode,
        }
        
        # Per-epoch checkpoint (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø—Ä–∏ –∫—Ä–∞—à–∞—Ö)
        epoch_path = save_path.with_suffix(f'.epoch{epoch+1}.pt')
        torch.save(checkpoint, str(epoch_path))
        
        # Best checkpoint
        if eval_loss < best_loss:
            best_loss = eval_loss
            torch.save(checkpoint, str(save_path))
            print(f"  ‚úì Best saved: {save_path} ({quant_mode})")
        else:
            print(f"  ‚Ü∫ Epoch checkpoint: {epoch_path.name}")
    
    print(f"\n{'‚ïê'*60}")
    print(f"  Done! Best eval loss: {best_loss:.4f}")
    print(f"  Weights: {save_path}")
    print(f"{'‚ïê'*60}")


if __name__ == "__main__":
    args = parse_args()
    train(args)
