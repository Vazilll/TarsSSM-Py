"""
train_all.py ‚Äî TARS v3 Unified Training Pipeline.

–û–±—É—á–∞–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:
  Phase 0: OmegaCore C++ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
  Phase 1: Reflex Classifier (30 —ç–ø–æ—Ö) ‚Äî ~3 –º–∏–Ω, CPU
  Phase 2: MinGRU Language Model (20 —ç–ø–æ—Ö + HF –¥–∞–Ω–Ω—ã–µ) ‚Äî ~1.5-2—á, CPU
  Phase 3: Mamba-2 Brain 1.58-bit (–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ):
            Phase 1‚Üí Full pretrain (2 —ç–ø–æ—Ö–∏) ‚Üí ~3-4—á
            Phase 2‚Üí Fine-tune WKV+Fusion (1 —ç–ø–æ—Ö–∞) ‚Üí ~1.5-2—á
  Phase 4: Whisper Vocabulary Boost ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ STT

Usage:
    python training/train_all.py                     # –í—Å—ë
    python training/train_all.py --only reflex       # –¢–æ–ª—å–∫–æ —Ä–µ—Ñ–ª–µ–∫—Å—ã
    python training/train_all.py --only mamba2       # –¢–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–∑–≥
    python training/train_all.py --only mingru       # –¢–æ–ª—å–∫–æ MinGRU
    python training/train_all.py --device cuda       # GPU mode
    python training/train_all.py --data data/wiki.txt  # –°–≤–æ–π –∫–æ—Ä–ø—É—Å
    python training/train_all.py --phase 2           # –¢–æ–ª—å–∫–æ Phase 2 Mamba-2
"""
import argparse
import logging
import time
import subprocess
import sys
import os
import json
import shutil
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)
logger = logging.getLogger("TrainAll")

# –ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
TRAINING = os.path.dirname(os.path.abspath(__file__))
PYTHON = sys.executable
TARS_V3_DIR = os.path.join(ROOT, "models", "tars_v3")


def consolidate_models(results: dict, total_time: float):
    """–ö–æ–ø–∏—Ä—É–µ—Ç –≤—Å–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –≤ models/tars_v3/ –∏ –ø–∏—à–µ—Ç training_log.json."""
    os.makedirs(TARS_V3_DIR, exist_ok=True)
    
    # –ú–∞–ø–ø–∏–Ω–≥: –æ—Ç–∫—É–¥–∞ ‚Üí –∫—É–¥–∞
    copies = {
        "reflex": (os.path.join(ROOT, "models", "reflex", "reflex_classifier.pt"),
                   os.path.join(TARS_V3_DIR, "reflex.pt")),
        "mingru": (os.path.join(ROOT, "models", "mingru_weights.pt"),
                   os.path.join(TARS_V3_DIR, "mingru.pt")),
        "mamba2": (os.path.join(ROOT, "models", "mamba2", "mamba2_omega_158bit.pt"),
                   os.path.join(TARS_V3_DIR, "mamba2.pt")),
    }
    
    copied = []
    for name, (src, dst) in copies.items():
        if os.path.exists(src):
            shutil.copy2(src, dst)
            size_mb = os.path.getsize(dst) / (1024 * 1024)
            logger.info(f"üì¶ {name}: {os.path.basename(src)} ‚Üí tars_v3/{os.path.basename(dst)} ({size_mb:.1f} MB)")
            copied.append(name)
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –ª–æ–≥ –æ–±—É—á–µ–Ω–∏—è
    log_path = os.path.join(TARS_V3_DIR, "training_log.json")
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "total_time_sec": round(total_time, 1),
        "results": {k: ("ok" if v else "failed") for k, v in results.items()},
        "models_consolidated": copied,
        "encoding": "cp1251",
        "vocab_size": 256,
    }
    
    # Append to existing log
    logs = []
    if os.path.exists(log_path):
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        except Exception:
            pass
    if not isinstance(logs, list):
        logs = []
    logs.append(log_entry)
    
    with open(log_path, 'w', encoding='utf-8') as f:
        json.dump(logs, f, ensure_ascii=False, indent=2)
    
    logger.info(f"üìã Training log: {log_path}")


def run_script(script: str, extra_args: list = None, cwd: str = None):
    """Run a training script as subprocess with retry for Windows Defender/Launcher bugs."""
    cmd = [PYTHON, script] + (extra_args or [])
    logger.info(f"‚ñ∂ {' '.join(cmd)}")
    t0 = time.time()
    
    # Retry logic for Windows process creation bugs
    for attempt in range(3):
        try:
            result = subprocess.run(cmd, cwd=cwd or ROOT)
            
            # Python launcher bug on Windows ("Unable to create process") returns 101
            if result.returncode == 101:
                if attempt < 2:
                    logger.warning(f"‚ö† –û—à–∏–±–∫–∞ –ª–∞—É–Ω—á–µ—Ä–∞ (–∫–æ–¥ 101), –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 3 —Å–µ–∫... ({attempt+1}/3)")
                    time.sleep(3)
                    continue
                else:
                    logger.error(f"‚ùå {os.path.basename(script)} ‚Äî –û—à–∏–±–∫–∞ –ª–∞—É–Ω—á–µ—Ä–∞ –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫")
                    return False
            break
        except PermissionError:
            if attempt < 2:
                logger.warning(f"‚ö† PermissionError (–∞–Ω—Ç–∏–≤–∏—Ä—É—Å?), –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 3 —Å–µ–∫... ({attempt+1}/3)")
                time.sleep(3)
            else:
                logger.error(f"‚ùå {os.path.basename(script)} ‚Äî PermissionError –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫")
                return False
    
    elapsed = time.time() - t0
    if result.returncode == 0:
        logger.info(f"‚úÖ {os.path.basename(script)} ‚Üí {elapsed:.1f}s")
    else:
        logger.error(f"‚ùå {os.path.basename(script)} failed (code {result.returncode})")
    return result.returncode == 0


def build_omega_core():
    """Phase 0: Compile OmegaCore C++ kernel (optional)."""
    logger.info("‚ïê" * 60)
    logger.info("PHASE 0: Building OmegaCore C++ Kernel")
    logger.info("‚ïê" * 60)
    ps1 = os.path.join(ROOT, "brain", "omega_core", "build_omega.ps1")
    if not os.path.exists(ps1):
        logger.warning("build_omega.ps1 not found ‚Äî skipping C++ build")
        return True
    result = subprocess.run(
        ["powershell", "-ExecutionPolicy", "Bypass", "-File", ps1],
        cwd=ROOT
    )
    return result.returncode == 0


def train_reflex(args):
    """Phase 1: Train Tier-1 Reflex Classifier (~30s, CPU)."""
    logger.info("‚ïê" * 60)
    logger.info("PHASE 1: Reflex Classifier (MinGRU Intent)")
    logger.info("‚ïê" * 60)
    return run_script(
        os.path.join(TRAINING, "train_reflex.py"),
        ["--epochs", str(args.reflex_epochs), "--lr", str(args.reflex_lr)]
    )


def train_mingru(args):
    """Phase 2: Train MinGRU LM for fast responses."""
    logger.info("‚ïê" * 60)
    logger.info("PHASE 2: MinGRU Language Model (System 1)")
    logger.info("  + HuggingFace augmented data for better quality")
    logger.info("‚ïê" * 60)
    extra = [
        "--epochs", str(args.mingru_epochs),
        "--lr", str(args.mingru_lr),
        "--augment",  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–¥–∫–∞—á–∫—É —Å HuggingFace!
    ]
    # –£—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ CPU –¥–ª—è –Ω–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ (–º–∞–∫—Å. –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞ ~1.5 —á–∞—Å–∞)
    if args.device == "cpu":
        extra += [
            "--dim", "512",       # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 512 –¥–ª—è System 1
            "--layers", "6",      # –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
            "--batch", "8",       # –≠–∫–æ–Ω–æ–º–∏—è RAM, –Ω–æ –≤—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            "--seq_len", "256",   # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
            "--max_samples", "15000", # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ 15 000 –ø—Ä–∏–º–µ—Ä–æ–≤
        ]
    # train_mingru.py auto-detects CUDA, no --device flag
    return run_script(os.path.join(TRAINING, "train_mingru.py"), extra)


def train_mamba2(args):
    """Phase 3: Mamba-2 Brain ‚Äî Progressive 1.58-bit Training.
    
    Step A: FP16 init
    Step B: Quantize -> 1.58-bit
    Step C: Phase 1 (full pretrain) + Phase 2 (fine-tune WKV/Fusion)
    """
    logger.info("‚ïê" * 60)
    logger.info("PHASE 3: Mamba-2 Brain ‚Äî Progressive 1.58-bit")
    logger.info("  Phase 1: Full pretrain (all params)")
    logger.info("  Phase 2: Fine-tune WKV + Fusion (SSD frozen)")
    logger.info("‚ïê" * 60)
    
    base_extra = [
        "--d_model", str(args.d_model),
        "--n_layers", str(args.n_layers),
        "--batch", "8",       # –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –±–∞—Ç—á –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        "--seq_len", "256",   # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è CPU
        "--max_samples", "50000", # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ—Ä–ø—É—Å 50 000 –ø—Ä–∏–º–µ—Ä–∞–º–∏ (–≤–º–µ—Å—Ç–æ 1–ú)
        "--quant",            # 1.58-bit —Ä–µ–∂–∏–º
    ]
    if args.device != "cpu":
        base_extra += ["--device", args.device]
    if args.data:
        base_extra += ["--data", args.data]
    if args.pretrained:
        base_extra += ["--pretrained", args.pretrained]
    
    # ‚ïê‚ïê‚ïê Transfer embedding from MinGRU ‚Üí Mamba-2 ‚ïê‚ïê‚ïê
    mingru_weights = os.path.join(ROOT, "models", "mingru_weights.pt")
    if os.path.exists(mingru_weights):
        logger.info("üîó Transferring MinGRU embedding ‚Üí Mamba-2 (shared cp1251 matrix)")
        try:
            import torch
            cp = torch.load(mingru_weights, map_location='cpu', weights_only=False)
            state = cp.get('model_state_dict', cp)
            # MinGRU stores shared embedding as shared_embedding.weight
            emb_key = None
            for k in state:
                if 'shared_embedding' in k or 'emb.weight' in k:
                    emb_key = k
                    break
            if emb_key:
                emb_tensor = state[emb_key]
                emb_path = os.path.join(TARS_V3_DIR, "_transfer_embedding.pt")
                os.makedirs(TARS_V3_DIR, exist_ok=True)
                torch.save(emb_tensor, emb_path)
                logger.info(f"  Saved embedding ({emb_tensor.shape}) ‚Üí {emb_path}")
                # Mamba-2 will pick this up via --pretrained-emb flag
                base_extra += ["--pretrained_emb", emb_path]
        except Exception as e:
            logger.warning(f"  Embedding transfer failed: {e}")
    
    # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ñ–∞–∑–∞ ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—ë
    if args.phase:
        extra = base_extra + [
            "--epochs", str(args.mamba_epochs),
            "--lr", str(args.mamba_lr),
            "--phase", str(args.phase),
        ]
        return run_script(os.path.join(TRAINING, "train_mamba2.py"), extra)
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: Phase 1 -> Phase 2
    # Phase 1: Full pretrain (2 —ç–ø–æ—Ö–∏, –ø–æ–ª–Ω—ã–π LR)
    logger.info("‚îÄ‚îÄ Phase 1/2: Full pretrain ‚îÄ‚îÄ")
    extra1 = base_extra + [
        "--epochs", str(max(args.mamba_epochs - 1, 1)),
        "--lr", str(args.mamba_lr),
        "--phase", "1",
    ]
    ok1 = run_script(os.path.join(TRAINING, "train_mamba2.py"), extra1)
    
    # Phase 2: Fine-tune WKV + Fusion (1 —ç–ø–æ—Ö–∞, –ø–æ–Ω–∏–∂–µ–Ω–Ω—ã–π LR, --resume)
    logger.info("‚îÄ‚îÄ Phase 2/2: Fine-tune WKV + Fusion ‚îÄ‚îÄ")
    extra2 = base_extra + [
        "--epochs", "1",
        "--lr", str(args.mamba_lr * 0.3),  # –ù–∏–∂–µ LR –¥–ª—è fine-tune
        "--phase", "2",
        "--resume",          # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞ Phase 1
    ]
    ok2 = run_script(os.path.join(TRAINING, "train_mamba2.py"), extra2)
    
    return ok1 and ok2


def whisper_boost(args):
    """Phase 4: Build Whisper vocabulary boost from corpus."""
    logger.info("‚ïê" * 60)
    logger.info("PHASE 4: Whisper Vocabulary Boost")
    logger.info("‚ïê" * 60)
    return run_script(os.path.join(TRAINING, "whisper_boost.py"))


def quantize(args):
    """Phase 5: BitNet 1.58-bit quantization + fine-tune."""
    logger.info("‚ïê" * 60)
    logger.info("PHASE 5: BitNet 1.58-bit Quantization + Fine-Tune")
    logger.info("‚ïê" * 60)
    extra = ["--epochs", "2"]
    if args.device != "cpu":
        extra += ["--device", args.device]
    return run_script(os.path.join(TRAINING, "quantize_models.py"), extra)


def main():
    parser = argparse.ArgumentParser(
        description="TARS v3 Unified Training Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python training/train_all.py                          # Full pipeline
  python training/train_all.py --only mamba2 --phase 1  # Mamba-2 pre-train
  python training/train_all.py --only mamba2 --phase 4  # WKV RAG fine-tune
  python training/train_all.py --data data/wiki_ru.txt  # Train on Wikipedia
  python training/train_all.py --device cuda --mamba-epochs 10
        """
    )
    
    # What to train
    parser.add_argument("--only", choices=["reflex", "mingru", "mamba2", "quantize"],
                        help="Train only one component")
    parser.add_argument("--phase", type=int, choices=[1, 2, 3, 4],
                        help="Mamba-2 hybrid training phase (1=all, 2=WKV+Fusion, 3=MoLE, 4=RAG)")
    
    # Hardware
    parser.add_argument("--device", default="auto", choices=["auto", "cpu", "cuda"])
    
    # Data
    parser.add_argument("--data", type=str, default=None,
                        help="Path to text corpus (.txt)")
    parser.add_argument("--pretrained", type=str, default=None,
                        help="Path to pretrained weights for fine-tuning")
    
    # Reflex params
    parser.add_argument("--reflex-epochs", type=int, default=50) # –ú–∞–∫—Å–∏–º—É–º —Ç–æ—á–Ω–æ—Å—Ç–∏
    parser.add_argument("--reflex-lr", type=float, default=0.002)
    
    # MinGRU params
    parser.add_argument("--mingru-epochs", type=int, default=15) # –ò —Ç–∞–∫ —É—á–∏—Ç—Å—è –±—ã—Å—Ç—Ä–æ —Å 15–∫ –ø—Ä–∏–º–µ—Ä–∞–º–∏
    parser.add_argument("--mingru-lr", type=float, default=3e-3)
    
    # Mamba-2 params
    parser.add_argument("--mamba-epochs", type=int, default=2) # 2 —ç–ø–æ—Ö–∏ –Ω–∞ 50–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ (~2 —á–∞—Å–∞)
    parser.add_argument("--mamba-lr", type=float, default=3e-4)
    parser.add_argument("--d_model", type=int, default=256,
                        help="Model dimension (128=test, 256=demo, 768=full)")
    parser.add_argument("--n_layers", type=int, default=4,
                        help="Number of TarsBlocks (2=test, 4=demo, 12=full)")
    
    # Extras
    parser.add_argument("--skip-omega", action="store_true", default=True,
                        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∫–æ–º–ø–∏–ª—è—Ü–∏—é OmegaCore (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è)")
    parser.add_argument("--build-omega", action="store_true",
                        help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å OmegaCore C++ —è–¥—Ä–æ")
    parser.add_argument("--skip-quantize", action="store_true")
    
    args = parser.parse_args()
    
    # Auto-detect device
    if args.device == "auto":
        try:
            import torch
            args.device = "cuda" if torch.cuda.is_available() else "cpu"
        except ImportError:
            args.device = "cpu"
    
    logger.info("‚ïî" + "‚ïê" * 58 + "‚ïó")
    logger.info("‚ïë   TARS v3 Training Pipeline (Deep WuNeng Core)          ‚ïë")
    logger.info("‚ïö" + "‚ïê" * 58 + "‚ïù")
    logger.info(f"  Device:     {args.device}")
    logger.info(f"  Components: {'ALL' if not args.only else args.only.upper()}")
    logger.info(f"  Data:       {args.data or 'built-in corpus'}")
    if args.phase:
        logger.info(f"  Phase:      {args.phase}")
    logger.info("")
    
    t0 = time.time()
    results = {}
    
    # Phase 0: OmegaCore (optional, requires Zig)
    if args.build_omega and args.only is None:
        results["omega"] = build_omega_core()
    
    # Phase 1: Reflex
    if args.only in (None, "reflex"):
        results["reflex"] = train_reflex(args)
    
    # Phase 2: MinGRU
    if args.only in (None, "mingru"):
        results["mingru"] = train_mingru(args)
    
    # Phase 3: Mamba-2 + RWKV-7 (1.58-bit ‚Äî —Å—Ä–∞–∑—É –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
    if args.only in (None, "mamba2"):
        results["mamba2"] = train_mamba2(args)
    
    # Phase 4: Whisper Vocabulary Boost
    if args.only is None:
        results["whisper"] = whisper_boost(args)
    
    # Summary
    total = time.time() - t0
    logger.info("")
    logger.info("‚ïî" + "‚ïê" * 58 + "‚ïó")
    logger.info("‚ïë   Training Summary                                       ‚ïë")
    logger.info("‚ï†" + "‚ïê" * 58 + "‚ï£")
    for name, ok in results.items():
        icon = "‚úÖ" if ok else "‚ùå"
        logger.info(f"‚ïë   {icon} {name:<20s}                               ‚ïë")
    logger.info("‚ï†" + "‚ïê" * 58 + "‚ï£")
    logger.info(f"‚ïë   Total time: {total:.0f}s                                      ‚ïë")
    logger.info("‚ïö" + "‚ïê" * 58 + "‚ïù")
    
    # ‚ïê‚ïê‚ïê Consolidate models into models/tars_v3/ ‚ïê‚ïê‚ïê
    consolidate_models(results, total)
    
    if all(results.values()):
        logger.info("\nüéØ All training phases completed successfully!")
        logger.info("   Models consolidated: models/tars_v3/")
        logger.info("   Run: python launch_tars.py")
    else:
        logger.error("\n‚ö†Ô∏è Some phases failed. Check logs above.")


if __name__ == "__main__":
    main()
