"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  TARS v3 â€” Kaggle Training Notebook
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ½Ğ° Kaggle Ñ GPU (P100/T4/A100):

1. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸ Ğ²ĞµÑÑŒ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğº Dataset Ğ½Ğ° Kaggle
   (Ğ½Ğ°Ğ·Ğ¾Ğ²Ğ¸ ĞµĞ³Ğ¾, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: "tarsssm-py")
2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Notebook â†’ Add Data â†’ Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ ÑĞ²Ğ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚
3. Ğ’ĞºĞ»ÑÑ‡Ğ¸ GPU: Settings â†’ Accelerator â†’ GPU P100 / T4Ã—2
4. Ğ¡ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞ¹ ÑÑ‚Ğ¾Ñ‚ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ² Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑÑ‡ĞµĞ¹ĞºÑƒ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸

Ğ˜Ğ»Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒ ÑÑ‚Ğ¾Ñ‚ Ñ„Ğ°Ğ¹Ğ» Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ½Ğ¾ÑƒÑ‚Ğ±ÑƒĞº:
  !python /kaggle/input/tarsssm-py/kaggle_train.py

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import time
import shutil
import subprocess
from pathlib import Path

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  1. ĞĞŸĞ Ğ•Ğ”Ğ•Ğ›Ğ•ĞĞ˜Ğ• ĞĞšĞ Ğ£Ğ–Ğ•ĞĞ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

IS_KAGGLE = os.path.exists("/kaggle")
IS_COLAB = os.path.exists("/content")

if IS_KAGGLE:
    INPUT_DIR = Path("/kaggle/input")
    WORK_DIR = Path("/kaggle/working/TarsSSM-Py")
    OUTPUT_DIR = Path("/kaggle/working/output")
elif IS_COLAB:
    INPUT_DIR = Path("/content/drive/MyDrive")
    WORK_DIR = Path("/content/TarsSSM-Py")
    OUTPUT_DIR = Path("/content/output")
else:
    # Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº
    INPUT_DIR = Path(__file__).resolve().parent
    WORK_DIR = INPUT_DIR
    OUTPUT_DIR = INPUT_DIR / "output"

print("=" * 65)
print("  Ğ¢ĞĞ Ğ¡ v3 â€” Kaggle/Colab Training Pipeline")
print("=" * 65)
print(f"  Environment: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}")
print(f"  Input:  {INPUT_DIR}")
print(f"  Work:   {WORK_DIR}")
print(f"  Output: {OUTPUT_DIR}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  2. ĞšĞĞŸĞ˜Ğ ĞĞ’ĞĞĞ˜Ğ• Ğ Ğ•ĞŸĞĞ—Ğ˜Ğ¢ĞĞ Ğ˜Ğ¯ Ğ’ Ğ ĞĞ‘ĞĞ§Ğ£Ğ® Ğ”Ğ˜Ğ Ğ•ĞšĞ¢ĞĞ Ğ˜Ğ®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def setup_workspace():
    """
    Kaggle Ğ¼Ğ¾Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ² read-only /kaggle/input/.
    ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² /kaggle/working/ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.
    """
    if WORK_DIR.exists() and (WORK_DIR / "mega_train.py").exists():
        print("\nâœ… Ğ Ğ°Ğ±Ğ¾Ñ‡Ğ°Ñ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ÑƒĞ¶Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ°")
        return True
    
    # Ğ˜Ñ‰ĞµĞ¼ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Kaggle
    repo_src = None
    if IS_KAGGLE:
        for d in INPUT_DIR.iterdir():
            if d.is_dir():
                # Ğ˜Ñ‰ĞµĞ¼ mega_train.py ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞµÑ€ Ğ½Ğ°ÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ¿Ğ¾
                if (d / "mega_train.py").exists():
                    repo_src = d
                    break
                # Ğ˜Ğ»Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ° 1 ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ
                for sd in d.iterdir():
                    if sd.is_dir() and (sd / "mega_train.py").exists():
                        repo_src = sd
                        break
                if repo_src:
                    break
    elif IS_COLAB:
        # Ğ’ Colab Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· git
        if not WORK_DIR.exists():
            print("\nğŸ“¥ ĞšĞ»Ğ¾Ğ½Ğ¸Ñ€ÑƒĞ¹ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹:")
            print("  !git clone https://github.com/Vazilll/TarsSSM-Py /content/TarsSSM-Py")
            return False
        return True
    else:
        # Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº â€” Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµĞ¼ in-place
        return True
    
    if repo_src is None:
        print("\nâŒ Ğ ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² /kaggle/input/!")
        print("   Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Add Data")
        print("   (Ñ„Ğ°Ğ¹Ğ» mega_train.py Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ² ĞºĞ¾Ñ€Ğ½Ğµ)")
        return False
    
    print(f"\nğŸ“‚ ĞšĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ {repo_src} â†’ {WORK_DIR}...")
    t0 = time.time()
    
    # ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²ÑÑ‘, ĞºÑ€Ğ¾Ğ¼Ğµ Ñ‚ÑĞ¶Ñ‘Ğ»Ñ‹Ñ… Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹
    SKIP = {'.git', '__pycache__', 'venv', '.venv', 'node_modules', '.mypy_cache'}
    
    def copy_tree(src, dst):
        dst.mkdir(parents=True, exist_ok=True)
        for item in src.iterdir():
            if item.name in SKIP:
                continue
            target = dst / item.name
            if item.is_dir():
                copy_tree(item, target)
            else:
                shutil.copy2(str(item), str(target))
    
    copy_tree(repo_src, WORK_DIR)
    elapsed = time.time() - t0
    print(f"  âœ… Ğ¡ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ·Ğ° {elapsed:.1f}s")
    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  3. Ğ£Ğ¡Ğ¢ĞĞĞĞ’ĞšĞ Ğ—ĞĞ’Ğ˜Ğ¡Ğ˜ĞœĞĞ¡Ğ¢Ğ•Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def install_deps():
    """Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ (torch ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ Ğ² Kaggle)."""
    print("\n" + "=" * 65)
    print("  Ğ¤Ğ°Ğ·Ğ° 0: Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹")
    print("=" * 65)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ torch + CUDA
    try:
        import torch
        print(f"\n  PyTorch: {torch.__version__}")
        print(f"  CUDA:    {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"  GPU:     {torch.cuda.get_device_name(0)}")
            print(f"  VRAM:    {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    except ImportError:
        print("  âš  PyTorch Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ â€” ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ...")
        subprocess.run([sys.executable, "-m", "pip", "install", "torch", "-q"])
    
    # ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹ (Ğ±ĞµĞ· torch â€” Ğ¾Ğ½ ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ)
    packages = [
        "einops", "tqdm", "psutil",
        "sentencepiece", "tokenizers",
        "sentence-transformers",
        "datasets",
        "transformers",
    ]
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ ÑƒĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾
    to_install = []
    for pkg in packages:
        try:
            __import__(pkg.replace("-", "_"))
        except ImportError:
            to_install.append(pkg)
    
    if to_install:
        print(f"\n  ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°: {', '.join(to_install)}")
        subprocess.run(
            [sys.executable, "-m", "pip", "install"] + to_install + ["-q"],
            check=False
        )
    else:
        print("\n  âœ… Ğ’ÑĞµ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹ ÑƒĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹")
    
    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  4. Ğ¡ĞšĞĞ§Ğ˜Ğ’ĞĞĞ˜Ğ• Ğ”ĞĞĞĞ«Ğ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def download_data():
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Wikipedia + HF)."""
    print("\n" + "=" * 65)
    print("  Ğ¤Ğ°Ğ·Ğ° 1: Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    print("=" * 65)
    
    os.chdir(str(WORK_DIR))
    sys.path.insert(0, str(WORK_DIR))
    
    data_dir = WORK_DIR / "data"
    data_dir.mkdir(exist_ok=True)
    
    # 1. Wikipedia
    wiki_path = data_dir / "wiki_ru.txt"
    if wiki_path.exists() and wiki_path.stat().st_size > 100_000:
        wiki_mb = wiki_path.stat().st_size / 1024 / 1024
        print(f"\n  ğŸ“š Wikipedia: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({wiki_mb:.1f} MB)")
    else:
        print("\n  ğŸ“š Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Wikipedia (10 000 ÑÑ‚Ğ°Ñ‚ĞµĞ¹)...")
        try:
            result = subprocess.run(
                [sys.executable, str(WORK_DIR / "training" / "download_wiki.py"),
                 "--count", "10000"],
                cwd=str(WORK_DIR), timeout=1800
            )
            if result.returncode == 0:
                print("  âœ… Wikipedia ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ°")
            else:
                print("  âš  Wikipedia Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ° (Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾)")
        except Exception as e:
            print(f"  âš  Wikipedia: {e}")
    
    # 2. HuggingFace datasets
    hf_files = list(data_dir.glob("hf_*.txt"))
    if len(hf_files) >= 1:
        total_mb = sum(f.stat().st_size for f in hf_files) / 1024 / 1024
        print(f"  ğŸ¤— HuggingFace: ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ ({len(hf_files)} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ², {total_mb:.0f} MB)")
    else:
        print("  ğŸ¤— Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ HuggingFace Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²...")
        try:
            result = subprocess.run(
                [sys.executable, str(WORK_DIR / "training" / "download_hf_dataset.py"),
                 "--preset", "all"],
                cwd=str(WORK_DIR), timeout=1800
            )
            if result.returncode == 0:
                print("  âœ… HF Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½Ñ‹")
            else:
                print("  âš  HF Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ½Ñ‹ (Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾)")
        except Exception as e:
            print(f"  âš  HF: {e}")
    
    # Ğ˜Ñ‚Ğ¾Ğ³Ğ¾
    total = sum(f.stat().st_size for f in data_dir.glob("*") if f.is_file())
    print(f"\n  ğŸ“Š Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {total / 1024 / 1024:.0f} MB")
    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  5. ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_reflex():
    """Ğ¤Ğ°Ğ·Ğ° 2: Ğ ĞµÑ„Ğ»ĞµĞºÑÑ‹ (MinGRU classifier, ~1 Ğ¼Ğ¸Ğ½)."""
    print("\n" + "=" * 65)
    print("  Ğ¤Ğ°Ğ·Ğ° 2: Ğ ĞµÑ„Ğ»ĞµĞºÑÑ‹ (MinGRU Classifier)")
    print("=" * 65)
    
    result = subprocess.run(
        [sys.executable, str(WORK_DIR / "training" / "train_reflex.py"),
         "--epochs", "100", "--lr", "0.002"],
        cwd=str(WORK_DIR), timeout=600
    )
    return result.returncode == 0


def train_mingru():
    """Ğ¤Ğ°Ğ·Ğ° 3: MinGRU LM (System 1, ~15 Ğ¼Ğ¸Ğ½ GPU)."""
    print("\n" + "=" * 65)
    print("  Ğ¤Ğ°Ğ·Ğ° 3: MinGRU Language Model (System 1)")
    print("=" * 65)
    
    result = subprocess.run(
        [sys.executable, str(WORK_DIR / "training" / "train_mingru.py"),
         "--epochs", "25",
         "--lr", "3e-3",
         "--dim", "512",
         "--layers", "6",
         "--batch", "32",
         "--seq_len", "256",
         "--augment",
        ],
        cwd=str(WORK_DIR), timeout=3600
    )
    return result.returncode == 0


def train_mamba2():
    """
    Ğ¤Ğ°Ğ·Ğ° 4: Mamba-2 Brain (Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ~2-4Ñ‡ GPU).
    
    12 ÑĞ»Ğ¾Ñ‘Ğ² Ã— 768d, 4 Ğ¿Ğ¾Ğ´-Ñ„Ğ°Ğ·Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ:
      Phase 1: Full pretrain (Ğ²ÑĞµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, 5 ÑĞ¿Ğ¾Ñ…)
      Phase 2: Fine-tune WKV + Fusion (SSD frozen, 3 ÑĞ¿Ğ¾Ñ…Ğ¸)
      Phase 3: Fine-tune MoLE + MatrixPool (2 ÑĞ¿Ğ¾Ñ…Ğ¸)
      Phase 4: Fine-tune WKV + RAG + Memory (2 ÑĞ¿Ğ¾Ñ…Ğ¸)
    """
    print("\n" + "=" * 65)
    print("  Ğ¤Ğ°Ğ·Ğ° 4: Mamba-2 Brain (12Ã—768d, Full Architecture)")
    print("=" * 65)
    
    import torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ batch size Ğ¿Ğ¾ VRAM
    if torch.cuda.is_available():
        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if vram_gb >= 40:      # A100
            batch = "32"
            accum = "2"
        elif vram_gb >= 16:    # P100 / T4
            batch = "16"
            accum = "4"
        else:                  # P4 / K80
            batch = "8"
            accum = "8"
        print(f"\n  GPU: {torch.cuda.get_device_name(0)} ({vram_gb:.0f} GB)")
        print(f"  Batch: {batch} Ã— {accum} = {int(batch) * int(accum)} effective")
    else:
        batch = "4"
        accum = "4"
        print("\n  âš  ĞĞµÑ‚ GPU â€” Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ÑƒĞ´ĞµÑ‚ ĞĞ§Ğ•ĞĞ¬ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼!")
    
    # Transfer embedding MinGRU â†’ Mamba-2 (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
    emb_args = []
    mingru_path = WORK_DIR / "models" / "mingru_weights.pt"
    if mingru_path.exists():
        print(f"  ğŸ”— Transfer embedding: {mingru_path}")
        try:
            cp = torch.load(str(mingru_path), map_location='cpu', weights_only=False)
            state = cp.get('model_state_dict', cp)
            for k in state:
                if 'shared_embedding' in k or 'emb.weight' in k:
                    emb_path = WORK_DIR / "models" / "tars_v3" / "_transfer_embedding.pt"
                    emb_path.parent.mkdir(parents=True, exist_ok=True)
                    torch.save(state[k], str(emb_path))
                    emb_args = ["--pretrained_emb", str(emb_path)]
                    print(f"  âœ… Embedding ({state[k].shape}) saved")
                    break
        except Exception as e:
            print(f"  âš  Transfer failed: {e}")
    
    base_args = [
        "--d_model", "768",
        "--n_layers", "12",
        "--vocab_size", "256",
        "--batch", batch,
        "--accum_steps", accum,
        "--device", device,
        "--curriculum",
        "--label_smoothing", "0.1",
    ] + emb_args
    
    train_script = str(WORK_DIR / "training" / "train_mamba2.py")
    
    phases = [
        # (phase, epochs, lr, seq_len, description)
        ("1", "5", "3e-4", "256", "Full pretrain (SSD + WKV + Î©-SSM + MoLE)"),
        ("2", "3", "1e-4", "512", "Fine-tune WKV + Fusion (SSD frozen)"),
        ("3", "2", "3e-5", "512", "Fine-tune MoLE + MatrixPool"),
        ("4", "2", "1.5e-5", "512", "Fine-tune WKV + RAG + Memory"),
    ]
    
    results = {}
    for phase, epochs, lr, seq_len, desc in phases:
        print(f"\n  â”€â”€ Phase {phase}/4: {desc} â”€â”€")
        
        phase_args = base_args + [
            "--epochs", epochs,
            "--lr", lr,
            "--phase", phase,
            "--seq_len", seq_len,
        ]
        if phase != "1":
            phase_args.append("--resume")
        
        result = subprocess.run(
            [sys.executable, train_script] + phase_args,
            cwd=str(WORK_DIR), timeout=7200  # 2Ñ‡ Ğ½Ğ° Ñ„Ğ°Ğ·Ñƒ Ğ¼Ğ°ĞºÑ
        )
        
        results[f"p{phase}"] = result.returncode == 0
        if result.returncode != 0:
            print(f"  âš  Phase {phase} finished with errors")
    
    all_ok = all(results.values())
    if all_ok:
        print("\n  âœ… Ğ’ÑĞµ 4 Ñ„Ğ°Ğ·Ñ‹ Mamba-2 Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ñ‹!")
    else:
        failed = [k for k, v in results.items() if not v]
        print(f"\n  âš  Ğ¤Ğ°Ğ·Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸: {failed}")
    
    return all_ok


def train_quantize():
    """Ğ¤Ğ°Ğ·Ğ° 5: ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ FP16 â†’ 1.58-bit + Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."""
    print("\n" + "=" * 65)
    print("  Ğ¤Ğ°Ğ·Ğ° 5: ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ BitNet 1.58-bit")
    print("=" * 65)
    
    fp16_path = WORK_DIR / "models" / "mamba2" / "mamba2_omega.pt"
    if not fp16_path.exists():
        print(f"  âš  FP16 Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {fp16_path}")
        print("  ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ")
        return False
    
    import torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    result = subprocess.run(
        [sys.executable, str(WORK_DIR / "training" / "train_mamba2.py"),
         "--d_model", "768", "--n_layers", "12",
         "--batch", "16", "--accum_steps", "4",
         "--epochs", "3", "--lr", "5e-5",
         "--phase", "1", "--quant",
         "--resume", "--device", device,
         "--seq_len", "256", "--label_smoothing", "0.1",
        ],
        cwd=str(WORK_DIR), timeout=7200
    )
    return result.returncode == 0


def validate_model():
    """Ğ¤Ğ°Ğ·Ğ° 7: Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ."""
    print("\n" + "=" * 65)
    print("  Ğ¤Ğ°Ğ·Ğ° 7: Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ")
    print("=" * 65)
    
    result = subprocess.run(
        [sys.executable, "-c", f"""
import sys
sys.path.insert(0, '{WORK_DIR}')
import torch
from brain.tokenizer import TarsTokenizer
from brain.mamba2.model import TarsMamba2LM

device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = TarsTokenizer()

model, ckpt = TarsMamba2LM.load_pretrained(device=device)
model.eval()

if ckpt is None:
    print("  No trained weights found")
else:
    print(f"  Model: {{ckpt}}")
    params = sum(p.numel() for p in model.parameters())
    print(f"  Parameters: {{params:,}}")
    
    for prompt in ["Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚", "ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°", "Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ"]:
        tokens = tokenizer.encode(prompt)
        input_ids = torch.tensor([tokens], dtype=torch.long, device=device)
        with torch.no_grad():
            logits = model(input_ids)
        probs = torch.softmax(logits[0, -1, :], dim=-1)
        top5 = torch.topk(probs, 5)
        preds = []
        for idx, prob in zip(top5.indices.tolist(), top5.values.tolist()):
            char = tokenizer.decode([idx])
            preds.append(f"'{{char}}'({{prob:.2%}})")
        print(f"  '{{prompt}}' â†’ {{', '.join(preds)}}")
    
    print("  âœ… Model works!")
"""],
        cwd=str(WORK_DIR), timeout=120
    )
    return result.returncode == 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  6. Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ• Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢ĞĞ’
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save_outputs():
    """ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² output Ğ´Ğ»Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ."""
    print("\n" + "=" * 65)
    print("  Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²")
    print("=" * 65)
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ
    model_files = [
        WORK_DIR / "models" / "mamba2" / "mamba2_omega.pt",
        WORK_DIR / "models" / "mamba2" / "mamba2_omega_158bit.pt",
        WORK_DIR / "models" / "mingru_weights.pt",
        WORK_DIR / "models" / "reflex" / "reflex_classifier.pt",
    ]
    
    # Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸
    log_files = [
        WORK_DIR / "mega_train.log",
    ]
    
    saved = []
    for src in model_files + log_files:
        if src.exists():
            dst = OUTPUT_DIR / src.name
            shutil.copy2(str(src), str(dst))
            size_mb = dst.stat().st_size / 1024 / 1024
            print(f"  ğŸ“¦ {src.name} â†’ output/ ({size_mb:.1f} MB)")
            saved.append(src.name)
        else:
            print(f"  â­ {src.name} â€” Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
    
    # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ tars_v3 ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ²ÑĞµ
    tars_v3_out = OUTPUT_DIR / "tars_v3"
    tars_v3_out.mkdir(exist_ok=True)
    
    copies = {
        "reflex.pt": WORK_DIR / "models" / "reflex" / "reflex_classifier.pt",
        "mingru.pt": WORK_DIR / "models" / "mingru_weights.pt",
        "mamba2.pt": WORK_DIR / "models" / "mamba2" / "mamba2_omega.pt",
        "mamba2_158bit.pt": WORK_DIR / "models" / "mamba2" / "mamba2_omega_158bit.pt",
    }
    
    for dst_name, src in copies.items():
        if src.exists():
            shutil.copy2(str(src), str(tars_v3_out / dst_name))
    
    # Config
    config = {
        "encoding": "cp1251",
        "vocab_size": 256,
        "d_model": 768,
        "n_layers": 12,
        "n_experts": 8,
        "omega_dim": 32,
        "pool_size": 48,
    }
    import json
    (tars_v3_out / "config.json").write_text(
        json.dumps({"models": {"mamba2": {"params": config}}}, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    
    print(f"\n  ğŸ“ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²: {OUTPUT_DIR}")
    print(f"  ğŸ“ tars_v3 ÑĞ±Ğ¾Ñ€ĞºĞ°: {tars_v3_out}")
    
    if IS_KAGGLE:
        print(f"\n  ğŸ’¡ Ğ¡ĞºĞ°Ñ‡Ğ°Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:")
        print(f"     Notebook â†’ Output â†’ Download All")
    
    return saved


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  7. MAIN PIPELINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    t0 = time.time()
    results = {}
    
    # â”€â”€ Setup â”€â”€
    if not setup_workspace():
        print("\nâŒ Workspace setup failed!")
        return
    
    os.chdir(str(WORK_DIR))
    sys.path.insert(0, str(WORK_DIR))
    
    # â”€â”€ Phase 0: Dependencies â”€â”€
    results["install"] = install_deps()
    
    # â”€â”€ Phase 1: Data â”€â”€
    results["download"] = download_data()
    
    # â”€â”€ Phase 2: Reflex â”€â”€
    try:
        results["reflex"] = train_reflex()
    except Exception as e:
        print(f"  âš  Reflex error: {e}")
        results["reflex"] = False
    
    # â”€â”€ Phase 3: MinGRU â”€â”€
    try:
        results["mingru"] = train_mingru()
    except Exception as e:
        print(f"  âš  MinGRU error: {e}")
        results["mingru"] = False
    
    # â”€â”€ Phase 4: Mamba-2 (MAIN) â”€â”€
    try:
        results["mamba2"] = train_mamba2()
    except Exception as e:
        print(f"  âš  Mamba-2 error: {e}")
        results["mamba2"] = False
    
    # â”€â”€ Phase 5: Quantization â”€â”€
    try:
        results["quantize"] = train_quantize()
    except Exception as e:
        print(f"  âš  Quantize error: {e}")
        results["quantize"] = False
    
    # â”€â”€ Phase 7: Validation â”€â”€
    try:
        results["validate"] = validate_model()
    except Exception as e:
        print(f"  âš  Validate error: {e}")
        results["validate"] = False
    
    # â”€â”€ Save outputs â”€â”€
    saved = save_outputs()
    
    # â•â•â• Ğ˜Ğ¢ĞĞ“Ğ˜ â•â•â•
    total_time = time.time() - t0
    hours = total_time / 3600
    
    print("\n" + "=" * 65)
    print("  Ğ˜Ğ¢ĞĞ“Ğ˜ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯")
    print("=" * 65)
    for name, ok in results.items():
        icon = "âœ…" if ok else "âŒ"
        print(f"  {icon} {name}")
    print(f"\n  â±  Ğ’Ñ€ĞµĞ¼Ñ: {hours:.1f} Ñ‡Ğ°ÑĞ¾Ğ² ({total_time:.0f} ÑĞµĞº)")
    
    if all(results.values()):
        print("\n  ğŸ¯ Ğ’Ğ¡Ğ• Ğ¤ĞĞ—Ğ« Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ« Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞ!")
    else:
        failed = [k for k, v in results.items() if not v]
        print(f"\n  âš  Ğ¤Ğ°Ğ·Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸: {', '.join(failed)}")
    
    print("=" * 65)


if __name__ == "__main__":
    main()
