# TARS — Архитектура когнитивной системы
## Техническая документация v3.0 (Omega Core)

---

## Оглавление

1. [Обзор системы](#1-обзор-системы)
2. [Hardware Optimization & Zero-Idle State](#2-hardware-optimization--zero-idle-state)
3. [Когнитивный конвейер (3-Tier Architecture)](#3-когнитивный-конвейер-3-tier-architecture)
4. [Brain — Мозг](#4-brain--мозг)
   - 4.1 [Tier 1: ReflexCore (MinGRU)](#41-tier-1-reflexcore-mingru)
   - 4.2 [Tier 1.5: RRN (Relational Memory Core)](#42-tier-15-rrn-relational-memory-core)
   - 4.3 [Tier 2: Primary Brain (Mamba-2 SSD)](#43-tier-2-primary-brain-mamba-2-ssd)
   - 4.4 [Tier 3: Omega Core (IDME Matrix Pool)](#44-tier-3-omega-core-idme-matrix-pool)
   - 4.5 [MoLE (Mixture of LoRA Experts)](#45-mole-mixture-of-lora-experts)
   - 4.6 [Ω-SSM (Lie Algebra)](#46-ω-ssm-lie-algebra)
   - 4.7 [Интерактивное Размышление и Внутренний Монолог](#47-интерактивное-размышление-и-внутренний-монолог)
5. [Memory — Память](#5-memory--память)
   - 5.1 [Titans — Нейронная LTM](#51-titans--нейронная-ltm)
   - 5.2 [LEANN — Векторный индекс](#52-leann--векторный-индекс)
   - 5.3 [TarsStorage — Персоналия](#53-tarsstorage--персоналия)
   - 5.4 [Бесконечный Контекст (Paging Memory)](#54-бесконечный-контекст-paging-memory)
6. [Agent — Агент](#6-agent--агент)
   - 6.1 [GIE — General Intelligence Executive](#61-gie--general-intelligence-executive)
   - 6.2 [Knowledge Injector (RAG)](#62-knowledge-injector-rag)
   - 6.3 [MoIRA — Нейронный роутер](#63-moira--нейронный-роутер)
   - 6.4 [ActionEngine — Исполнитель](#64-actionengine--исполнитель)
7. [Sensory — Сенсорика](#7-sensory--сенсорика)
   - 7.1 [Voice — Голос](#71-voice--голос)
   - 7.2 [Vision — Зрение](#72-vision--зрение)
8. [Научные основы](#8-научные-основы)
9. [Фундаментальная математика TARS: Интегральный Кортекс](#9-фундаментальная-математика-tars-интегральный-кортекс)
10. [Единая Полевая Архитектура (Unified Core)](#10-единая-полевая-архитектура-unified-core)
11. [Будущее: Дорожная Карта TarsUltra](#11-будущее-дорожная-карта-tarsultra)

---

## 1. Обзор системы

TARS v3 — это **автономный ИИ-ассистент нового поколения**, работающий полностью локально. 
Главная архитектурная особенность версии 3.0 заключается в замене монолитных LLM (трансформеров) на **трёхуровневый оркестр (3-Tier Pipeline)**, состоящий из ультра-быстрых RNN-сетей и State Space Models (Mamba-2), усиленных динамическим пулом размышлений (Omega Core).

| Компонент | Аналогия с мозгом | Модель / Технология |
|-----------|-------------------|----------------------|
| **Tier 1: ReflexCore** | Спинной мозг (рефлексы) | MinGRU (Were RNNs All We Needed?) |
| **Tier 1.5: RRN** | Мозжечок + Гиппокамп | DeepMind RMC + MessagePassing + TRM |
| **Tier 2: Primary Brain** | Базовая кора ГМ | Mamba-2 SSD (12 слоёв) |
| **Tier 3: Omega Core** | Лобные доли (абстракция) | IDME Paged Matrix Pool (До 100+ слоёв) |
| **Titans & LEANN** | Долговременная память | Surprise-based LTM & HNSW Вектор |
| **MoLE** | Специализация полушарий | LoRA + Sparse Top-2 Routing |
| **MoIRA & ActionEngine** | Моторика рук и пальцев | Нейронный роутинг в ОС |

В отличие от классических моделей, TARS не тратит 2 гигабайта VRAM на каждый новый вопрос. Его контекстное окно не ограничено KV-кэшем.

---

## 2. Hardware Optimization & Zero-Idle State

Важнейший принцип проекта: **Тяжёлые GPU (типа RTX 4090) используются ПОЧТИ ТОЛЬКО для offline-обучения (тренировки весов).** Инференс (работа) спроектирован для Edge-устройств (телефоны, обычные ПК).

### Режим Ожидания (Zero-Idle)
В фоновом режиме:
- Базовая часть Mamba-2 и массивы IDME выгружены в Swap или на SSD.
- Потребление ресурсов — близко к 0% CPU/GPU.
- Бодрствует **только MinGRU Reflex (Tier 1)** и микро-VAD. Reflex-мозг занимает доли мегабайта в RAM и мгновенно сканирует аудио и текст на триггеры.

### Плавное Пробуждение (Cascading Wake-Up)
Когда вы обращаетесь к системе:
1. Отрабатывает **MinGRU Reflex**. Если команда проста ("Пауза видео"), она исполняется за 20мс. Mamba-2 даже не грузится.
2. Если Reflex видит потребность в логике, запускается **RRN Grounding** (построение реляционной карты).
3. При необходимости включается большая **Mamba-2**. В отличие от LLM-трансформеров, ей не нужно переваривать гигантский KV-кэш истории (что долго). Mamba-2 просто берёт свой сжатый $S$ вектор состояния и `веса`, и сразу начинает думать.
4. После выдачи глубокого ответа, если нет активности $N$ секунд, Mamba-2 делает "State Snapshot" в LEANN и засыпает, возвращаясь в Zero-Idle state.

---

## 3. Когнитивный конвейер (3-Tier Architecture)

Запрос от пользователя не грузит всю систему сразу, а проходит "воронку сложности":

```
Голос/Текст/Экран
    │
    ▼
┌─────────────────────────────────────────┐
│ Tier 1: ReflexCore (MinGRU)             │  ← 0.1 мс, Нейронная проверка уверенности (P_conf)
│ "Привет" → уверенность 0.98 → Ответ     │  Если P_conf < 0.85 → Запрос идет дальше
└────────────┬────────────────────────────┘
             │
             ▼
┌─────────────────────────────────────────┐
│ Tier 1.5: RRN (Relational Memory)       │  ← 4-8 адаптивных шагов рекурсии
│ Ищет связи в LEANN и Titans. Строит     │  Решает, нужна ли глубокая Mamba-2.
│ "контекстный граунд" перед ответом.     │
└────────────┬────────────────────────────┘
             │ Grounded Context
             ▼
┌─────────────────────────────────────────┐
│ Tier 2: Primary Brain (Mamba-2 SSD)     │  ← Базовый проход 12 слоёв. 
│ Высчитывает p-аппроксимацию сходимости  │  Если подумали, и p > 1.2 → Ответ.
│ (Integral Auditor).                     │  Если мысль пуста (p < 0.5) → Идём в Tier 3.
└────────────┬────────────────────────────┘
             │ Не сошлось (Unconverged State)
             ▼
┌─────────────────────────────────────────┐
│ Tier 3: Omega Core (IDME Matrix Pool)   │  ← Рекрутирует НОВЫЕ матрицы из пула по 4 шт.
│ Глубокий анализ. Может прерываться для  │  Генерирует внутренний монолог <thought>.
│ интерактивного RAG-инжекта (Поиск ПК/Web)  Включает защиту от зацикливания (Hankel SVD).
└────────────┬────────────────────────────┘
             │ Отфильтрованный ответ / Команда
             ▼
┌─────────────────────────────────────────┐
│ System Agents (MoIRA + ActionEngine)    │  ← Выбор: Terminal / Browser / Vision
│ Выполнение команды в ОС                 │
└─────────────────────────────────────────┘
```

---

## 4. Brain — Мозг

### 4.1 Tier 1: ReflexCore (MinGRU)
*   **Архитектура:** Ультра-легкая RNN на основе исследования *"Were RNNs All We Needed?" (Feng et al., 2024)*.
*   **Особенность:** Удален `tanh` и зависимость от скрытого состояния в гейтах. Обучается параллельно как трансформер, но при инференсе дает $O(1)$ памяти.
*   **Роль:** Первый заслон. Распознает паттерны, делает тривиальные ответы, оценивает сложность вопроса.

### 4.2 Tier 1.5: RRN (Relational Memory Core)
*Глубоко модернизированное ядро промежуточной памяти (гибрид архитектур DeepMind, Samsung TRM и Message Passing).*
*   **Relational Memory:** Набор 'слотов' (матрица), где каждый слот взаимодействует с другими через `Multi-Head Self-Attention`. Это позволяет модели понимать семантические отношения между фактами из прошлого (в отличие от обычного вектора).
*   **Message Passing:** Каждый слот памяти работает как "узел", посылая сообщения другим, что превосходно решает задачи многошаговой логики.
*   **Recursive Reasoning (TRM):** Цикл из $N$ итераций, где на каждом шаге оригинальный вход заново "вливается" в мысль (`Input Injection`), позволяя рекурсии не забывать изначальный вопрос. Управляется `Confidence Head` (адаптивная остановка при достижении ответа).

### 4.3 Tier 2: Primary Brain (Mamba-2 SSD)
*   **Архитектура:** `Mamba-2` базируется на алгоритме State Space Duality (SSD), который делит матрицу на независимые чанки для параллельного скана. Реализация полностью на PyTorch (`ssd.py`), без зависимости от Тритона, что позволяет запускать ядро на CPU, Android и Raspberry Pi. В ней находятся базовые 12 слоев.

### 4.4 Tier 3: Omega Core (IDME Matrix Pool)
Вместо фиксированных 64 слоёв монолитной сети (которая всегда потребляет максимум ресурсов), TARS имеет **Infinite Dynamic Matrix Expansion (IDME).**
*   **48+ Свободных матриц:** Находятся в пуле на SSD/RAM.
*   Если мысль сложновата, мозг подбирает из пула (по косинусному сходству с текущим вектором мысли) 4 самых подходящих матрицы и пропускает через них процесс.
*   Проверяется интеграл (Auditor). Если мысль всё ещё не сошлась, достаются ЕЩЁ 4 матрицы. Это позволяет ИИ "думать" 5-10 секунд, виртуально создавая нейросеть глубиной 100+ слоёв на лету.
*   **Recirculation:** Самые полезные матрицы (решившие задачу) получают приоритет в очереди. Вызов защищен матрицей Ганкеля (`Hankel SVD`) от зацикливания.

### 4.5 MoLE (Mixture of LoRA Experts)
Обеспечивает специализацию "полушарий". У Mamba-2 есть 8 адаптеров `LoRA` (rank=8, каждый 16K параметров): `general`, `analyzer`, `critic`, `creative`, `math`, `code`, `memory`, `action`.
На каждом раунде (в том числе в Omega Core), Sparse-роутер (`TopicRouter`) активирует ровно **двух** экспертов, чьи веса накладываются на базовую Mamba-2 поверх логики. Это дает эффект 8-ми моделей-экспертов по стоимости 64К параметров.

### 4.6 Ω-SSM (Lie Algebra)
*Каждый третий слой* модели использует ортогональное многообразие $SO(n)$ с Преобразованием Кэли (`Cayley Transform`). 
Зачем: В стандартных RNN градиенты затухают или взрываются на большой глубине контекста. На сфере (Lie Algebra) вектор просто вращается, но его норма остаётся = 1. Это гарантирует 100% стабильность даже на миллиардном токене. Дополнительно встроен `VQ Codebook` для оперирования дискретной логикой.

### 4.7 Интерактивное Размышление и Внутренний Монолог
TARS не является "честным черным ящиком трансформера", где выстрелил запрос и ждешь текст.
*   **DeepSeek Style:** Для глубоких задач модель генерирует тег `<thought>`. Интерфейс рисует "Анализирую...", а модель генерирует тексты промежуточного планирования (внутренний монолог).
*   **Интерактивный Инжект:** Пока модель "крутит" матрицы в IDME (внутри монолога), пользователь может написать: *"Смени инструмент, используй PostgreSQL!"*. Микро-мозг `Reflex` моментально захватит это уточнение, векторизует и пробросит ВНУТРЬ работающей Mamba-2 (через residual), меняя курс мыслей **на ходу**. Без отмены и перезапуска!

---

## 5. Memory — Память

В отличие от трансформеров, память TARS разбита на разные биологические подсистемы:

### 5.1 Titans — Нейронная LTM
Основано на *"Titans: Learning to Memorize at Test Time" (Google)*. Долговременная память запекается в "веса" мини-нейросети (MLP). Работает на принципе удивления (Surprise-based Learning): записывает опыт, только если ошибка прогнозирования велика (т.е. произошло реально что-то новое).

### 5.2 LEANN — Векторный индекс
Использует кастомный HNSW (Hierarchical Navigable Small World) для молниеносного поиска соседей. Эмбеддинги (`all-MiniLM`) кэшируются. Обладает Emergency-системой отката на `TF-IDF + хеширование`, если нейронных библиотек нет.

### 5.3 TarsStorage — Персоналия
Файлы-профили (Mem0 API + JSON fallback). Хранят информацию: кто пользователь, что он любит писать на Python, какой у него проект.

### 5.4 Бесконечный Контекст (Paging Memory)
У Mamba-2 размер вектора контекста `$S$` строго фиксирован. Но чтобы ИИ не забыл начало 5-летней беседы, реализована система Страничной Памяти:
*   Старые мысли "вымываются" из активного вектора и **фоново архивируются** в `LEANN` (тексты) и `Titans` (латентные концепты).
*   **Knowledge Inject (Self-Recall):** Если на миллионном токене пользователь спрашивает факт из глубокого прошлого, модель сама генерирует токен `<tool>recall: имя героя</tool>`. Нужный кусок поднимается с SSD, векторизуется и спринцуется в `memory_vec` Mamba-2.
*   *Итог:* Контекст ограничен лишь свободным местом на SSD (миллиарды токенов), RAM не растет.

---

## 6. Agent — Агент

### 6.1 GIE (General Intelligence Executive)
Координатор ОС. Реализует:
*   Каскадную трансляцию (Reflex → RRN → Brain)
*   **Sleep Phase**: После $N$ событий (ночью) запускает RRN для консолидации дневной памяти (перевод из Working Memory в LTM).

### 6.2 Knowledge Injector (RAG)
Во время IDME циклов глубокого размышления модель может выбросить команды: 
`<tool>search_web</tool>` или `<tool>read_file</tool>`.
Внешний `ActionEngine` идет в duckduckgo/локальные папки, достает текст, сжимает его в SentenceTransformer вектор и прокидывает внутрь текущего состояния `Mamba`. 

### 6.3 MoIRA — Нейронный роутер
Перенаправляет абстрактные `thought` вектора в конкретные системы (Execute Python, Write Markdown).
*Гибридный роутинг:* (1) Cosine similarity с зашитыми эмбеддингами инструментов. (2) Жёсткий Override по ключевым словам.

### 6.4 ActionEngine — Исполнитель
Реальное взаимодействие с Windows/Linux/Android:
Шеллы (`subprocess`), `PyAutoGUI` клики, чтение окон, парсинг URL. Если модель попросит кликнуть, вступает в дело Vision-модуль.

---

## 7. Sensory — Сенсорика

### 7.1 Voice — Голос
*   **Micro-VAD**: Дремлет, пока все системы выключены. 
*   **Faster-Whisper (tiny / large-v3-turbo)**: Переводит речь в текст.
*   **Piper (ONNX)**: Отвечает голосом без задержек.

### 7.2 Vision — Зрение
*   **YOLOv11** детекция UI-элементов (Кнопки, Инпуты) и OCR.
*   В TarsUltra `Vision` будет отправлять не слова, а латентные вектора напрямую в `Hemispheric Glue` (мозг), создавая неразрывное физическое ощущение реальности.

---

## 8. Научные основы

*Опираемся исключительно на передовые публикации 2024-2026 годов:*
1. **Mamba-2**: *"Transformers are SSMs: Structured State Space Duality" (Gu & Dao, 2024)*.
2. **MinGRU**: *"Were RNNs All We Needed?" (Feng et al., 2024)*.
3. **Titans Memory**: *"Titans: Learning to Memorize at Test Time" (Google Research, 2024)*.
4. **Relational RNNs**: *"Relational recurrent neural networks" (DeepMind, NeurIPS 2018)*.
5. **TRM Reasoning**: *"Less is More: Recursive Reasoning with Tiny Networks" (Samsung, 2024)*.
6. **LoRA Experts**: *"LoRA: Low-Rank Adaptation" (Hu et al., 2021)* + MoE концепции.
7. **Integral Convergence**: *"О признаках сходимости несобственных интегралов" (И.А. Чикулаев, В.А. Кадымов, 2026)*.

---

## 9. Фундаментальная математика TARS: Интегральный Кортекс

Система использует строгую математическую базу (Чикулаев-Кадымов, 2026) для определения того, "когда мысль сформирована". 
Эволюция мысли (State Space) — это функция, $f(t) = \| h(t) - h(t-1) \|_2$.

**Integral Auditor (IA):** Высчитывает в реальном времени коэффициент $p$ (из $1/t^p$) методом МНК на скользящем окне:
$$\ln f(t) = \ln C - p \ln t + \varepsilon$$
Если $p > 1.2$, по второму признаку сходмости интеграла, ИИ понял мысль до конца — генерация прекращается (защита от галлюцинаций).
Если $p < 0.5$, ИИ находится в поиске и активирует Omega Core пулы матриц.
Также `Hankel SVD` (разложение матрицы Ганкеля на сингулярные числа) обнаруживает коллапс ранга ($\sigma_2 / \sigma_1 \to 0$), прерывая зацикливание модели.

---

## 10. Единая Полевая Архитектура (Unified Core)

*   **SSM-VAD:** Звук детектируется теми же матрицами, разложенными на резонансе, что и тексты. "Интеграл звука" (высокий $p$) сразу будит смысловые слои SSM без задержек.
*   **Бикамеральный Ансамбль (BIA):** В супер-сложных задачах TARS активирует все 4 пути одновременно (Аналитик / Рефлекс / Критик / Креатив). Если их сходимость ($p$) различается, система использует внутренний перекрестный разговор, пока консенсус стабильности не будет достигнут.
*   **Матричная Рециркуляци:** Эффективная матрица (та, при которой $p$ резко вырос) перекидывается в начало "очереди размышлений" для будущего использования.

---

## 11. Будущее: Дорожная Карта TarsUltra

Ветка TarsUltra фокусируется на:
1.  **BitNet 1.58-bit:** Перевод весов в тернарную систему (-1, 0, 1) для снижения RAM до 400МБ и уменьшения энергопотребления в 70 раз (NPU-first подход). Ожидается, что потребление TARS упадет ниже 1 Ватта.
2.  **Rust Backend:** Полный уход от Python GIL, сверхбыстрые биндинги памяти (`Zero-Overhead Memory`) в ветке `TarsSSM-rs`. LNN фильтры (Liquid Neural Networks) для фонового цикла мониторинга.
3.  **OSWorld & Управление через Accessibility (MCP):** Взаимодействие с ОС не через скриншоты, а через нативные API программных интерфейсов Windows/Android для обеспечения человеческой точности моторики.

---
*Ядро Системы: Antigravity / TARS AI, 2026*
