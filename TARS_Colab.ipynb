{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "L4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83e\udd16 \u0422\u0410\u0420\u0421 v3 \u2014 \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 Colab\n",
                "\n",
                "**512d \u00d7 8 \u0441\u043b\u043e\u0451\u0432 (~103M params)** | \u0410\u0432\u0442\u043e-\u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u043e\u0434 GPU\n",
                "\n",
                "\u26a1 **Runtime \u2192 Change runtime type \u2192 L4** (\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 1. GPU Check\n",
                "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 2. Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "print('\u2705 Drive \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0451\u043d')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 3. Clone / Update Project\n",
                "!git clone https://github.com/Vazilll/TarsSSM-Py.git /content/TarsSSM-Py 2>/dev/null || echo '\u2705 Already cloned'\n",
                "%cd /content/TarsSSM-Py\n",
                "!git pull"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 4. \ud83d\uddd1\ufe0f \u0423\u0434\u0430\u043b\u0438\u0442\u044c ВСЕ \u0441\u0442\u0430\u0440\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 (\u0437\u0430\u043f\u0443\u0441\u0442\u0438 \u043f\u0440\u0438 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438)\n",
                "import shutil, os, glob\n",
                "root = '/content/TarsSSM-Py'\n",
                "\n",
                "# \u0414\u0430\u0442\u0430\u0441\u0435\u0442\u044b\n",
                "hf = glob.glob(f'{root}/data/hf_*.txt')\n",
                "for f in hf: os.remove(f)\n",
                "print(f'\u2717 \u0423\u0434\u0430\u043b\u0435\u043d\u043e {len(hf)} \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432 (hf_*.txt)')\n",
                "\n",
                "# Wiki\n",
                "w = glob.glob(f'{root}/data/wiki_*.txt')\n",
                "for f in w: os.remove(f)\n",
                "print(f'\u2717 \u0423\u0434\u0430\u043b\u0435\u043d\u043e {len(w)} wiki \u0444\u0430\u0439\u043b\u043e\u0432')\n",
                "\n",
                "# \u041c\u043e\u0434\u0435\u043b\u0438\n",
                "for d in ['models/tars_v3', 'models/mamba2', 'models/embeddings']:\n",
                "    p = os.path.join(root, d)\n",
                "    if os.path.exists(p):\n",
                "        shutil.rmtree(p)\n",
                "        print(f'\u2717 \u0423\u0434\u0430\u043b\u0435\u043d\u0430 \u043f\u0430\u043f\u043a\u0430 {d}/')\n",
                "\n",
                "# \u041b\u043e\u0433\u0438\n",
                "for f in glob.glob(f'{root}/*.log'):\n",
                "    os.remove(f)\n",
                "\n",
                "# \u041a\u0435\u0448 HuggingFace (\u043e\u0441\u0432\u043e\u0431\u043e\u0434\u0438\u0442\u044c \u043c\u0435\u0441\u0442\u043e)\n",
                "hf_cache = os.path.expanduser('~/.cache/huggingface')\n",
                "if os.path.exists(hf_cache):\n",
                "    cache_gb = sum(f.stat().st_size for f in __import__('pathlib').Path(hf_cache).rglob('*') if f.is_file()) / 1e9\n",
                "    shutil.rmtree(hf_cache)\n",
                "    print(f'\u2717 \u041e\u0447\u0438\u0449\u0435\u043d \u043a\u0435\u0448 HuggingFace ({cache_gb:.1f} GB)')\n",
                "\n",
                "print('\\n\u2705 \u0412\u0441\u0451 \u0443\u0434\u0430\u043b\u0435\u043d\u043e. \u0417\u0430\u043f\u0443\u0441\u0442\u0438 \u044f\u0447\u0435\u0439\u043a\u0443 5 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 5. \ud83d\ude80 Train\n",
                "!python colab_train.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 6. \ud83d\udcca Results\n",
                "!ls -lh models/tars_v3/ 2>/dev/null || echo 'No models yet'\n",
                "print()\n",
                "!tail -20 mega_train.log 2>/dev/null"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udcc1 \u0413\u0434\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 Drive\n",
                "| \u0427\u0442\u043e | \u041f\u0443\u0442\u044c |\n",
                "|-----|------|\n",
                "| \u0414\u0430\u0442\u0430\u0441\u0435\u0442\u044b | `/MyDrive/TarsData/` |\n",
                "| \u041c\u043e\u0434\u0435\u043b\u0438 | `/MyDrive/TarsModels/` |\n",
                "\n",
                "### \ud83d\udd04 \u041f\u0440\u043e\u0434\u043e\u043b\u0436\u0438\u0442\u044c: `!python colab_train.py --resume`"
            ]
        }
    ]
}