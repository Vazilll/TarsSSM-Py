"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Ğ¢ĞĞ Ğ¡ v3 â€” Colab Training (Medium, 103M params)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Google Colab Ñ Ğ°Ğ²Ñ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ´ GPU.

  ĞœĞ¾Ğ´ĞµĞ»ÑŒ:       512d Ã— 8 ÑĞ»Ğ¾Ñ‘Ğ² (~103M params)
  Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ:       Wikipedia + HuggingFace + Personality
  ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:  1.58-bit BitNet
  
  A100 (40GB) â€” batch=32, bf16, ~30-45 Ğ¼Ğ¸Ğ½    ğŸ”¥ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ
  L4   (24GB) â€” batch=24, bf16, ~45-60 Ğ¼Ğ¸Ğ½    âš¡ Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ
  T4   (15GB) â€” batch=16, fp16, ~1-2 Ñ‡Ğ°ÑĞ°     âœ… Ğ‘ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğ¹

Ğ˜ĞĞ¡Ğ¢Ğ Ğ£ĞšĞ¦Ğ˜Ğ¯:
  1. Runtime â†’ Change runtime type â†’ L4 (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)
  2. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ (ZIP / Git / Drive)
  3. !python colab_train.py

ĞĞŸĞ¦Ğ˜Ğ˜:
  !python colab_train.py --resume           # ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ñ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°
  !python colab_train.py --skip-download    # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import time
import subprocess
import shutil
from pathlib import Path

# Fix encoding
if hasattr(sys.stdout, 'reconfigure'):
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass

ROOT = Path(__file__).resolve().parent
os.chdir(ROOT)
sys.path.insert(0, str(ROOT))

IS_COLAB = "COLAB_GPU" in os.environ or os.path.exists("/content")
PYTHON = sys.executable
DATA = ROOT / "data"
MODELS = ROOT / "models"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Google Drive
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DRIVE_DATA = None
DRIVE_MODELS = None

if IS_COLAB:
    try:
        from google.colab import drive
        drive.mount("/content/drive", force_remount=False)
        DRIVE_DATA = Path("/content/drive/MyDrive/TarsData")
        DRIVE_MODELS = Path("/content/drive/MyDrive/TarsModels")
        DRIVE_DATA.mkdir(parents=True, exist_ok=True)
        DRIVE_MODELS.mkdir(parents=True, exist_ok=True)
        print(f"  â˜ï¸  Google Drive Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ñ‘Ğ½")
        print(f"     Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ:  {DRIVE_DATA}")
        print(f"     ĞœĞ¾Ğ´ĞµĞ»Ğ¸:  {DRIVE_MODELS}")
    except Exception as e:
        print(f"  âš ï¸  Drive Ğ½Ğµ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ñ‘Ğ½: {e}")


def restore_cached_data():
    """Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Drive (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)."""
    if not DRIVE_DATA or not DRIVE_DATA.exists():
        return 0
    
    restored = 0
    DATA.mkdir(parents=True, exist_ok=True)
    
    for f in DRIVE_DATA.glob("*"):
        dest = DATA / f.name
        if not dest.exists():
            if f.is_file():
                shutil.copy2(str(f), str(dest))
                restored += 1
    
    if restored > 0:
        print(f"  ğŸ“‚ Ğ’Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾ {restored} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ Drive")
    return restored


def save_data_to_drive():
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Drive."""
    if not DRIVE_DATA:
        return
    
    saved = 0
    for f in DATA.glob("*.txt"):
        dest = DRIVE_DATA / f.name
        if not dest.exists() or f.stat().st_size != dest.stat().st_size:
            shutil.copy2(str(f), str(dest))
            saved += 1
    
    for f in DATA.glob("*.json"):
        dest = DRIVE_DATA / f.name
        if not dest.exists():
            shutil.copy2(str(f), str(dest))
            saved += 1
    
    if saved > 0:
        print(f"  ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ {saved} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ½Ğ° Drive (Ğ½Ğµ Ğ±ÑƒĞ´ÑƒÑ‚ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾)")


def save_models_to_drive():
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Drive."""
    if not DRIVE_MODELS:
        return
    
    tars_v3 = MODELS / "tars_v3"
    if tars_v3.exists():
        for f in tars_v3.glob("*.pt"):
            dest = DRIVE_MODELS / f.name
            shutil.copy2(str(f), str(dest))
            mb = f.stat().st_size / 1024 / 1024
            print(f"  ğŸ’¾ {f.name}: {mb:.1f} MB â†’ Drive")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. GPU Detection + Auto-Optimization
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print()
print("â•" * 65)
print("  ğŸ¤– Ğ¢ĞĞ Ğ¡ v3 â€” MEDIUM TRAINING (Colab)")
print("â•" * 65)
print()

gpu_tier = "t4"
bf16_ok = False
vram = 0

try:
    import torch
    if torch.cuda.is_available():
        gpu = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        bf16_ok = torch.cuda.get_device_capability(0) >= (8, 0)
        print(f"  ğŸ® GPU:    {gpu}")
        print(f"  ğŸ’¾ VRAM:   {vram:.1f} GB")
        print(f"  âš¡ bf16:   {'Yes' if bf16_ok else 'No (fp16)'}")
        
        if vram >= 35:
            gpu_tier = "a100"
            print(f"  ğŸ”¥ A100/H100 â†’ batch=32, bf16, ~30-45 Ğ¼Ğ¸Ğ½")
        elif vram >= 20:
            gpu_tier = "l4"
            print(f"  âš¡ L4/RTX â†’ batch=24, bf16, ~45-60 Ğ¼Ğ¸Ğ½")
        elif vram >= 14:
            gpu_tier = "t4"
            print(f"  âœ… T4 â†’ batch=16, fp16, ~1-2 Ñ‡Ğ°ÑĞ°")
        else:
            gpu_tier = "small"
            print(f"  âš ï¸  ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹ VRAM â€” batch=8")
    else:
        print("  âš ï¸  GPU Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½!")
        print("  ğŸ”§ Runtime â†’ Change runtime type â†’ L4")
        sys.exit(1)
except ImportError:
    print("  ğŸ“¦ PyTorch Ğ½Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ (Ğ±ÑƒĞ´ĞµÑ‚ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½)")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Restore cached data
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

restore_cached_data()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. Training
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

configs = {
    "a100": {"batch": 32, "accum": 1, "amp": "bf16",  "time": "30-45 Ğ¼Ğ¸Ğ½"},
    "l4":   {"batch": 24, "accum": 1, "amp": "bf16",  "time": "45-60 Ğ¼Ğ¸Ğ½"},
    "t4":   {"batch": 16, "accum": 2, "amp": "fp16",  "time": "1-2 Ñ‡Ğ°ÑĞ°"},
    "small":{"batch": 8,  "accum": 4, "amp": "fp16",  "time": "2-4 Ñ‡Ğ°ÑĞ°"},
}
cfg = configs[gpu_tier]

print()
print(f"  ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (Ğ°Ğ²Ñ‚Ğ¾-{gpu_tier.upper()}):")
print(f"    ĞœĞ¾Ğ´ĞµĞ»ÑŒ:        512d Ã— 8 ÑĞ»Ğ¾Ñ‘Ğ² (~103M params)")
print(f"    Batch:         {cfg['batch']} Ã— {cfg['accum']} = {cfg['batch']*cfg['accum']} effective")
print(f"    AMP:           {cfg['amp']}")
print(f"    Mamba-2:       10+5+3+3 = 21 ÑĞ¿Ğ¾Ñ…Ğ° Ã— 4 Ñ„Ğ°Ğ·Ñ‹ + Phase 5")
print(f"    ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ:   1.58-bit BitNet")
print(f"    Ğ’Ñ€ĞµĞ¼Ñ:         ~{cfg['time']}")
print()
print("â”€" * 65)

t0 = time.time()

# Parse extra args
extra_args = []
for arg in sys.argv[1:]:
    if arg in ("--skip-download", "--resume", "--skip-quantize"):
        extra_args.append(arg)

# mega_train.py ÑĞ°Ğ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ GPU Ğ¸ Ğ²Ñ‹Ğ±ĞµÑ€ĞµÑ‚ batch/bf16
cmd = [PYTHON, "mega_train.py", "--skip-voice", "--drive"] + extra_args
result = subprocess.run(cmd, cwd=str(ROOT))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. Save + Report
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

save_data_to_drive()
if result.returncode == 0:
    save_models_to_drive()

elapsed = time.time() - t0
hours = elapsed / 3600
minutes = elapsed / 60

print()
print("â•" * 65)
if result.returncode == 0:
    print(f"  âœ… ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ Ğ·Ğ° {minutes:.0f} Ğ¼Ğ¸Ğ½ ({hours:.1f} Ñ‡)!")
    print()
    print(f"  ĞœĞ¾Ğ´ĞµĞ»ÑŒ: 512d Ã— 8L (~103M params)")
    print()
    
    tars_v3 = ROOT / "models" / "tars_v3"
    if tars_v3.exists():
        total_mb = 0
        for f in tars_v3.glob("*.pt"):
            mb = f.stat().st_size / 1024 / 1024
            total_mb += mb
            print(f"    {f.name}: {mb:.1f} MB")
        print(f"    {'â”€' * 30}")
        print(f"    Ğ˜Ñ‚Ğ¾Ğ³Ğ¾: {total_mb:.0f} MB")
    
    print()
    if DRIVE_MODELS:
        print(f"  ğŸ’¾ ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Drive: {DRIVE_MODELS}")
        print(f"  ğŸ’¾ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Drive: {DRIVE_DATA}")
    print()
    print("  ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº: python launch_tars.py")
else:
    print(f"  âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° (ĞºĞ¾Ğ´ {result.returncode})")
    print(f"     Ğ’Ñ€ĞµĞ¼Ñ: {minutes:.0f} Ğ¼Ğ¸Ğ½")
    print()
    print("  Ğ›Ğ¾Ğ³Ğ¸: !cat mega_train.log | tail -50")
    print("  ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ: !python colab_train.py --resume")
print("â•" * 65)
