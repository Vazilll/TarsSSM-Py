"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  TarsMamba2LM ‚Äî –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å –¢–ê–†–° v3 (Deep WuNeng Core)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
  Embedding ‚Üí 12 √ó TarsBlock ‚Üí IDME ‚Üí LM Head

  –ö–∞–∂–¥—ã–π TarsBlock —Å–æ–¥–µ—Ä–∂–∏—Ç TarsCoreBlock (ssd.py) + –æ–±–≤—è–∑–∫—É:
    - TarsCoreBlock: Deep Hybrid (Mamba-2 SSD + RWKV-7 WKV + WuNeng Fusion)
       –û–±—â–∞—è in_proj / out_proj, —Å–ª–∏—è–Ω–∏–µ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
    - Œ©-SSM Lie (Cayley SO(n))
    - MoLE (per-block, sparse top-2 of 8 experts)
    - NoveltyGate + RAG injection
    
  –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
    - IDME MatrixPool (Speculative Matrix Routing)
    - Integral Auditor (p-convergence tracking)
    - Hankel SVD (anti-cycle detection)
"""

import math
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import json
import os
import logging
from typing import Optional, Tuple, Any
from torch.utils.checkpoint import checkpoint as grad_checkpoint

from brain.mamba2.tars_block import TarsBlock
from brain.mamba2.integral_auditor import IntegralAuditor, MetaAuditor
from brain.mamba2.matrix_pool import MatrixPool
from brain.mamba2.novelty import NoveltyGate, HankelDetector
from brain.mamba2.logger import ThinkingLogger
from brain.mamba2.neuromodulator import Neuromodulator
from brain.mamba2.oscillations import OscillatoryBinding
from brain.mamba2.dendrites import DendriticBlock
from brain.mamba2.hyperbolic import HyperbolicSimilarity, project_to_poincare
from brain.mamba2.active_inference import BeliefState
from brain.mamba2.thinking_chain import ThinkingChain
from brain.mamba2.bitnet import (
    UniversalLinear, convert_model_to_158bit,
    convert_model_to_fp16, model_stats, replace_linear_with_universal
)

_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class RotaryPositionEmbedding(nn.Module):
    """
    Unified RoPE: Rotary Position Embedding (Qwen3/Mamba-3 style).
    
    Uses base=1,000,000 for up to 32K+ context (vs standard 10,000).
    Applied to WKV branch for positional awareness in SSM-Attention hybrid.
    
    Math: RoPE(x, pos) = x * cos(Œ∏_pos) + rotate_half(x) * sin(Œ∏_pos)
    where Œ∏_i = pos / base^(2i/d)
    """
    
    def __init__(self, dim, base=1_000_000, max_seq_len=32768):
        super().__init__()
        self.dim = dim
        self.base = base
        self.max_seq_len = max_seq_len
        
        # Precompute inverse frequencies
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq, persistent=False)
        
        # Precompute cos/sin tables
        t = torch.arange(max_seq_len).float()
        freqs = torch.outer(t, inv_freq)  # [max_seq_len, dim//2]
        emb = torch.cat([freqs, freqs], dim=-1)  # [max_seq_len, dim]
        self.register_buffer('cos_cached', emb.cos(), persistent=False)
        self.register_buffer('sin_cached', emb.sin(), persistent=False)
    
    @staticmethod
    def _rotate_half(x):
        """Rotate half of the hidden dims: [x1, x2, ..., xn] ‚Üí [-x_{n/2+1}, ..., -xn, x1, ..., x_{n/2}]"""
        x1, x2 = x.chunk(2, dim=-1)
        return torch.cat((-x2, x1), dim=-1)
    
    def forward(self, x, seq_len=None, offset=0):
        """
        Apply RoPE to input tensor.
        
        Args:
            x: [B, L, D] or [B, H, L, D]
            seq_len: override sequence length
            offset: position offset (for cached generation)
        
        Returns:
            x with rotary position encoding applied
        """
        if seq_len is None:
            seq_len = x.shape[-2]
        
        cos = self.cos_cached[offset:offset + seq_len, :self.dim]
        sin = self.sin_cached[offset:offset + seq_len, :self.dim]
        
        # Broadcast to match x dimensions
        if x.ndim == 4:  # [B, H, L, D]
            cos = cos.unsqueeze(0).unsqueeze(0)
            sin = sin.unsqueeze(0).unsqueeze(0)
        elif x.ndim == 3:  # [B, L, D]
            cos = cos.unsqueeze(0)
            sin = sin.unsqueeze(0)
        
        # Only apply to first `dim` dimensions
        x_rope = x[..., :self.dim]
        x_rest = x[..., self.dim:]
        
        x_rope = x_rope * cos + self._rotate_half(x_rope) * sin
        
        return torch.cat([x_rope, x_rest], dim=-1) if x_rest.size(-1) > 0 else x_rope


class WaveConsolidation(nn.Module):
    """
    –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Å–ª–æ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –≤–æ–ª–Ω—ã.
    
    –ó–∞–º–µ–Ω—è–µ—Ç –ª—ë–≥–∫–∏–π WaveMerge+WaveGate.
    –≠—Ç–æ ¬´–±–æ–ª—å—à–æ–π¬ª —Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–æ–∏—Ö –±–ª–æ–∫–æ–≤
    –∏ —Ä–µ—Ñ–ª–µ–∫—Å—ã (MoLE experts) –≤ –µ–¥–∏–Ω—ã–π –≤—ã—Ö–æ–¥.
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
      1. Dimension-wise Gate: œÉ(W ¬∑ [h_L; h_R]) ‚àà (0,1)^d
         ‚Äî –Ω–µ —Å–∫–∞–ª—è—Ä, –∞ –ø–æ–ª–Ω—ã–π d_model gate
      2. Deep Fusion MLP: [h_L; h_R] ‚Üí 2d ‚Üí SiLU ‚Üí d ‚Üí d
         ‚Äî –≥–ª—É–±–æ–∫–∞—è –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è
      3. Reflex integration: —Å–∏–≥–Ω–∞–ª –æ—Ç MoLE stats –æ–±–æ–∏—Ö –±–ª–æ–∫–æ–≤
      4. Output: gate * x_left + (1-gate) * x_right + fusion + reflex
    """
    
    def __init__(self, d_model: int = 768):
        super().__init__()
        self.d_model = d_model
        
        # 1. Dimension-wise gate (–ø–æ–ª–Ω—ã–π, –Ω–µ —Å–∫–∞–ª—è—Ä)
        self.gate = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.Sigmoid(),
        )
        
        # 2. Deep Fusion MLP (–±–æ–ª—å—à–æ–π —Å–ª–æ–π)
        self.fusion = nn.Sequential(
            nn.Linear(d_model * 2, d_model * 2),
            nn.SiLU(),
            nn.Linear(d_model * 2, d_model),
            nn.SiLU(),
            nn.Linear(d_model, d_model),
        )
        
        # 3. Reflex integration gate (MoLE expert signals)
        self.reflex_gate = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.Tanh(),
        )
        
        # 4. Output normalization
        self.norm = nn.LayerNorm(d_model)
        
        # –ú–∞—Å—à—Ç–∞–± fusion –∏ reflex
        self.fusion_scale = nn.Parameter(torch.tensor(0.1))
        self.reflex_scale = nn.Parameter(torch.tensor(0.05))
    
    def forward(
        self,
        x_left: torch.Tensor,
        x_right: torch.Tensor,
        stats_l: dict = None,
        stats_r: dict = None,
    ) -> Tuple[torch.Tensor, float]:
        """
        x_left, x_right: [B, L, d_model]
        Returns: (x_merged [B, L, d_model], alpha_mean: float)
        """
        h_left = x_left.mean(dim=1)   # [B, d_model]
        h_right = x_right.mean(dim=1)  # [B, d_model]
        h_cat = torch.cat([h_left, h_right], dim=-1)  # [B, 2*d_model]
        
        # 1. Dimension-wise gate
        alpha = self.gate(h_cat)  # [B, d_model]
        alpha_3d = alpha.unsqueeze(1)  # [B, 1, d_model]
        x_gated = (1 - alpha_3d) * x_left + alpha_3d * x_right
        
        # 2. Deep fusion correction
        fusion = self.fusion(h_cat)  # [B, d_model]
        
        # 3. Reflex integration
        reflex_signal = self.reflex_gate(h_cat)  # [B, d_model]
        
        # Combine
        x = x_gated + self.fusion_scale * fusion.unsqueeze(1) \
                     + self.reflex_scale * reflex_signal.unsqueeze(1)
        
        return self.norm(x), alpha.mean().item()


class GlobalWorkspace(nn.Module):
    """
    Global Workspace Theory (Baars, 1988).
    
    –ù–µ–π—Ä–æ–Ω–∞—É–∫–∞: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç
    –∑–∞ –¥–æ—Å—Ç—É–ø –≤ ¬´–≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞–±–æ—á–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ¬ª (–∞–Ω–∞–ª–æ–≥ —Å–æ–∑–Ω–∞–Ω–∏—è).
    –ü–æ–±–µ–¥–∏—Ç–µ–ª—å —Ç—Ä–∞–Ω—Å–ª–∏—Ä—É–µ—Ç—Å—è (broadcast) –≤—Å–µ–º –º–æ–¥—É–ª—è–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
    
    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è: –≤—Å–µ 12 –±–ª–æ–∫–æ–≤ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç h_mean –≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—é.
    –ú—è–≥–∫–∏–π attention –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç ¬´–¥–æ–º–∏–Ω–∞–Ω—Ç—É¬ª. Broadcast signal
    –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é.
    
    –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:
      g = softmax(W_g ¬∑ [h‚ÇÅ ‚äï h‚ÇÇ ‚äï ... ‚äï h_n])  # competition
      broadcast = Œ£·µ¢ g·µ¢ ¬∑ h·µ¢                        # winner-take-most
    """
    
    def __init__(self, d_model: int = 768, n_blocks: int = 12):
        super().__init__()
        self.n_blocks = n_blocks
        
        # Competition: –≤—Å–µ –±–ª–æ–∫–∏ –±–æ—Ä—é—Ç—Å—è –∑–∞ –≤–Ω–∏–º–∞–Ω–∏–µ
        self.competition = nn.Linear(d_model * n_blocks, n_blocks)
        
        # Broadcast projection: —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
        self.broadcast_proj = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.SiLU(),
            nn.Linear(d_model, d_model),
        )
        
        # Mixing strength (–æ–±—É—á–∞–µ–º—ã–π)
        self.mix = nn.Parameter(torch.tensor(0.1))
        
        # Norm
        self.norm = nn.LayerNorm(d_model)
    
    def forward(self, block_outputs: list, x_current: torch.Tensor) -> torch.Tensor:
        """
        block_outputs: list of [B, L, d_model] ‚Äî outputs from each block
        x_current: [B, L, d_model] ‚Äî current stream (post all waves)
        
        Returns: x_current + broadcast signal
        """
        if len(block_outputs) < 2:
            return x_current
        
        # h_mean per block: [B, d_model] each
        h_means = [h.mean(dim=1) for h in block_outputs]
        
        # Pad if fewer blocks than expected
        while len(h_means) < self.n_blocks:
            h_means.append(torch.zeros_like(h_means[0]))
        h_means = h_means[:self.n_blocks]  # trim if more
        
        # Concatenate all block representations
        h_cat = torch.cat(h_means, dim=-1)  # [B, d_model * n_blocks]
        
        # Competition gate: who gets the workspace?
        gates = F.softmax(self.competition(h_cat), dim=-1)  # [B, n_blocks]
        
        # Winner-take-most broadcast
        broadcast = torch.zeros_like(h_means[0])  # [B, d_model]
        for i, h in enumerate(h_means):
            broadcast = broadcast + gates[:, i:i+1] * h
        
        # Project and mix
        broadcast = self.broadcast_proj(broadcast)  # [B, d_model]
        
        return self.norm(x_current + self.mix * broadcast.unsqueeze(1))


class TarsMamba2LM(nn.Module):
    """
    –¢–ê–†–° v3: Deep WuNeng Core (Mamba-2 + RWKV-7 inside one kernel).
    
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –º–æ–∑–≥ —Å –µ–¥–∏–Ω—ã–º —è–¥—Ä–æ–º TarsCoreBlock:
    Mamba-2 SSD –∏ RWKV-7 WKV –¥–µ–ª—è—Ç –æ–±—â–∏–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –∏ —Å–ª–∏–≤–∞—é—Ç—Å—è
    –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —á–µ—Ä–µ–∑ Deep Gated Fusion.
    
    ~130M params (fp16: ~260MB, 1.58-bit: ~60MB)
    """
    
    @classmethod
    def from_config(cls, config_path=None, device="cpu"):
        """–°–æ–∑–¥–∞—ë—Ç –º–æ–¥–µ–ª—å –∏–∑ config.json."""
        if config_path is None:
            config_path = os.path.join(_ROOT, "models", "tars_v3", "config.json")
        with open(config_path, "r", encoding="utf-8") as f:
            cfg = json.load(f)
        p = cfg["models"]["mamba2"]["params"]
        return cls(
            d_model=p.get("d_model", 768), n_layers=p.get("n_layers", 12),
            vocab_size=p.get("vocab_size", 256), d_state=p.get("d_state", 64),
            headdim=p.get("headdim", 64), omega_dim=p.get("omega_dim", 32),
            pool_size=p.get("pool_size", 48), n_experts=p.get("n_experts", 8),
        ).to(device)
    
    @classmethod
    def load_pretrained(cls, checkpoint_path=None, config_path=None, device="cpu"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ config + checkpoint."""
        _logger = logging.getLogger("Tars.Mamba2LM")
        model = cls.from_config(config_path, device)
        
        if checkpoint_path is None:
            for p in [
                os.path.join(_ROOT, "models", "tars_v3", "mamba2.pt"),
                os.path.join(_ROOT, "models", "mamba2", "mamba2_omega_158bit.pt"),
                os.path.join(_ROOT, "models", "mamba2", "mamba2_omega.pt"),
            ]:
                if os.path.exists(p):
                    checkpoint_path = p
                    break
        
        if checkpoint_path and os.path.exists(checkpoint_path):
            _logger.info(f"Loading weights: {checkpoint_path}")
            cp = torch.load(checkpoint_path, map_location=device, weights_only=False)
            state = cp.get("model_state_dict", cp)
            model_state = model.state_dict()
            loaded, skipped = 0, 0
            for key, value in state.items():
                if key in model_state and model_state[key].shape == value.shape:
                    model_state[key] = value
                    loaded += 1
                else:
                    skipped += 1
            model.load_state_dict(model_state, strict=False)
            _logger.info(f"Loaded {loaded} tensors, skipped {skipped}")
            return model, checkpoint_path
        
        _logger.warning("No checkpoint found ‚Äî UNTRAINED")
        return model, None
    
    def __init__(
        self,
        d_model: int = 2048,     # TARS 1B: max intelligence in 1GB RAM
        n_layers: int = 24,      # 24 rich blocks ‚âà 36 vanilla blocks
        vocab_size: int = 32000,
        d_state: int = 128,      # 2x working memory (BitMamba-2 style)
        headdim: int = 64,
        mingru_dim: int = 256,  # legacy arg, ignored
        omega_dim: int = 32,
        pool_size: int = 48,
        n_experts: int = 8,
        expert_rank: int = 8,
        quant_mode: str = "fp16",
    ):
        super().__init__()
        self.d_model = d_model
        self.n_layers = n_layers
        self.vocab_size = vocab_size
        self.quant_mode = quant_mode
        self.logger = logging.getLogger("Tars.Mamba2LM")
        
        # ‚ïê‚ïê‚ïê Embedding ‚ïê‚ïê‚ïê
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # ‚ïê‚ïê‚ïê –û—Å–Ω–æ–≤–Ω—ã–µ –±–ª–æ–∫–∏ (12 √ó TarsBlock) ‚Äî Cortical Columns ‚ïê‚ïê‚ïê
        # –ù–µ–π—Ä–æ–Ω–∞—É–∫–∞: –∫–æ—Ä–∞ –º–æ–∑–≥–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ –≥–ª—É–±–∏–Ω–µ:
        #   –†–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ (V1/V2)     ‚Üí –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ª–µ–∫—Å–∏–∫–∞, –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è)
        #   –°—Ä–µ–¥–Ω–∏–µ —Å–ª–æ–∏ (IT/STS)   ‚Üí —Å–µ–º–∞–Ω—Ç–∏–∫–∞, –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        #   –ì–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏ (PFC/ACC) ‚Üí –ª–æ–≥–∏–∫–∞, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏
        # –ö–∞–∂–¥—ã–π "—Å–ª–æ–π –∫–æ—Ä—ã" –∏–º–µ–µ—Ç —Å–≤–æ—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é omega/experts/dropout.
        self.blocks = nn.ModuleList()
        for i in range(n_layers):
            depth_ratio = i / max(n_layers - 1, 1)  # 0.0 ‚Üí 1.0
            
            # Cortical Column config –ø–æ –≥–ª—É–±–∏–Ω–µ
            layer_omega = int(omega_dim * (0.5 + 1.5 * depth_ratio))  # 16‚Üí80 –ø—Ä–∏ omega_dim=32
            layer_experts = max(4, int(n_experts * (0.5 + 0.5 * depth_ratio)))  # 4‚Üí8
            layer_dropout = 0.05 + 0.10 * depth_ratio  # 0.05‚Üí0.15
            
            self.blocks.append(TarsBlock(
                d_model=d_model, d_state=d_state,
                headdim=headdim, omega_dim=layer_omega,
                n_experts=layer_experts, layer_idx=i,
                quant_mode=quant_mode,
                dropout=layer_dropout,
            ))
        
        # ‚ïê‚ïê‚ïê Wave Consolidation (6 —Å–ª–æ—ë–≤ –¥–ª—è 12/2=6 –≤–æ–ª–Ω) ‚ïê‚ïê‚ïê
        # –ö–∞–∂–¥—ã–π consolidation ‚Äî –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Å–ª–æ–π —Å–ª–∏—è–Ω–∏—è:
        #   1. Dimension-wise gate (–Ω–µ —Å–∫–∞–ª—è—Ä, –∞ –ø–æ–ª–Ω—ã–π d_model)
        #   2. Deep fusion MLP (2d ‚Üí 2d ‚Üí d ‚Üí d) 
        #   3. Reflex integration (MoLE expert signal)
        n_waves = n_layers // 2
        self.wave_consolidations = nn.ModuleList([
            WaveConsolidation(d_model) for _ in range(n_waves)
        ])
        
        # ‚ïê‚ïê‚ïê Global Workspace (Baars, 1988) ‚ïê‚ïê‚ïê
        # –í—Å–µ –±–ª–æ–∫–∏ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—Ç –∑–∞ –¥–æ—Å—Ç—É–ø –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ.
        # –ü–æ–±–µ–¥–∏—Ç–µ–ª—å broadcast'–∏—Ç —Å–≤–æ—ë –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.
        self.global_workspace = GlobalWorkspace(d_model, n_layers)
        
        # ‚ïê‚ïê‚ïê IDME Matrix Pool (48+, –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ) ‚ïê‚ïê‚ïê
        self.matrix_pool = MatrixPool(d_model, pool_size)
        
        # ‚ïê‚ïê‚ïê Integral Auditor ‚ïê‚ïê‚ïê
        self.integral_auditor = IntegralAuditor(window=8, default_threshold=1.1)
        self.meta_auditor = MetaAuditor()
        
        # ‚ïê‚ïê‚ïê Hankel SVD (–≥–ª–æ–±–∞–ª—å–Ω—ã–π –∞–Ω—Ç–∏-—Ü–∏–∫–ª) ‚ïê‚ïê‚ïê
        self.hankel = HankelDetector(window=6)
        
        # ‚ïê‚ïê‚ïê NoveltyGate (–¥–ª—è IDME) ‚ïê‚ïê‚ïê
        self.novelty_gate = NoveltyGate(d_model)
        
        # ‚ïê‚ïê‚ïê Neuromodulator (DA, NA, ACh, 5HT) ‚ïê‚ïê‚ïê
        # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –Ω–µ–π—Ä–æ–º–æ–¥—É–ª—è—Ü–∏—è: 4 –Ω–µ–π—Ä–æ–º–µ–¥–∏–∞—Ç–æ—Ä–∞ –º–æ–¥—É–ª–∏—Ä—É—é—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.
        #   DA ‚Üí MoLE routing sharpness
        #   NA ‚Üí thinking depth (p-threshold)
        #   ACh ‚Üí self-learning LR
        #   5HT ‚Üí patience (max depth)
        self.neuromodulator = Neuromodulator(d_model)
        
        # ‚ïê‚ïê‚ïê Oscillatory Binding (Œ∏-Œ≥ phase coding) ‚ïê‚ïê‚ïê
        # Œ∏-—Ä–∏—Ç–º (hippocampus) –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç memory encoding,
        # Œ≥-—Ä–∏—Ç–º (cortex) –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤–Ω—É—Ç—Ä–∏ Œ∏-—Ü–∏–∫–ª–∞.
        self.oscillatory = OscillatoryBinding(d_model)
        
        # ‚ïê‚ïê‚ïê Active Dendrites (Numenta, 2021-2025) ‚ïê‚ïê‚ïê
        # –î–µ–Ω–¥—Ä–∏—Ç–Ω–∞—è –º–æ–¥—É–ª—è—Ü–∏—è: –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ –≤—ã–±–∏—Ä–∞–µ—Ç
        # —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ç–≤—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ WTA selection.
        self.dendritic_block = DendriticBlock(d_model, d_model, n_segments=7)
        
        # ‚ïê‚ïê‚ïê Hyperbolic Similarity (Poincar√© ball) ‚ïê‚ïê‚ïê
        # –ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
        # (–¥–µ—Ä–µ–≤—å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π, —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏, memory hierarchy).
        self.hyper_sim = HyperbolicSimilarity(c=1.0, scale=1.0)
        
        # ‚ïê‚ïê‚ïê Active Inference: Belief State (Friston, 2006) ‚ïê‚ïê‚ïê
        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —É–±–µ–∂–¥–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ q(s).
        # –û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ precision-weighted update.
        self.belief_state = BeliefState(d_state=128)
        
        # ‚ïê‚ïê‚ïê Thinking Logger ‚ïê‚ïê‚ïê
        self.thinking_logger = ThinkingLogger()
        
        # ‚ïê‚ïê‚ïê Titans Memory Hook (384d LTM) ‚ïê‚ïê‚ïê
        # –ü—Ä–æ–µ–∫—Ü–∏—è d_model ‚Üí 384d (–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–∞–º—è—Ç–∏ LEANN/Titans)
        self.mem_dim = 384
        self.to_memory_space = nn.Linear(d_model, self.mem_dim, bias=False)
        self.from_memory_space = nn.Linear(self.mem_dim, d_model, bias=False)
        self.titans_memory = None  # Set externally: model.titans_memory = TitansMemory(384)
        
        # ‚ïê‚ïê‚ïê Output head ‚ïê‚ïê‚ïê
        self.norm_f = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)
        
        # Tie weights
        self.lm_head.weight = self.embedding.weight
        
        # Init
        self.apply(self._init_weights)
        
        # Gradient checkpointing flag
        self.use_checkpointing = False
        
        # MoLE auxiliary loss (accumulated during forward)
        self.mole_aux_loss = torch.tensor(0.0)
        
        # ‚ïê‚ïê‚ïê Speculative Decoding (Granite 4.0) ‚ïê‚ïê‚ïê
        self.spec_draft_head = nn.Sequential(
            nn.Linear(d_model, d_model // 4),
            nn.SiLU(),
            nn.Linear(d_model // 4, vocab_size),
        )
        
        # ‚ïê‚ïê‚ïê Native Chain-of-Thought v2: 5 –ø–æ–¥—Å–∏—Å—Ç–µ–º ‚ïê‚ïê‚ïê
        # 1. Retrieval-triggered RAG –º–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏
        # 2. Multi-scale memory (working/session/longterm)
        # 3. Confidence-gated output (–Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Üí smoothing)
        # 4. Wave skip (–µ—Å–ª–∏ confidence > 0.9)
        # 5. Self-verification (–ø–æ–≤—Ç–æ—Ä–Ω—ã–π –ø—Ä–æ–≥–æ–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)
        self.thinking_chain = ThinkingChain(
            d_model, d_memory=384, n_max_waves=n_layers // 2, vocab_size=vocab_size
        )
        
        # ‚ïê‚ïê‚ïê Cached SSM states for fast generation ‚ïê‚ïê‚ïê
        self._gen_cache = None  # Initialized by reset_cache()
        self._prefix_cache = None  # For prefix caching (system prompt)
        
        # ‚ïê‚ïê‚ïê Unified RoPE for WKV branch (Qwen3/Mamba-3 style) ‚ïê‚ïê‚ïê
        self.rope = RotaryPositionEmbedding(d_model // (d_model // 64), base=1_000_000)
    
    def reset_cache(self):
        """–°–±—Ä–æ—Å –∫–µ—à–∞ SSM-—Å–æ—Å—Ç–æ—è–Ω–∏–π (–≤—ã–∑—ã–≤–∞—Ç—å –ø–µ—Ä–µ–¥ –Ω–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–æ–º)."""
        self._gen_cache = {
            "wkv_states": [None] * self.n_layers,
            "x_prevs": [None] * self.n_layers,
            "ssd_states": [None] * self.n_layers,
            "conv_states": [None] * self.n_layers,
            "memory_vec": None,
        }
    
    def prefix_cache_save(self):
        """
        Prefix Caching: —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—É—â–µ–µ SSM-—Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–∞–∫ "prefix".
        
        –í—ã–∑—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ system prompt. –ü—Ä–∏ —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö
        —Å —Ç–µ–º –∂–µ system prompt ‚Äî –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ prefix_cache_load().
        
        –≠—Ñ—Ñ–µ–∫—Ç: 2-5x speedup –Ω–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö.
        """
        import copy
        if self._gen_cache is not None:
            self._prefix_cache = copy.deepcopy(self._gen_cache)
    
    def prefix_cache_load(self):
        """
        –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å SSM-—Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ prefix cache.
        
        Usage:
            model.reset_cache()
            model.step(system_prompt_ids)
            model.prefix_cache_save()  # save state after system prompt
            
            # For each user query:
            model.prefix_cache_load()  # restore, skip system prompt processing 
            model.step(user_query_ids)
        """
        import copy
        if self._prefix_cache is not None:
            self._gen_cache = copy.deepcopy(self._prefix_cache)
            return True
        return False
    
    @torch.no_grad()
    def step(self, token_ids: torch.Tensor) -> torch.Tensor:
        """
        Fast single-step forward –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ SSM-—Å–æ—Å—Ç–æ—è–Ω–∏—è ‚Äî –Ω–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç –∏—Ö.
        –í—ã–∑—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ reset_cache() + prefill —á–µ—Ä–µ–∑ forward().
        
        token_ids: [B, L] (–æ–±—ã—á–Ω–æ L=1 –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏)
        Returns: logits [B, L, vocab_size]
        """
        if self._gen_cache is None:
            self.reset_cache()
        
        c = self._gen_cache
        x = self.embedding(token_ids)
        
        n_waves = self.n_layers // 2
        
        for wave_idx in range(n_waves):
            b_left = wave_idx * 2
            b_right = wave_idx * 2 + 1
            
            x_left, c["wkv_states"][b_left], c["x_prevs"][b_left], _, \
                c["ssd_states"][b_left], c["conv_states"][b_left] = self.blocks[b_left](
                    x, c["wkv_states"][b_left], c["x_prevs"][b_left],
                    c["memory_vec"], None,
                    c["ssd_states"][b_left], c["conv_states"][b_left]
                )
            x_right, c["wkv_states"][b_right], c["x_prevs"][b_right], _, \
                c["ssd_states"][b_right], c["conv_states"][b_right] = self.blocks[b_right](
                    x, c["wkv_states"][b_right], c["x_prevs"][b_right],
                    c["memory_vec"], None,
                    c["ssd_states"][b_right], c["conv_states"][b_right]
                )
            
            # Wave Consolidation (full merge layer)
            x, _ = self.wave_consolidations[wave_idx](x_left, x_right)
            
            # Spine: –æ–±–Ω–æ–≤–ª—è–µ–º –ø–∞–º—è—Ç—å –º–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏
            if wave_idx < n_waves - 1 and hasattr(self, 'to_memory_space'):
                try:
                    h_curr = x.mean(dim=1)
                    h_for_mem = self.to_memory_space(h_curr)
                    if c["memory_vec"] is None:
                        c["memory_vec"] = h_for_mem
                    else:
                        c["memory_vec"] = 0.7 * c["memory_vec"] + 0.3 * h_for_mem
                except Exception:
                    pass
        
        # Final memory injection + output
        if c["memory_vec"] is not None and hasattr(self, 'from_memory_space'):
            try:
                mem_signal = self.from_memory_space(c["memory_vec"])
                x = x + 0.1 * mem_signal.unsqueeze(1)
            except Exception:
                pass
        
        x = self.norm_f(x)
        return self.lm_head(x)
    
    @torch.no_grad()
    def generate_speculative(
        self,
        prompt_ids: torch.Tensor,
        max_tokens: int = 128,
        n_draft: int = 4,
        temperature: float = 0.7,
        top_k: int = 50,
    ) -> torch.Tensor:
        """
        Speculative Decoding ‚Äî 2-3x speedup for generation.
        
        Algorithm:
        1. Draft head predicts N future tokens (fast, ~1M params)
        2. Main model verifies them in 1 forward pass
        3. Accept prefix of correct tokens, reject rest
        4. Continue from last accepted token
        
        Args:
            prompt_ids: [1, L] initial tokens
            max_tokens: max tokens to generate
            n_draft: number of speculative tokens per step (4-8 optimal)
            temperature: sampling temperature
            top_k: top-k sampling
        
        Returns:
            generated: [1, L + max_tokens] full sequence
        """
        self.eval()
        device = prompt_ids.device
        
        # Prefill: process entire prompt
        self.reset_cache()
        logits = self.step(prompt_ids)  # [1, L, V]
        
        generated = prompt_ids.clone()  # [1, L]
        n_accepted = 0
        n_total = 0
        
        for _ in range(max_tokens):
            if generated.shape[1] - prompt_ids.shape[1] >= max_tokens:
                break
            
            # Get last hidden state for draft predictions
            last_logits = logits[:, -1, :]  # [1, V]
            
            # Sample from main model for first token
            first_token = self._sample(last_logits, temperature, top_k)
            
            # Draft: predict N more tokens using lightweight head
            draft_tokens = [first_token]
            draft_input = first_token
            
            for _ in range(n_draft - 1):
                draft_logits = self.step(draft_input.unsqueeze(1))
                draft_next = self._sample(
                    draft_logits[:, -1, :], temperature, top_k
                )
                draft_tokens.append(draft_next)
                draft_input = draft_next
            
            # We've already processed the draft tokens through step()
            # The cache now contains states for all draft tokens
            # If draft was wrong, we need to rewind ‚Äî for SSM this means
            # we accept what we've generated since SSM state is cumulative
            
            # Append all draft tokens (SSM doesn't need verification rewind)
            # In SSM models, speculative decoding is simpler than in Transformers
            # because we can't easily "undo" state updates
            draft_tensor = torch.stack(draft_tokens, dim=0).unsqueeze(0)  # [1, N]
            generated = torch.cat([generated, draft_tensor], dim=1)
            
            n_accepted += len(draft_tokens)
            n_total += len(draft_tokens)
            
            # Get logits for next iteration
            logits = self.step(draft_tokens[-1].unsqueeze(0).unsqueeze(0))
        
        return generated
    
    def _sample(self, logits, temperature=0.7, top_k=50):
        """Sample a token from logits with temperature and top-k."""
        if temperature <= 0:
            return logits.argmax(dim=-1)
        
        logits = logits / temperature
        
        if top_k > 0:
            v, _ = logits.topk(top_k, dim=-1)
            logits[logits < v[:, -1:]] = float('-inf')
        
        probs = F.softmax(logits, dim=-1)
        return torch.multinomial(probs, 1).squeeze(-1)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, std=0.02)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        memory_vec: Optional[torch.Tensor] = None,
        rag_state: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Forward pass (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏).
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Parallel Wave Merge: [B0||B1] ‚Üí gate ‚Üí merge ‚Üí ...
        –í—Å–µ wave_consolidations –æ–±—É—á–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ backprop.
        
        input_ids: [B, L]
        Returns: logits [B, L, vocab_size]
        """
        x = self.embedding(input_ids)  # [B, L, d_model]
        
        wkv_states = [None] * self.n_layers
        x_prevs = [None] * self.n_layers
        ssd_states = [None] * self.n_layers
        conv_states = [None] * self.n_layers
        
        # Accumulate MoLE aux loss from all blocks
        mole_aux_total = torch.tensor(0.0, device=input_ids.device)
        
        # Parallel Wave through TarsBlocks
        n_waves = self.n_layers // 2
        
        for wave_idx in range(n_waves):
            b_left = wave_idx * 2
            b_right = wave_idx * 2 + 1
            
            if b_right >= self.n_layers:
                break
            
            # 2 –±–ª–æ–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            if self.use_checkpointing and self.training:
                x_left, wkv_states[b_left], x_prevs[b_left], stats_l, \
                    ssd_states[b_left], conv_states[b_left] = grad_checkpoint(
                    self.blocks[b_left], x, wkv_states[b_left],
                    x_prevs[b_left], memory_vec, rag_state,
                    ssd_states[b_left], conv_states[b_left],
                    use_reentrant=False
                )
                x_right, wkv_states[b_right], x_prevs[b_right], stats_r, \
                    ssd_states[b_right], conv_states[b_right] = grad_checkpoint(
                    self.blocks[b_right], x, wkv_states[b_right],
                    x_prevs[b_right], memory_vec, rag_state,
                    ssd_states[b_right], conv_states[b_right],
                    use_reentrant=False
                )
            else:
                x_left, wkv_states[b_left], x_prevs[b_left], stats_l, \
                    ssd_states[b_left], conv_states[b_left] = self.blocks[b_left](
                    x, wkv_states[b_left], x_prevs[b_left], memory_vec, rag_state,
                    ssd_states[b_left], conv_states[b_left]
                )
                x_right, wkv_states[b_right], x_prevs[b_right], stats_r, \
                    ssd_states[b_right], conv_states[b_right] = self.blocks[b_right](
                    x, wkv_states[b_right], x_prevs[b_right], memory_vec, rag_state,
                    ssd_states[b_right], conv_states[b_right]
                )
            
            # Collect MoLE aux losses from both blocks
            for stats in [stats_l, stats_r]:
                if isinstance(stats, dict) and "mole_aux_loss" in stats:
                    mole_aux_total = mole_aux_total + stats["mole_aux_loss"]
            
            # Wave Consolidation: full merge with reflex integration
            x, _ = self.wave_consolidations[wave_idx](x_left, x_right, stats_l, stats_r)
            
            # ‚ïê‚ïê‚ïê –°–ø–∏–Ω–Ω–æ–π –º–æ–∑–≥: –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏ (Tars.txt ¬ß2.4) ‚ïê‚ïê‚ïê
            # –ë–µ–∑ —ç—Ç–æ–≥–æ to_memory_space, mem_query_proj, mem_gate –Ω–µ –ø–æ–ª—É—á–∞—é—Ç
            # –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã. –ö–∞–∂–¥–∞—è —Å–ª–µ–¥—É—é—â–∞—è –≤–æ–ª–Ω–∞ –¥–æ–ª–∂–Ω–∞ –ø–æ–ª—É—á–∞—Ç—å –æ–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
            if wave_idx < n_waves - 1:
                h_curr = x.mean(dim=1)  # [B, d_model]
                h_for_mem = self.to_memory_space(h_curr)  # [B, 384]
                if memory_vec is None:
                    # –ü–µ—Ä–≤–∞—è –≤–æ–ª–Ω–∞ ‚Äî –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º memory_vec –∏–∑ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –º–æ–∑–≥–∞
                    memory_vec = h_for_mem
                else:
                    # –°–ø–∏–Ω–Ω–æ–π –º–æ–∑–≥: 70% —Å—Ç–∞—Ä–∞—è –ø–∞–º—è—Ç—å + 30% –Ω–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã (¬ß2.4)
                    memory_vec = 0.7 * memory_vec + 0.3 * h_for_mem
        
        # Handle odd number of blocks
        if self.n_layers % 2 == 1:
            last_block = self.blocks[-1]
            x, _, _, stats_last, _, _ = last_block(x, None, None, memory_vec, rag_state)
            if isinstance(stats_last, dict) and "mole_aux_loss" in stats_last:
                mole_aux_total = mole_aux_total + stats_last["mole_aux_loss"]
        
        # Store for external access (training loop)
        self.mole_aux_loss = mole_aux_total
        
        # Output
        x = self.norm_f(x)
        logits = self.lm_head(x)
        
        if labels is not None:
            lm_loss = F.cross_entropy(
                logits.view(-1, self.vocab_size),
                labels.view(-1),
                ignore_index=-100
            )
            # Combine LM loss + MoLE auxiliary loss (load balancing + z-loss)
            total_loss = lm_loss + mole_aux_total
            return logits, total_loss
        
        return logits
    
    def think(
        self,
        input_ids: torch.Tensor,
        query_text: str = "",
        memory_vec: Optional[torch.Tensor] = None,
        rag_state: Optional[torch.Tensor] = None,
        force_deep: bool = False,
        max_expansion_rounds: int = 12,
        reflex_ctx: Any = None,
        supplement_queue = None,
    ) -> Tuple[torch.Tensor, dict]:
        """
        –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π forward —Å Integral Auditor –∏ Speculative Matrix Routing.
        
        –ù–æ–≤–æ–µ: reflex_ctx (ReflexContext) —É–ø—Ä–∞–≤–ª—è–µ—Ç –≥–ª—É–±–∏–Ω–æ–π:
          - trivial (depth=4): –ø—Ä–æ—Ö–æ–¥–∏–º —Ç–æ–ª—å–∫–æ 4 –±–ª–æ–∫–∞
          - simple (depth=6):  6 –±–ª–æ–∫–æ–≤
          - complex (depth=12): –≤—Å–µ 12 –±–ª–æ–∫–æ–≤ + IDME
        
        supplement_queue: thread-safe Queue —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è–º–∏ –æ—Ç –≥–æ–ª–æ—Å–∞.
          –ú–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –Ω–∞ –Ω–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ä–µ—á–∏.
          –ï—Å–ª–∏ –µ—Å—Ç—å ‚Äî –∏–Ω–∂–µ–∫—Ç–∏–º —á–µ—Ä–µ–∑ 2 –±–ª–æ–∫–∞ + WaveConsolidation + spine.
          –ü–æ—Å–ª–µ –∏–Ω–∂–µ–∫—Ü–∏–∏ ‚Äî 3 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–ª–Ω—ã (6 –±–ª–æ–∫–æ–≤).
        """
        start_time = time.time()
        
        # Reset –∞—É–¥–∏—Ç–æ—Ä–æ–≤
        self.integral_auditor.reset()
        self.matrix_pool.reset()
        self.hankel.reset()
        self.thinking_chain.reset()  # –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî –Ω–æ–≤–∞—è —Ü–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
        
        # Meta-Auditor: —Ç–∏–ø –∑–∞–¥–∞—á–∏ –∏ –ø–æ—Ä–æ–≥
        task_type, p_threshold = self.meta_auditor.classify_task(query_text)
        self.integral_auditor.threshold = p_threshold
        
        # Start session AFTER task classification
        self.thinking_logger.start_session(query_text, task_type, p_threshold)
        
        # ‚ïê‚ïê‚ïê Adaptive Depth from ReflexContext ‚ïê‚ïê‚ïê
        estimated_depth = self.n_layers  # default: all layers
        reflex_urgency = 0.0
        
        if reflex_ctx is not None:
            estimated_depth = min(
                getattr(reflex_ctx, 'estimated_depth', self.n_layers),
                self.n_layers
            )
            # Use reflex memory_vec if provided and not already set
            if memory_vec is None and getattr(reflex_ctx, 'memory_vec', None) is not None:
                memory_vec = reflex_ctx.memory_vec
            # Use reflex max_expansion_rounds
            if getattr(reflex_ctx, 'needs_idme', False):
                max_expansion_rounds = getattr(reflex_ctx, 'max_expansion_rounds', 12)
            else:
                max_expansion_rounds = 2  # Skip IDME for simple queries
            reflex_urgency = getattr(reflex_ctx, 'urgency', 0.0)
        
        # –õ–∏–º–∏—Ç—ã —Ä–∞—É–Ω–¥–æ–≤ –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏
        TASK_MAX_ROUNDS = {
            "chat": 2, "action": 2, "code": 6,
            "math": 8, "deep": 20, "infinite": 100,
        }
        task_max = TASK_MAX_ROUNDS.get(task_type, 4)
        max_expansion_rounds = min(max_expansion_rounds, task_max)
        if force_deep:
            max_expansion_rounds = 100
            estimated_depth = self.n_layers
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # 1. Parallel Wave Depth (2 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–∞ ‚Üí merge ‚Üí spine)
        #
        # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å:  [B0 || B1] ‚Üí merge ‚Üí spine ‚Üí –°–û–®–õ–û–°–¨ (2 –±–ª–æ–∫–∞)
        # –°—Ä–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å:  ... ‚Üí [B2 || B3] ‚Üí merge ‚Üí spine ‚Üí –°–û–®–õ–û–°–¨ (4)
        # –°–ª–æ–∂–Ω—ã–π –∑–∞–ø—Ä–æ—Å:  ... –≤—Å–µ 6 –≤–æ–ª–Ω (12 –±–ª–æ–∫–æ–≤) ‚Üí IDME
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        x = self.embedding(input_ids)
        
        wkv_states = [None] * self.n_layers   # WKV state per block
        x_prevs = [None] * self.n_layers      # time-shift per block
        ssd_states = [None] * self.n_layers   # SSD recurrent state per block
        conv_states = [None] * self.n_layers  # Conv1d rolling state per block
        h_prev = x.mean(dim=1).detach()
        
        block_stats = []
        blocks_executed = 0
        surprise_signals = []
        converged_early = False
        wave_count = 0
        per_wave_experts = []  # [{"wave": 1, "left": [...], "right": [...]}]
        supplement_injected = False
        supplement_extra_waves = 0  # —Å–∫–æ–ª—å–∫–æ –¥–æ–ø–≤–æ–ª–Ω –æ—Å—Ç–∞–ª–æ—Å—å
        
        max_waves = self.n_layers // 2   # 12/2 = 6 –≤–æ–ª–Ω
        
        # ‚ïê‚ïê‚ïê Thought Cache Shortcut (v3) ‚ïê‚ïê‚ïê
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å—Ç—å –ª–∏ –ø–æ—Ö–æ–∂–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –º—ã—à–ª–µ–Ω–∏—è –≤ –∫—ç—à–µ.
        # –ï—Å–ª–∏ –¥–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º explore/analyze —Ñ–∞–∑—ã.
        cache_skip_waves = 0
        if hasattr(self, 'thinking_chain'):
            try:
                cache_skip_waves = self.thinking_chain.try_cache_shortcut(x.mean(dim=1))
                cached_mem = self.thinking_chain.get_cached_memory()
                if cached_mem is not None and cache_skip_waves > 0:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º memory_vec –∏–∑ –∫—ç—à–∞
                    memory_vec = cached_mem.to(x.device)
                    self.logger.debug(
                        f"ThoughtCache hit: skip {cache_skip_waves} waves"
                    )
            except Exception:
                cache_skip_waves = 0
        
        # ‚ïê‚ïê‚ïê Cross-Wave Residual Storage (v3) ‚ïê‚ïê‚ïê
        wave_outputs = {}  # {wave_idx: x_output} –¥–ª—è skip connections
        
        for wave_idx in range(max_waves):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º depth limit
            if blocks_executed >= estimated_depth and not force_deep:
                break
            
            # Thought Cache: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–Ω–Ω–∏–µ –≤–æ–ª–Ω—ã –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –∫—ç—à
            if wave_idx < cache_skip_waves:
                wave_count += 1
                blocks_executed += 2
                continue
            
            wave_count += 1
            b_left = wave_idx * 2       # –∏–Ω–¥–µ–∫—Å –ª–µ–≤–æ–≥–æ –±–ª–æ–∫–∞
            b_right = wave_idx * 2 + 1  # –∏–Ω–¥–µ–∫—Å –ø—Ä–∞–≤–æ–≥–æ –±–ª–æ–∫–∞
            
            if b_right >= self.n_layers:
                break
            
            # ‚îÄ‚îÄ 2 –±–ª–æ–∫–∞ —Ä–∞–±–æ—Ç–∞—é—Ç –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û –Ω–∞ –æ–¥–Ω–æ–º –≤—Ö–æ–¥–µ ‚îÄ‚îÄ
            x_left, wkv_states[b_left], x_prevs[b_left], stats_l, \
                ssd_states[b_left], conv_states[b_left] = self.blocks[b_left](
                x, wkv_states[b_left], x_prevs[b_left], memory_vec, rag_state,
                ssd_states[b_left], conv_states[b_left]
            )
            x_right, wkv_states[b_right], x_prevs[b_right], stats_r, \
                ssd_states[b_right], conv_states[b_right] = self.blocks[b_right](
                x, wkv_states[b_right], x_prevs[b_right], memory_vec, rag_state,
                ssd_states[b_right], conv_states[b_right]
            )
            block_stats.extend([stats_l, stats_r])
            blocks_executed += 2
            
            # –°–æ–±–∏—Ä–∞–µ–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å –∫–∞–∂–¥–æ–π –≤–æ–ª–Ω—ã
            wave_experts = {
                "wave": wave_count,
                "left": stats_l.get("mole_experts", []),
                "right": stats_r.get("mole_experts", []),
            }
            per_wave_experts.append(wave_experts)
            
            # ‚îÄ‚îÄ WaveConsolidation: –ø–æ–ª–Ω—ã–π —Å–ª–æ–π —Å–ª–∏—è–Ω–∏—è ‚îÄ‚îÄ
            x, merge_alpha = self.wave_consolidations[wave_idx](
                x_left, x_right, stats_l, stats_r
            )
            
            # ‚ïê‚ïê‚ïê Cross-Wave Residual (v3) ‚ïê‚ïê‚ïê
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã—Ö–æ–¥ –≤–æ–ª–Ω—ã –¥–ª—è skip connection
            wave_outputs[wave_idx] = x.detach()
            
            # –î–æ–±–∞–≤–ª—è–µ–º skip connection –æ—Ç –≤–æ–ª–Ω—ã (i-3) –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            # –≠—Ç–æ –∫–∞–∫ ResNet skip connections –º–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏
            skip_from = wave_idx - 3
            if skip_from in wave_outputs:
                x = x + 0.1 * wave_outputs[skip_from]  # –º—è–≥–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º merge –¥–∞–Ω–Ω—ã–µ –≤ wave_experts
            wave_experts["merge_alpha"] = merge_alpha
            
            # –°–æ–±–∏—Ä–∞–µ–º surprise
            for stats in [stats_l, stats_r]:
                if stats.get("surprise", 0.0) > 0.3:
                    surprise_signals.append({
                        "layer": stats["layer_idx"],
                        "surprise": stats["surprise"],
                        "mem_relevance": stats.get("mem_relevance", 0.0),
                    })
            
            # ‚îÄ‚îÄ Integral Auditor: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ ‚îÄ‚îÄ
            h_curr = x.mean(dim=1).detach()
            ia_result = self.integral_auditor.observe(h_curr, h_prev)
            h_prev = h_curr
            
            self.thinking_logger.log_step(blocks_executed - 1, {
                "p": ia_result["p"], "r_squared": ia_result["r_squared"],
                "f_t": ia_result["f_t"], "converged": ia_result["converged"],
                "wave": wave_count, "depth": blocks_executed,
                "width": 2,
            })
            
            self.logger.debug(
                f"Wave {wave_count}: [B{b_left}||B{b_right}] ‚Üí merge ‚Üí "
                f"p={ia_result['p']:.3f} | converged={ia_result['converged']}"
            )
            
            # ‚îÄ‚îÄ –°–æ—à–ª–æ—Å—å? (–¥–≤–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è: IA convergence –ò–õ–ò ThinkingChain confidence) ‚îÄ‚îÄ
            ia_converged = ia_result["converged"] and wave_count >= 2
            tc_skip = hasattr(self, 'thinking_chain') and self.thinking_chain.should_skip_remaining()
            
            if ia_converged or tc_skip:
                converged_early = True
                skip_reason = "IA" if ia_converged else "ThinkingChain_confidence"
                self.logger.debug(
                    f"Converged at wave {wave_count} (depth={blocks_executed}, reason={skip_reason})"
                )
                break
            
            # ‚îÄ‚îÄ –°–ø–∏–Ω–Ω–æ–π –º–æ–∑–≥ –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞–º—è—Ç—å –º–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏ ‚îÄ‚îÄ
            spine_updated = False
            if wave_idx < max_waves - 1:
                if hasattr(self, 'to_memory_space'):
                    try:
                        h_for_mem = self.to_memory_space(h_curr)
                        if memory_vec is None:
                            # –ü–µ—Ä–≤–∞—è –≤–æ–ª–Ω–∞ ‚Äî –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º memory_vec
                            memory_vec = h_for_mem.detach()
                        else:
                            # –°–ø–∏–Ω–Ω–æ–π –º–æ–∑–≥: 70% —Å—Ç–∞—Ä–∞—è + 30% –Ω–æ–≤–∞—è (¬ß2.4)
                            memory_vec = 0.7 * memory_vec + 0.3 * h_for_mem.detach()
                        spine_updated = True
                    except Exception:
                        pass
                
                # Titans surprise feedback –º–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏
                if hasattr(self, 'titans_memory') and self.titans_memory is not None:
                    wave_surprises = [s for s in surprise_signals 
                                     if s["layer"] >= blocks_executed - 2]
                    if wave_surprises:
                        try:
                            h_for_titans = self.to_memory_space(h_curr)
                            self.titans_memory.forward(h_for_titans)
                        except Exception:
                            pass
                
                # ‚ïê‚ïê‚ïê Native CoT: ThinkingChain —É—Ç–æ—á–Ω—è–µ—Ç memory_vec ‚ïê‚ïê‚ïê
                # –ö–∞–∂–¥–∞—è –≤–æ–ª–Ω–∞ ‚Äî —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. ThinkingChain –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç
                # memory_vec —Ç–∞–∫, —á—Ç–æ —Å–ª–µ–¥—É—é—â–∞—è –≤–æ–ª–Ω–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è
                # –Ω–∞ –¥—Ä—É–≥–æ–º –∞—Å–ø–µ–∫—Ç–µ –∑–∞–¥–∞—á–∏ (–æ—Ç –æ–±—â–µ–≥–æ –∫ —á–∞—Å—Ç–Ω–æ–º—É).
                if memory_vec is not None and hasattr(self, 'thinking_chain'):
                    try:
                        memory_vec, thought_info = self.thinking_chain.step(
                            h_curr, memory_vec, wave_idx, max_waves
                        )
                        self.thinking_logger.log_step(blocks_executed, {
                            "thinking_phase": thought_info.get("phase", "unknown"),
                            "thinking_confidence": thought_info.get("confidence", 0),
                            "thought_gate": thought_info.get("gate_mean", 0),
                        })
                    except Exception:
                        pass
            
            wave_experts["spine_updated"] = spine_updated
            
            # ‚ïê‚ïê‚ïê Supplement Injection: –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—á–µ—Ä–µ–¥—å –≥–æ–ª–æ—Å–æ–≤—ã—Ö –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–π ‚ïê‚ïê‚ïê
            if supplement_queue is not None and not supplement_queue.empty():
                try:
                    supplement = supplement_queue.get_nowait()
                    sup_text = supplement.get("text", "")
                    sup_tokens = supplement.get("tokens")  # pre-tokenized
                    
                    if sup_text or sup_tokens is not None:
                        self.logger.info(
                            f"üé§ Supplement injection at wave {wave_count}: "
                            f"'{sup_text[:50]}...'"
                        )
                        
                        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
                        if sup_tokens is None:
                            sup_ids = input_ids  # fallback
                        else:
                            sup_ids = sup_tokens.to(x.device)
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ 2 –±–ª–æ–∫–∞ —Ç–µ–∫—É—â–µ–π –≤–æ–ª–Ω—ã
                        x_sup = self.embedding(sup_ids)
                        
                        # 2 –±–ª–æ–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (—Ç–µ –∂–µ –≤–µ—Å–∞ —á—Ç–æ —Ç–µ–∫—É—â–∞—è –≤–æ–ª–Ω–∞)
                        xs_l, _, _, _, _, _ = self.blocks[b_left](
                            x_sup, None, None, memory_vec, rag_state, None, None
                        )
                        xs_r, _, _, _, _, _ = self.blocks[b_right](
                            x_sup, None, None, memory_vec, rag_state, None, None
                        )
                        
                        # WaveConsolidation: —Å—É–º–º–∏—Ä—É—é—â–∞—è –º–∞—Ç—Ä–∏—Ü–∞
                        x_sup_merged, _ = self.wave_consolidations[wave_idx](
                            xs_l, xs_r, {}, {}
                        )
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: —Å–ø–∏–Ω–Ω–æ–π –º–æ–∑–≥ –∏–Ω–∂–µ–∫—Ç–∏—Ä—É–µ—Ç
                        h_sup = x_sup_merged.mean(dim=1).detach()
                        if hasattr(self, 'to_memory_space'):
                            h_sup_mem = self.to_memory_space(h_sup)
                            if memory_vec is not None:
                                # 50/50 —Å–º–µ—Å—å —Å—Ç–∞—Ä–æ–≥–æ + –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
                                memory_vec = 0.5 * memory_vec + 0.5 * h_sup_mem.detach()
                            else:
                                memory_vec = h_sup_mem.detach()
                        
                        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å ‚Äî –Ω–∞–¥–æ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏—Ç—å
                        self.integral_auditor.reset()
                        converged_early = False
                        h_prev = h_sup
                        
                        supplement_injected = True
                        supplement_extra_waves = 3  # +6 –±–ª–æ–∫–æ–≤
                        
                        self.logger.info(
                            f"‚úÖ Supplement merged. +3 extra waves scheduled."
                        )
                except Exception as e:
                    self.logger.warning(f"Supplement injection error: {e}")
        
        # ‚ïê‚ïê‚ïê Extra waves –ø–æ—Å–ª–µ supplement (–ø–æ–ª–æ–≤–∏–Ω–∞ –æ—Ç 12 = 6 –±–ª–æ–∫–æ–≤ = 3 –≤–æ–ª–Ω—ã) ‚ïê‚ïê‚ïê
        if supplement_injected and supplement_extra_waves > 0:
            self.logger.info(
                f"üîÑ Running {supplement_extra_waves} extra waves for supplement"
            )
            for extra_idx in range(supplement_extra_waves):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 3 –≤–æ–ª–Ω—ã (–±–ª–æ–∫–∏ 0-5) –ø–æ–≤—Ç–æ—Ä–Ω–æ
                b_l = extra_idx * 2
                b_r = extra_idx * 2 + 1
                if b_r >= self.n_layers:
                    break
                
                wave_count += 1
                x_left, wkv_states[b_l], x_prevs[b_l], stats_l, \
                    ssd_states[b_l], conv_states[b_l] = self.blocks[b_l](
                    x, wkv_states[b_l], x_prevs[b_l], memory_vec, rag_state,
                    ssd_states[b_l], conv_states[b_l]
                )
                x_right, wkv_states[b_r], x_prevs[b_r], stats_r, \
                    ssd_states[b_r], conv_states[b_r] = self.blocks[b_r](
                    x, wkv_states[b_r], x_prevs[b_r], memory_vec, rag_state,
                    ssd_states[b_r], conv_states[b_r]
                )
                blocks_executed += 2
                
                x, merge_alpha = self.wave_consolidations[extra_idx](
                    x_left, x_right, stats_l, stats_r
                )
                
                # Spine update
                h_curr = x.mean(dim=1).detach()
                if hasattr(self, 'to_memory_space'):
                    try:
                        h_for_mem = self.to_memory_space(h_curr)
                        memory_vec = 0.7 * memory_vec + 0.3 * h_for_mem.detach()
                    except Exception:
                        pass
                
                ia_result = self.integral_auditor.observe(h_curr, h_prev)
                h_prev = h_curr
                
                self.logger.debug(
                    f"Extra wave {extra_idx+1}/{supplement_extra_waves}: "
                    f"p={ia_result['p']:.3f}"
                )
                
                if ia_result["converged"]:
                    converged_early = True
                    break
        
        # ‚ïê‚ïê‚ïê Titans Feedback: —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª ‚ïê‚ïê‚ïê
        if hasattr(self, 'titans_memory') and self.titans_memory is not None:
            if surprise_signals:
                avg_surprise = sum(s["surprise"] for s in surprise_signals) / len(surprise_signals)
                try:
                    h_for_titans = self.to_memory_space(x.mean(dim=1).detach())
                    self.titans_memory.forward(h_for_titans)
                    self.logger.debug(
                        f"Titans final: {len(surprise_signals)} layers surprised, "
                        f"avg={avg_surprise:.3f}"
                    )
                except Exception as e:
                    self.logger.debug(f"Titans feedback error: {e}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ –≤—Å–µ–º –≤–æ–ª–Ω–∞–º
        all_expert_names = []
        for we in per_wave_experts:
            all_expert_names.extend(we.get("left", []))
            all_expert_names.extend(we.get("right", []))
        
        # ‚ïê‚ïê‚ïê 2. Speculative Matrix Routing (IDME) ‚ïê‚ïê‚ïê
        h_curr = x.mean(dim=1).detach()
        ia_result = self.integral_auditor.observe(h_curr, h_prev)
        
        expansion_round = 0
        total_matrices_recruited = 0
        branches_tested = 0
        branches_won = []
        
        prev_p = ia_result.get("p", 0.0)
        no_improve_count = 0
        N_CANDIDATES = 3
        
        while not ia_result["converged"] and expansion_round < max_expansion_rounds:
            expansion_round += 1
            h_prev = h_curr
            
            candidates, indices = self.matrix_pool.select(h_curr.mean(0), k=N_CANDIDATES)
            total_matrices_recruited += len(candidates)
            branches_tested += len(candidates)
            
            # ‚ïê‚ïê‚ïê Lazy Expansion: –ø—É–ª –∏—Å—á–µ—Ä–ø–∞–Ω ‚Üí —Ä–µ–∫—Ä—É—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –º–∞—Ç—Ä–∏—Ü—ã ‚ïê‚ïê‚ïê
            if not candidates:
                available = self.matrix_pool.total_available
                used = len(getattr(self.matrix_pool, 'used_mask', []))
                if available <= used + N_CANDIDATES:
                    try:
                        self.matrix_pool._lazy_expand(4, h_curr.mean(0))
                        self.logger.debug(
                            f"IDME lazy expand: +4 matrices (total={self.matrix_pool.total_available})"
                        )
                        candidates, indices = self.matrix_pool.select(h_curr.mean(0), k=N_CANDIDATES)
                        total_matrices_recruited += len(candidates)
                        branches_tested += len(candidates)
                    except Exception as e:
                        self.logger.debug(f"Lazy expand failed: {e}")
                if not candidates:
                    break
            
            # Branch & Bound
            best_p = prev_p
            best_x = None
            best_idx = -1
            
            h_state = x.mean(dim=1)
            
            for matrix, idx in zip(candidates, indices):
                h_clone = h_state.clone()
                h_refined = matrix(h_clone)
                h_gated, _ = self.novelty_gate(h_clone, h_refined)
                
                with torch.no_grad():
                    f_t = (h_gated - h_state).float().norm().item()
                    candidate_p = prev_p + (1.0 / max(f_t, 1e-8)) * 0.01
                
                if candidate_p > best_p:
                    best_p = candidate_p
                    best_x = h_gated
                    best_idx = idx
            
            if best_x is not None and best_p > prev_p:
                x = x + 0.1 * (best_x - h_state).unsqueeze(1)
                h_curr = x.mean(dim=1).detach()
                ia_result = self.integral_auditor.observe(h_curr, h_prev)
                
                if best_idx >= 0:
                    self.matrix_pool.recirculate(best_idx, max(0, ia_result["p"] - prev_p))
                    branches_won.append((best_idx, ia_result["p"] - prev_p))
                
                hankel_result = self.hankel.observe(h_curr)
                if hankel_result["collapsed"] and hankel_result["collapse_count"] >= 3:
                    break
            else:
                no_improve_count += 1
                h_curr = x.mean(dim=1).detach()
                ia_result = self.integral_auditor.observe(h_curr, h_prev)
            
            if ia_result["p"] <= prev_p + 0.01:
                no_improve_count += 1
                if no_improve_count >= 2:
                    break
            else:
                no_improve_count = 0
            prev_p = ia_result["p"]
        
        # ‚ïê‚ïê‚ïê 3. Output ‚ïê‚ïê‚ïê
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –∏–Ω—ä–µ–∫—Ü–∏—è –ø–∞–º—è—Ç–∏ —Å–ø–∏–Ω–Ω–æ–≥–æ –º–æ–∑–≥–∞ –≤ –≤—ã—Ö–æ–¥
        # (–±–µ–∑ —ç—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –º–µ–∂–¥—É –≤–æ–ª–Ω–∞–º–∏, —Ç–µ—Ä—è–µ—Ç—Å—è)
        if memory_vec is not None and hasattr(self, 'from_memory_space'):
            try:
                mem_signal = self.from_memory_space(memory_vec)  # [B, d_model]
                x = x + 0.1 * mem_signal.unsqueeze(1)  # [B, L, d_model]
            except Exception:
                pass
        
        x = self.norm_f(x)
        logits = self.lm_head(x)
        
        # ‚ïê‚ïê‚ïê Confidence-Gated Output ‚ïê‚ïê‚ïê
        # –ï—Å–ª–∏ ThinkingChain –Ω–µ —É–≤–µ—Ä–µ–Ω ‚Üí —Å–≥–ª–∞–∂–∏–≤–∞–µ–º logits
        if hasattr(self, 'thinking_chain'):
            try:
                logits = self.thinking_chain.apply_confidence_gate(logits)
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–µ—Å—Å–∏–æ–Ω–Ω—É—é –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞
                if memory_vec is not None:
                    self.thinking_chain.update_session_memory(memory_vec)
            except Exception:
                pass
        
        total_time = (time.time() - start_time) * 1000
        
        # ‚ïê‚ïê‚ïê ThinkingChain Finalize (v3) ‚ïê‚ïê‚ïê
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –≤ ThoughtCache + Sleep recording
        tc_summary = {}
        if hasattr(self, 'thinking_chain'):
            try:
                avg_surprise = 0.0
                if surprise_signals:
                    avg_surprise = sum(s["surprise"] for s in surprise_signals) / len(surprise_signals)
                self.thinking_chain.finalize(
                    query_embedding=x.mean(dim=1).detach(),
                    final_memory=memory_vec,
                    surprise_level=avg_surprise,
                    task_type=task_type,
                )
            except Exception:
                pass
            tc_summary = self.thinking_chain.get_summary()
        
        stats = {
            "task_type": task_type,
            "p_threshold": p_threshold,
            "final_p": ia_result["p"],
            "r_squared": ia_result.get("r_squared", 0),
            "converged": ia_result["converged"],
            "converged_early": converged_early,
            "total_blocks": self.n_layers,
            "blocks_executed": blocks_executed,
            "waves": wave_count,
            "estimated_depth": estimated_depth,
            "expansion_rounds": expansion_round,
            "matrices_recruited": total_matrices_recruited,
            "total_matrices": self.n_layers + total_matrices_recruited,
            "branches_tested": branches_tested,
            "branches_won": len(branches_won),
            "per_wave_experts": per_wave_experts,
            "hankel_collapses": self.hankel.collapse_count,
            "surprise_layers": len(surprise_signals),
            "supplement_injected": supplement_injected,
            "supplement_extra_waves": supplement_extra_waves if supplement_injected else 0,
            "rwkv_state_size_mb": sum(s.numel() * 4 / 1024 / 1024 for s in wkv_states if s is not None),
            "total_ms": total_time,
            # ThinkingChain v2 stats
            "thinking_chain": tc_summary,
            "tc_confidence": tc_summary.get("final_confidence", 0),
            "tc_phases": tc_summary.get("phases", []),
            "tc_retrieval_count": tc_summary.get("retrieval_count", 0),
        }
        
        self.thinking_logger.end_session(total_time, "")
        
        win_ratio = f"{len(branches_won)}/{branches_tested}" if branches_tested > 0 else "0/0"
        self.logger.info(
            f"Think: {task_type} | p={stats['final_p']:.3f} | "
            f"conf={stats['tc_confidence']:.2f} | "
            f"phases={'‚Üí'.join(tc_summary.get('phases', []))}"
        )
        
        return logits, stats
    
    @torch.no_grad()
    def self_verify(self, input_ids, generated_ids, memory_vec=None):
        """
        Self-Verification: –ø—Ä–æ–≥–æ–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ —á–µ—Ä–µ–∑ 2 –≤–æ–ª–Ω—ã.
        
        –ï—Å–ª–∏ consistency < 0.8 ‚Üí –æ—Ç–≤–µ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π.
        DeepSeek-R1 –¥–µ–ª–∞–µ—Ç —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–º (<verify>).
        –¢–ê–†–° ‚Äî –≤ hidden states (–Ω—É–ª–µ–≤–æ–π overhead).
        
        Args:
            input_ids: [1, L_query] ‚Äî –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            generated_ids: [1, L_answer] ‚Äî —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            memory_vec: [1, 384] ‚Äî –ø–∞–º—è—Ç—å –æ—Ç think()
        
        Returns:
            consistency: float (0-1) ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω
            should_regenerate: bool ‚Äî –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å
        """
        self.eval()
        
        # –ü—Ä–æ–≥–æ–Ω—è–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ 2 –≤–æ–ª–Ω—ã (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        combined = torch.cat([input_ids, generated_ids], dim=1)
        x = self.embedding(combined)
        
        # 2 –≤–æ–ª–Ω—ã (4 –±–ª–æ–∫–∞)
        for wave_idx in range(min(2, self.n_layers // 2)):
            b_left = wave_idx * 2
            b_right = wave_idx * 2 + 1
            if b_right >= self.n_layers:
                break
            
            x_left, _, _, _, _, _ = self.blocks[b_left](
                x, None, None, memory_vec, None, None, None
            )
            x_right, _, _, _, _, _ = self.blocks[b_right](
                x, None, None, memory_vec, None, None, None
            )
            x, _ = self.wave_consolidations[wave_idx](x_left, x_right)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º logits –¥–ª—è –æ—Ç–≤–µ—Ç–Ω–æ–π —á–∞—Å—Ç–∏
        verify_logits = self.lm_head(self.norm_f(x))
        
        # Consistency = cosine similarity –º–µ–∂–¥—É hidden states
        # –ë–µ—Ä—ë–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –æ—Ç–≤–µ—Ç–Ω–æ–π —á–∞—Å—Ç–∏
        L_q = input_ids.shape[1]
        answer_hidden = x[:, L_q:, :].mean(dim=1)  # [1, d_model]
        query_hidden = x[:, :L_q, :].mean(dim=1)   # [1, d_model]
        
        consistency = F.cosine_similarity(
            answer_hidden, query_hidden, dim=-1
        ).item()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ [0, 1]
        consistency = (consistency + 1.0) / 2.0  # cosine [-1,1] ‚Üí [0,1]
        
        should_regenerate = consistency < 0.8
        
        self.logger.info(
            f"Self-verify: consistency={consistency:.3f}, "
            f"regenerate={should_regenerate}"
        )
        
        return consistency, should_regenerate
    
    def encode_rag(self, rag_tokens: torch.Tensor) -> torch.Tensor:
        """
        –ü—Ä–æ–≥–æ–Ω—è–µ—Ç RAG-–¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ TarsCoreBlock –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤,
        —Ñ–æ—Ä–º–∏—Ä—É—è —Å–∂–∞—Ç—É—é –ú–∞—Ç—Ä–∏—Ü—É –ó–Ω–∞–Ω–∏–π (WKV State).
        
        rag_tokens: [1, L_doc] ‚Äî —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        Returns: [1, d_state, d_state] ‚Äî —Å–∂–∞—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        """
        with torch.no_grad():
            x = self.embedding(rag_tokens)
            wkv_state = None
            x_prev = None
            
            for block in self.blocks:
                x, wkv_state, x_prev, _, _, _ = block(
                    x, wkv_state, x_prev
                )
        
        self.logger.info(
            f"RAG encoded: {rag_tokens.shape[1]} tokens ‚Üí "
            f"state {wkv_state.shape} ({wkv_state.numel() * 4 / 1024 / 1024:.1f} MB)"
        )
        return wkv_state
    
    def count_parameters(self) -> dict:
        """–ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º."""
        def count(module):
            return sum(p.numel() for p in module.parameters())
        
        # Per-block breakdown (–ø–µ—Ä–≤—ã–π –±–ª–æ–∫ –∫–∞–∫ –ø—Ä–∏–º–µ—Ä)
        block = self.blocks[0]
        block_detail = {
            "tars_core (ssd+wkv+fusion)": count(block.core),
            "omega_ssm": count(block.omega),
            "mole": count(block.mole),
        }
        
        return {
            "embedding": count(self.embedding),
            "blocks_total": sum(count(b) for b in self.blocks),
            "block_detail (√ó1)": block_detail,
            "matrix_pool": count(self.matrix_pool),
            "novelty_gate": count(self.novelty_gate),
            "lm_head": 0,  # tied
            "total": count(self),
        }
