"""
tokenizer.py ‚Äî –ï–¥–∏–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¢–ê–†–° (cp1251 byte-level).

–û–¥–∏–Ω –∫–ª–∞—Å—Å –¥–ª—è –í–°–ï–• –º–æ–¥–µ–ª–µ–π: Reflex, MinGRU, Mamba-2.
1 —Å–∏–º–≤–æ–ª –∫–∏—Ä–∏–ª–ª–∏—Ü—ã = 1 –±–∞–π—Ç cp1251 = 1 —Ç–æ–∫–µ–Ω (0-255).

    "–ø—Ä–∏–≤–µ—Ç" ‚Üí encode ‚Üí [239, 240, 232, 226, 229, 242]
    [239, 240, 232, 226, 229, 242] ‚Üí decode ‚Üí "–ø—Ä–∏–≤–µ—Ç"

Vocab = 256 (–ø–æ–ª–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –±–∞–π—Ç–æ–≤).
–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: PAD=0, EOS=3 (ETX –≤ ASCII).
"""


class TarsTokenizer:
    """
    CP1251 Byte-Level Tokenizer.
    
    –ö–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –∫–∏—Ä–∏–ª–ª–∏—Ü—ã = 1 –±–∞–π—Ç = 1 —Ç–æ–∫–µ–Ω.
    –ù–∏–∫–∞–∫–∏—Ö BPE/SentencePiece ‚Äî –ø—Ä—è–º–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
    
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
      - Vocab = 256 (—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –Ω–µ –Ω—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å)
      - –ö–∞–∂–¥—ã–π –±–∞–π—Ç –≤–≤–æ–¥–∞ = 1 —Ç–æ–∫–µ–Ω (–Ω–µ—Ç UNK)
      - –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π (–Ω–µ—Ç –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏)
      - –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º —Ç–µ–∫—Å—Ç–æ–º (–¥–∞–∂–µ –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    """
    
    def __init__(self):
        self.vocab_size = 256
        self.pad_token_id = 0
        self.eos_token_id = 3  # ETX (End of Text) –≤ ASCII
        self.bos_token_id = 2  # STX (Start of Text) –≤ ASCII
    
    def encode(self, text: str) -> list:
        """
        –¢–µ–∫—Å—Ç ‚Üí —Å–ø–∏—Å–æ–∫ –±–∞–π—Ç–æ–≤ cp1251.
        
        Args:
            text: —Å—Ç—Ä–æ–∫–∞ (—Ä—É—Å—Å–∫–∏–π/–∞–Ω–≥–ª–∏–π—Å–∫–∏–π/–ª—é–±–æ–π)
        Returns:
            list[int] ‚Äî –±–∞–π—Ç–æ–≤—ã–µ ID (0-255)
            
        –ü—Ä–∏–º–µ—Ä:
            encode("–ø—Ä–∏–≤–µ—Ç") ‚Üí [239, 240, 232, 226, 229, 242]
            encode("hello")  ‚Üí [104, 101, 108, 108, 111]
        """
        return list(text.encode('cp1251', errors='replace'))
    
    def decode(self, ids: list) -> str:
        """
        –°–ø–∏—Å–æ–∫ –±–∞–π—Ç–æ–≤ ‚Üí —Ç–µ–∫—Å—Ç cp1251.
        
        Args:
            ids: list[int] ‚Äî –±–∞–π—Ç–æ–≤—ã–µ ID (0-255)
        Returns:
            str ‚Äî –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
            
        –ü—Ä–∏–º–µ—Ä:
            decode([239, 240, 232, 226, 229, 242]) ‚Üí "–ø—Ä–∏–≤–µ—Ç"
        """
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ
        clean = [b for b in ids if 0 <= b < 256 and b not in (self.pad_token_id, self.eos_token_id, self.bos_token_id)]
        return bytearray(clean).decode('cp1251', errors='replace')
    
    def encode_with_special(self, text: str) -> list:
        """Encode —Å BOS –∏ EOS —Ç–æ–∫–µ–Ω–∞–º–∏."""
        return [self.bos_token_id] + self.encode(text) + [self.eos_token_id]
    
    def __repr__(self):
        return f"TarsTokenizer(vocab={self.vocab_size}, encoding=cp1251)"


if __name__ == "__main__":
    t = TarsTokenizer()
    
    # –¢–µ—Å—Ç cp1251
    tests = [
        "–ø—Ä–∏–≤–µ—Ç",
        "–∫–∞–∫ –¥–µ–ª–∞?",
        "—á—Ç–æ —Ç–∞–∫–æ–µ –∏–Ω—Ç–µ–≥—Ä–∞–ª",
        "Hello World",
        "–¢–ê–†–° v3.0 ü§ñ",
    ]
    
    print(f"Tokenizer: {t}")
    print(f"Vocab: {t.vocab_size}")
    print()
    
    for text in tests:
        ids = t.encode(text)
        decoded = t.decode(ids)
        print(f"  '{text}' ‚Üí {ids[:10]}{'...' if len(ids) > 10 else ''} ‚Üí '{decoded}'")
        assert decoded.startswith(text[:3]) or len(text) < 3, f"Decode failed for '{text}'"
    
    # Verify specific cp1251 bytes
    assert t.encode("–∫") == [234], f"'–∫' should be [234], got {t.encode('–∫')}"
    assert t.encode("–∞") == [224], f"'–∞' should be [224], got {t.encode('–∞')}"
    assert t.encode(" ") == [32], f"' ' should be [32], got {t.encode(' ')}"
    
    print("\n‚úÖ All tests passed!")
