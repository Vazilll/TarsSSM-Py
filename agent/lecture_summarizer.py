"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  lecture_summarizer.py ‚Äî –ö–æ–Ω—Å–ø–µ–∫—Ç –∏–∑ –ª–µ–∫—Ü–∏–π/PDF TARS v3
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

"–°–¥–µ–ª–∞–π –∫–æ–Ω—Å–ø–µ–∫—Ç –∏–∑ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞"
"–ü–µ—Ä–µ—Å–∫–∞–∂–∏ –ª–µ–∫—Ü–∏—é –≤ 10 –ø—É–Ω–∫—Ç–æ–≤"
"–í—ã–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º—É–ª—ã"

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
  - –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã (.txt, .md)
  - PDF (—á–µ—Ä–µ–∑ PyPDF2 –∏–ª–∏ pdfplumber)
  - Word (.docx —á–µ—Ä–µ–∑ python-docx)
  - –ê—É–¥–∏–æ–∑–∞–ø–∏—Å–∏ –ª–µ–∫—Ü–∏–π (.wav, .mp3 ‚Üí Whisper)
"""

import os
import re
import logging
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from collections import defaultdict

logger = logging.getLogger("Tars.LectureSummarizer")

_ROOT = Path(__file__).parent.parent
_NOTES_DIR = _ROOT / "data" / "notes"
_NOTES_DIR.mkdir(parents=True, exist_ok=True)


class LectureSummarizer:
    """
    –°–æ–∑–¥–∞—ë—Ç —É–º–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤ –∏ –∞—É–¥–∏–æ.
    
    –ú–µ—Ç–æ–¥—ã:
      - Extractive summarization (–±–µ–∑ GPT, –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF + –ø–æ–∑–∏—Ü–∏–∏)
      - –í—ã–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, —Ñ–æ—Ä–º—É–ª, –¥–∞—Ç
      - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è flashcards –¥–ª—è LearningHelper
    """
    
    def __init__(self, whisper_model=None):
        self.whisper = whisper_model
    
    def summarize_file(self, file_path: str, max_points: int = 10) -> str:
        """
        –ö–æ–Ω—Å–ø–µ–∫—Ç –∏–∑ —Ñ–∞–π–ª–∞.
        
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –≤—ã–∑—ã–≤–∞–µ—Ç –Ω—É–∂–Ω—ã–π –ø–∞—Ä—Å–µ—Ä.
        """
        path = Path(file_path)
        if not path.exists():
            return f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"
        
        ext = path.suffix.lower()
        
        if ext in ('.txt', '.md', '.py', '.log'):
            text = self._read_text(file_path)
        elif ext == '.pdf':
            text = self._read_pdf(file_path)
        elif ext in ('.docx', '.doc'):
            text = self._read_docx(file_path)
        elif ext in ('.wav', '.mp3', '.ogg', '.m4a'):
            text = self._transcribe_audio(file_path)
        else:
            return f"‚ùå –§–æ—Ä–º–∞—Ç {ext} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. –ü–æ–¥–¥–µ—Ä–∂–∫–∞: txt, pdf, docx, wav, mp3"
        
        if not text or len(text) < 50:
            return "‚ùå –§–∞–π–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∫–æ–Ω—Å–ø–µ–∫—Ç–∞."
        
        return self._create_summary(text, path.name, max_points)
    
    def summarize_text(self, text: str, title: str = "–¢–µ–∫—Å—Ç", max_points: int = 10) -> str:
        """–ö–æ–Ω—Å–ø–µ–∫—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        return self._create_summary(text, title, max_points)
    
    def extract_definitions(self, text: str) -> List[str]:
        """–í—ã–¥–µ–ª–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        definitions = []
        
        patterns = [
            r'(.+?)\s*[‚Äî‚Äì-]\s*—ç—Ç–æ\s+(.+?)(?:\.|$)',        # X ‚Äî —ç—Ç–æ Y
            r'(.+?)\s*–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è\s+(.+?)(?:\.|$)',            # X –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è Y
            r'[–û–æ]–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ[:.]\s*(.+?)(?:\.|$)',            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: ...
            r'(.+?)\s*is\s+(?:a|an|the)\s+(.+?)(?:\.|$)',    # X is a Y
            r'[Dd]efinition[:.]\s*(.+?)(?:\.|$)',             # Definition: ...
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.MULTILINE | re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    definition = " ‚Äî ".join(str(m).strip() for m in match)
                else:
                    definition = str(match).strip()
                if len(definition) > 10 and definition not in definitions:
                    definitions.append(definition[:200])
        
        return definitions[:20]
    
    def extract_formulas(self, text: str) -> List[str]:
        """–í—ã–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º—É–ª—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        formulas = []
        
        patterns = [
            r'\$(.+?)\$',                      # LaTeX inline
            r'\\\[(.+?)\\\]',                  # LaTeX display
            r'[A-Za-z]+\s*=\s*[^,\n]{3,50}',  # X = expression
            r'‚àë|‚à´|‚àÇ|‚àö|¬±|‚â§|‚â•|‚â†|‚àà|‚àâ|‚äÇ|‚à™|‚à©',    # Unicode math
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for m in matches:
                formula = str(m).strip()
                if len(formula) > 2 and formula not in formulas:
                    formulas.append(formula[:100])
        
        return formulas[:15]
    
    def extract_dates_events(self, text: str) -> List[str]:
        """–í—ã–¥–µ–ª–∏—Ç—å –¥–∞—Ç—ã –∏ —Å–æ–±—ã—Ç–∏—è."""
        events = []
        
        # –†–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
        date_patterns = [
            r'(\d{1,2}[./]\d{1,2}[./]\d{2,4})\s*[‚Äî‚Äì-]?\s*(.{0,100})',
            r'(\d{4})\s*(?:–≥–æ–¥|–≥\.?)\s*[‚Äî‚Äì-]?\s*(.{0,100})',
            r'–≤\s+(\d{4})\s+(.{0,80})',
        ]
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            for date_str, event in matches:
                entry = f"{date_str.strip()} ‚Äî {event.strip()}"
                if len(entry) > 10 and entry not in events:
                    events.append(entry[:150])
        
        return events[:10]
    
    def _create_summary(self, text: str, title: str, max_points: int) -> str:
        """–°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Å–ø–µ–∫—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_sentences(text)
        
        if not sentences:
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."
        
        # –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        scores = self._score_sentences(sentences, text)
        
        # –¢–æ–ø-N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        ranked = sorted(range(len(sentences)), key=lambda i: scores[i], reverse=True)
        top_indices = sorted(ranked[:max_points])
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, —Ñ–æ—Ä–º—É–ª—ã, –¥–∞—Ç—ã
        definitions = self.extract_definitions(text)
        formulas = self.extract_formulas(text)
        dates = self.extract_dates_events(text)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Å–ø–µ–∫—Ç
        lines = [
            f"üìã –ö–æ–Ω—Å–ø–µ–∫—Ç: {title}",
            f"üìä –ò—Å—Ö–æ–¥–Ω–∏–∫: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí {max_points} —Ç–µ–∑–∏—Å–æ–≤\n",
        ]
        
        lines.append("üéØ –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–∑–∏—Å—ã:")
        for i, idx in enumerate(top_indices, 1):
            sent = sentences[idx].strip()
            if len(sent) > 200:
                sent = sent[:197] + "..."
            lines.append(f"  {i}. {sent}")
        
        if definitions:
            lines.append(f"\nüìñ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è ({len(definitions)}):")
            for d in definitions[:5]:
                lines.append(f"  ‚Ä¢ {d}")
        
        if formulas:
            lines.append(f"\nüî¢ –§–æ—Ä–º—É–ª—ã ({len(formulas)}):")
            for f in formulas[:5]:
                lines.append(f"  ‚Ä¢ {f}")
        
        if dates:
            lines.append(f"\nüìÖ –î–∞—Ç—ã –∏ —Å–æ–±—ã—Ç–∏—è ({len(dates)}):")
            for e in dates[:5]:
                lines.append(f"  ‚Ä¢ {e}")
        
        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (—Ç–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
        keywords = self._extract_keywords(text, n=8)
        if keywords:
            lines.append(f"\nüè∑ –¢–µ–º—ã: {', '.join(keywords)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Å–ø–µ–∫—Ç
        note_path = _NOTES_DIR / f"note_{datetime.now().strftime('%Y%m%d_%H%M')}_{title[:20]}.md"
        try:
            with open(note_path, "w", encoding="utf-8") as f:
                f.write("\n".join(lines))
            lines.append(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {note_path.name}")
        except Exception:
            pass
        
        return "\n".join(lines)
    
    def _split_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ç–æ—á–∫–∞–º, ? –∏ !
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
        return [s for s in sentences if len(s.strip()) > 20]
    
    def _score_sentences(self, sentences: List[str], full_text: str) -> List[float]:
        """
        –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        
        –§–∞–∫—Ç–æ—Ä—ã:
          1. TF-IDF –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
          2. –ü–æ–∑–∏—Ü–∏—è (–ø–µ—Ä–≤–æ–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ ‚Äî –≤–∞–∂–Ω–µ–µ)
          3. –î–ª–∏–Ω–∞ (—Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ª—É—á—à–µ)
          4. –ù–∞–ª–∏—á–∏–µ —Å–∏–≥–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        """
        # TF-IDF
        word_freq = self._word_frequencies(full_text)
        n = len(sentences)
        
        scores = []
        for i, sent in enumerate(sentences):
            score = 0.0
            
            # 1. –ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤
            words = sent.lower().split()
            if words:
                word_score = sum(word_freq.get(w, 0) for w in words) / len(words)
                score += word_score * 2.0
            
            # 2. –ü–æ–∑–∏—Ü–∏—è (–ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–∞–∂–Ω–µ–µ)
            position = i / max(1, n - 1)
            if position < 0.1 or position > 0.9:
                score += 1.5
            elif position < 0.3:
                score += 0.8
            
            # 3. –î–ª–∏–Ω–∞ (30-100 —Å–ª–æ–≤ = –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
            wlen = len(words) if words else 0
            if 5 <= wlen <= 30:
                score += 0.5
            
            # 4. –°–∏–≥–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            signal_words = [
                '–≤–∞–∂–Ω–æ', '–≥–ª–∞–≤–Ω–æ–µ', '–∏—Ç–æ–≥', '–≤—ã–≤–æ–¥', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç',
                '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—Ç–µ–æ—Ä–µ–º–∞', '—Ñ–æ—Ä–º—É–ª–∞', '–ø—Ä–∞–≤–∏–ª–æ', '–∑–∞–∫–æ–Ω',
                'important', 'conclusion', 'result', 'therefore', 'thus',
                'key', 'main', 'primary', 'essential',
            ]
            for sw in signal_words:
                if sw in sent.lower():
                    score += 1.0
                    break
            
            scores.append(score)
        
        return scores
    
    def _word_frequencies(self, text: str) -> Dict[str, float]:
        """–ß–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ)."""
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–∞', '–Ω–æ', '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫',
            '—è', '–º—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–Ω–µ', '–¥–∞', '–Ω–µ—Ç', '–¥–ª—è',
            '–∏–∑', '–æ—Ç', '–¥–æ', '–∑–∞', '–ø—Ä–∏', '–∏–ª–∏', '—Ç–æ', '—É', '–∫', '–∂–µ',
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
            'to', 'of', 'and', 'in', 'that', 'it', 'for', 'with', 'on',
        }
        
        words = re.findall(r'[–∞-—è—ëa-z]{3,}', text.lower())
        freq = defaultdict(int)
        for w in words:
            if w not in stop_words:
                freq[w] += 1
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        max_freq = max(freq.values()) if freq else 1
        return {w: c / max_freq for w, c in freq.items()}
    
    def _extract_keywords(self, text: str, n: int = 8) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞."""
        freq = self._word_frequencies(text)
        sorted_words = sorted(freq.items(), key=lambda x: -x[1])
        return [w for w, _ in sorted_words[:n]]
    
    def _read_text(self, path: str) -> str:
        """–ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞."""
        encodings = ['utf-8', 'cp1251', 'latin-1']
        for enc in encodings:
            try:
                with open(path, "r", encoding=enc) as f:
                    return f.read()
            except (UnicodeDecodeError, UnicodeError):
                continue
        return ""
    
    def _read_pdf(self, path: str) -> str:
        """–ß—Ç–µ–Ω–∏–µ PDF."""
        try:
            import PyPDF2
            text = ""
            with open(path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            return text
        except ImportError:
            try:
                import pdfplumber
                text = ""
                with pdfplumber.open(path) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                return text
            except ImportError:
                return "‚ùå –î–ª—è PDF —É—Å—Ç–∞–Ω–æ–≤–∏: pip install PyPDF2 –∏–ª–∏ pip install pdfplumber"
    
    def _read_docx(self, path: str) -> str:
        """–ß—Ç–µ–Ω–∏–µ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        try:
            import docx
            doc = docx.Document(path)
            return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
        except ImportError:
            return "‚ùå –î–ª—è DOCX —É—Å—Ç–∞–Ω–æ–≤–∏: pip install python-docx"
    
    def _transcribe_audio(self, path: str) -> str:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ Whisper."""
        if self.whisper:
            try:
                result = self.whisper.transcribe(path)
                return result.get("text", "")
            except Exception as e:
                return f"‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: {e}"
        
        # Fallback ‚Äî –ø—Ä–æ–±—É–µ–º whisper CLI
        try:
            import subprocess
            result = subprocess.run(
                ["whisper", path, "--language", "ru", "--output_format", "txt"],
                capture_output=True, text=True, timeout=300
            )
            txt_path = Path(path).with_suffix('.txt')
            if txt_path.exists():
                return txt_path.read_text(encoding='utf-8')
        except Exception:
            pass
        
        return "‚ùå Whisper –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏: pip install openai-whisper"
    
    def generate_flashcards(self, text: str, topic: str = "general") -> List[Dict]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç flashcards –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è LearningHelper.
        
        Returns: [{"question": ..., "answer": ..., "topic": ...}, ...]
        """
        cards = []
        
        # –ò–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        definitions = self.extract_definitions(text)
        for defn in definitions:
            parts = defn.split(" ‚Äî ", 1)
            if len(parts) == 2:
                cards.append({
                    "question": f"–ß—Ç–æ —Ç–∞–∫–æ–µ {parts[0].strip()}?",
                    "answer": parts[1].strip(),
                    "topic": topic,
                })
        
        # –ò–∑ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences = self._split_sentences(text)
        scores = self._score_sentences(sentences, text)
        ranked = sorted(range(len(sentences)), key=lambda i: scores[i], reverse=True)
        
        for idx in ranked[:5]:
            sent = sentences[idx].strip()
            if len(sent) > 30:
                # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                cards.append({
                    "question": f"–û–±—ä—è—Å–Ω–∏: {sent[:80]}...",
                    "answer": sent,
                    "topic": topic,
                })
        
        return cards[:10]
