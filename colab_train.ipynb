{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü§ñ TARS v3 ‚Äî Google Colab Training\n",
                "\n",
                "**–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ TARS v3 –Ω–∞ GPU T4 (16 GB VRAM)**\n",
                "\n",
                "### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n",
                "1. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
                "2. –ó–∞–ø—É—Å–∫–∞–π —è—á–µ–π–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É ‚ñ∂\n",
                "3. –í –∫–æ–Ω—Ü–µ —Å–∫–∞—á–∞–π –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(\"=\" * 60)\n",
                "print(\"  TARS v3 ‚Äî Google Colab Training\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    vram = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
                "    print(f\"  ‚úÖ GPU: {gpu_name}\")\n",
                "    print(f\"  ‚úÖ VRAM: {vram:.1f} GB\")\n",
                "    print(f\"  ‚úÖ CUDA: {torch.version.cuda}\")\n",
                "else:\n",
                "    print(\"  ‚ùå GPU –ù–ï –û–ë–ù–ê–†–£–ñ–ï–ù!\")\n",
                "    print(\"  ‚Üí Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\")\n",
                "    raise RuntimeError(\"GPU not found! Enable GPU in Runtime settings.\")\n",
                "\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å GitHub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "REPO_URL = \"https://github.com/Vazilll/TarsSSM-Py.git\"\n",
                "WORK_DIR = \"/content/TarsSSM-Py\"\n",
                "\n",
                "if os.path.exists(os.path.join(WORK_DIR, \"mega_train.py\")):\n",
                "    print(\"‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —É–∂–µ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω\")\n",
                "else:\n",
                "    print(f\"üì• –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {REPO_URL} ...\")\n",
                "    !git clone {REPO_URL} {WORK_DIR}\n",
                "    print(\"‚úÖ –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")\n",
                "\n",
                "os.chdir(WORK_DIR)\n",
                "print(f\"üìÇ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Torch —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ Colab, —Å—Ç–∞–≤–∏–º –æ—Å—Ç–∞–ª—å–Ω–æ–µ\n",
                "!pip install -q einops tqdm psutil sentencepiece tokenizers \\\n",
                "    sentence-transformers datasets transformers peft jiwer\n",
                "print(\"\\n‚úÖ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, WORK_DIR)\n",
                "\n",
                "os.makedirs(\"data\", exist_ok=True)\n",
                "\n",
                "# Wikipedia (10–∫ —Å—Ç–∞—Ç–µ–π)\n",
                "wiki_path = os.path.join(WORK_DIR, \"data\", \"wiki_ru.txt\")\n",
                "if os.path.exists(wiki_path) and os.path.getsize(wiki_path) > 100_000:\n",
                "    size_mb = os.path.getsize(wiki_path) / 1024 / 1024\n",
                "    print(f\"üìö Wikipedia: —É–∂–µ –µ—Å—Ç—å ({size_mb:.1f} MB)\")\n",
                "else:\n",
                "    print(\"üìö –°–∫–∞—á–∏–≤–∞–Ω–∏–µ Wikipedia (10 000 —Å—Ç–∞—Ç–µ–π)...\")\n",
                "    !python training/download_wiki.py --count 10000\n",
                "\n",
                "# HuggingFace datasets\n",
                "import glob\n",
                "hf_files = glob.glob(os.path.join(WORK_DIR, \"data\", \"hf_*.txt\"))\n",
                "if len(hf_files) >= 1:\n",
                "    total_mb = sum(os.path.getsize(f) for f in hf_files) / 1024 / 1024\n",
                "    print(f\"ü§ó HuggingFace: —É–∂–µ –µ—Å—Ç—å ({len(hf_files)} —Ñ–∞–π–ª–æ–≤, {total_mb:.0f} MB)\")\n",
                "else:\n",
                "    print(\"ü§ó –°–∫–∞—á–∏–≤–∞–Ω–∏–µ HuggingFace –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
                "    !python training/download_hf_dataset.py --preset all\n",
                "\n",
                "# –ò—Ç–æ–≥–æ\n",
                "data_files = [f for f in glob.glob(os.path.join(WORK_DIR, \"data\", \"*\")) if os.path.isfile(f)]\n",
                "total = sum(os.path.getsize(f) for f in data_files)\n",
                "print(f\"\\nüìä –ò—Ç–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö: {total / 1024 / 1024:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. –û–±—É—á–µ–Ω–∏–µ ‚Äî –§–∞–∑–∞ 2: –†–µ—Ñ–ª–µ–∫—Å—ã (MinGRU Classifier)\n",
                "~1-2 –º–∏–Ω –Ω–∞ GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"  –§–∞–∑–∞ 2: –†–µ—Ñ–ª–µ–∫—Å—ã (MinGRU Classifier)\")\n",
                "print(\"=\" * 60)\n",
                "!python training/train_reflex.py --epochs 100 --lr 0.002"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. –û–±—É—á–µ–Ω–∏–µ ‚Äî –§–∞–∑–∞ 3: MinGRU Language Model (System 1)\n",
                "~15 –º–∏–Ω –Ω–∞ T4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"  –§–∞–∑–∞ 3: MinGRU Language Model (System 1)\")\n",
                "print(\"=\" * 60)\n",
                "!python training/train_mingru.py \\\n",
                "    --epochs 25 \\\n",
                "    --lr 3e-3 \\\n",
                "    --dim 512 \\\n",
                "    --layers 6 \\\n",
                "    --batch 32 \\\n",
                "    --seq_len 256 \\\n",
                "    --augment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. –û–±—É—á–µ–Ω–∏–µ ‚Äî –§–∞–∑–∞ 4: Mamba-2 Brain (–æ—Å–Ω–æ–≤–Ω–æ–µ)\n",
                "\n",
                "**–≠—Ç–æ —Å–∞–º–∞—è –¥–æ–ª–≥–∞—è —Ñ–∞–∑–∞ (~2-4 —á–∞—Å–∞ –Ω–∞ T4)**\n",
                "\n",
                "12 —Å–ª–æ—ë–≤ √ó 768d, 4 –ø–æ–¥-—Ñ–∞–∑—ã:\n",
                "- Phase 1: Full pretrain (5 —ç–ø–æ—Ö)\n",
                "- Phase 2: Fine-tune WKV + Fusion (3 —ç–ø–æ—Ö–∏)\n",
                "- Phase 3: Fine-tune MoLE + MatrixPool (2 —ç–ø–æ—Ö–∏)\n",
                "- Phase 4: Fine-tune WKV + RAG + Memory (2 —ç–ø–æ—Ö–∏)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "vram_gb = torch.cuda.get_device_properties(0).total_mem / 1024**3 if torch.cuda.is_available() else 0\n",
                "\n",
                "# T4 = 16GB ‚Üí batch 16, accum 4\n",
                "if vram_gb >= 40:\n",
                "    BATCH, ACCUM = \"32\", \"2\"\n",
                "elif vram_gb >= 16:\n",
                "    BATCH, ACCUM = \"16\", \"4\"\n",
                "elif vram_gb >= 8:\n",
                "    BATCH, ACCUM = \"8\", \"8\"\n",
                "else:\n",
                "    BATCH, ACCUM = \"4\", \"4\"\n",
                "\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)} ({vram_gb:.0f} GB)\")\n",
                "print(f\"Batch: {BATCH} √ó {ACCUM} = {int(BATCH) * int(ACCUM)} effective\")\n",
                "\n",
                "# Transfer embedding from MinGRU\n",
                "emb_args = \"\"\n",
                "mingru_path = os.path.join(WORK_DIR, \"models\", \"mingru_weights.pt\")\n",
                "if os.path.exists(mingru_path):\n",
                "    cp = torch.load(mingru_path, map_location='cpu', weights_only=False)\n",
                "    state = cp.get('model_state_dict', cp)\n",
                "    for k in state:\n",
                "        if 'shared_embedding' in k or 'emb.weight' in k:\n",
                "            emb_dir = os.path.join(WORK_DIR, \"models\", \"tars_v3\")\n",
                "            os.makedirs(emb_dir, exist_ok=True)\n",
                "            emb_path = os.path.join(emb_dir, \"_transfer_embedding.pt\")\n",
                "            torch.save(state[k], emb_path)\n",
                "            emb_args = f\"--pretrained_emb {emb_path}\"\n",
                "            print(f\"üîó Transfer embedding: {state[k].shape}\")\n",
                "            break\n",
                "\n",
                "BASE_ARGS = f\"--d_model 768 --n_layers 12 --vocab_size 256 --batch {BATCH} --accum_steps {ACCUM} --device {device} --curriculum --label_smoothing 0.1 {emb_args}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase 1/4: Full pretrain\n",
                "print(\"=\" * 60)\n",
                "print(\"  Phase 1/4: Full pretrain (SSD + WKV + Œ©-SSM + MoLE)\")\n",
                "print(\"=\" * 60)\n",
                "!python training/train_mamba2.py {BASE_ARGS} --epochs 5 --lr 3e-4 --phase 1 --seq_len 256"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase 2/4: Fine-tune WKV + Fusion\n",
                "print(\"=\" * 60)\n",
                "print(\"  Phase 2/4: Fine-tune WKV + Fusion (SSD frozen)\")\n",
                "print(\"=\" * 60)\n",
                "!python training/train_mamba2.py {BASE_ARGS} --epochs 3 --lr 1e-4 --phase 2 --seq_len 512 --resume"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase 3/4: Fine-tune MoLE + MatrixPool\n",
                "print(\"=\" * 60)\n",
                "print(\"  Phase 3/4: Fine-tune MoLE + MatrixPool\")\n",
                "print(\"=\" * 60)\n",
                "!python training/train_mamba2.py {BASE_ARGS} --epochs 2 --lr 3e-5 --phase 3 --seq_len 512 --resume"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase 4/4: Fine-tune WKV + RAG + Memory\n",
                "print(\"=\" * 60)\n",
                "print(\"  Phase 4/4: Fine-tune WKV + RAG + Memory\")\n",
                "print(\"=\" * 60)\n",
                "!python training/train_mamba2.py {BASE_ARGS} --epochs 2 --lr 1.5e-5 --phase 4 --seq_len 512 --resume"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è BitNet 1.58-bit\n",
                "~15 –º–∏–Ω –Ω–∞ T4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fp16_path = os.path.join(WORK_DIR, \"models\", \"mamba2\", \"mamba2_omega.pt\")\n",
                "\n",
                "if os.path.exists(fp16_path):\n",
                "    print(\"=\" * 60)\n",
                "    print(\"  –§–∞–∑–∞ 5: –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è BitNet 1.58-bit\")\n",
                "    print(\"=\" * 60)\n",
                "    !python training/train_mamba2.py \\\n",
                "        --d_model 768 --n_layers 12 \\\n",
                "        --batch 16 --accum_steps 4 \\\n",
                "        --epochs 3 --lr 5e-5 \\\n",
                "        --phase 1 --quant \\\n",
                "        --resume --device cuda \\\n",
                "        --seq_len 256 --label_smoothing 0.1\n",
                "else:\n",
                "    print(\"‚ö† FP16 –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import sys\n",
                "sys.path.insert(0, WORK_DIR)\n",
                "\n",
                "from brain.tokenizer import TarsTokenizer\n",
                "from brain.mamba2.model import TarsMamba2LM\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "tokenizer = TarsTokenizer()\n",
                "model, ckpt = TarsMamba2LM.load_pretrained(device=device)\n",
                "model.eval()\n",
                "\n",
                "if ckpt is None:\n",
                "    print(\"‚ö† Trained weights not found\")\n",
                "else:\n",
                "    params = sum(p.numel() for p in model.parameters())\n",
                "    print(f\"‚úÖ Model: {ckpt}\")\n",
                "    print(f\"‚úÖ Parameters: {params:,}\")\n",
                "    print()\n",
                "    \n",
                "    for prompt in [\"–ø—Ä–∏–≤–µ—Ç\", \"–∫–∞–∫ –¥–µ–ª–∞\", \"—á—Ç–æ —Ç–∞–∫–æ–µ\", \"—Ä–∞—Å—Å–∫–∞–∂–∏\"]:\n",
                "        tokens = tokenizer.encode(prompt)\n",
                "        input_ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
                "        with torch.no_grad():\n",
                "            logits = model(input_ids)\n",
                "        probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
                "        top5 = torch.topk(probs, 5)\n",
                "        preds = []\n",
                "        for idx, prob in zip(top5.indices.tolist(), top5.values.tolist()):\n",
                "            char = tokenizer.decode([idx])\n",
                "            preds.append(f\"'{char}'({prob:.2%})\")\n",
                "        print(f\"  '{prompt}' ‚Üí {', '.join(preds)}\")\n",
                "    \n",
                "    print(\"\\n‚úÖ Model works!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
                "\n",
                "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –∑–∞–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –≤ `/content/tars_v3_models.tar.gz`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "import json\n",
                "\n",
                "OUTPUT_DIR = \"/content/tars_v3_output\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# –§–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π\n",
                "model_files = {\n",
                "    \"mamba2.pt\":        os.path.join(WORK_DIR, \"models\", \"mamba2\", \"mamba2_omega.pt\"),\n",
                "    \"mamba2_158bit.pt\": os.path.join(WORK_DIR, \"models\", \"mamba2\", \"mamba2_omega_158bit.pt\"),\n",
                "    \"mingru.pt\":        os.path.join(WORK_DIR, \"models\", \"mingru_weights.pt\"),\n",
                "    \"reflex.pt\":        os.path.join(WORK_DIR, \"models\", \"reflex\", \"reflex_classifier.pt\"),\n",
                "}\n",
                "\n",
                "print(\"üì¶ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:\")\n",
                "for name, src in model_files.items():\n",
                "    if os.path.exists(src):\n",
                "        dst = os.path.join(OUTPUT_DIR, name)\n",
                "        shutil.copy2(src, dst)\n",
                "        size_mb = os.path.getsize(dst) / 1024 / 1024\n",
                "        print(f\"  ‚úÖ {name} ({size_mb:.1f} MB)\")\n",
                "    else:\n",
                "        print(f\"  ‚è≠ {name} ‚Äî –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
                "\n",
                "# Config\n",
                "config = {\n",
                "    \"models\": {\n",
                "        \"mamba2\": {\n",
                "            \"params\": {\n",
                "                \"encoding\": \"cp1251\",\n",
                "                \"vocab_size\": 256,\n",
                "                \"d_model\": 768,\n",
                "                \"n_layers\": 12,\n",
                "                \"n_experts\": 8,\n",
                "                \"omega_dim\": 32,\n",
                "                \"pool_size\": 48\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "}\n",
                "with open(os.path.join(OUTPUT_DIR, \"config.json\"), \"w\") as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "# –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º\n",
                "archive = \"/content/tars_v3_models\"\n",
                "shutil.make_archive(archive, 'gztar', OUTPUT_DIR)\n",
                "archive_path = archive + \".tar.gz\"\n",
                "size_mb = os.path.getsize(archive_path) / 1024 / 1024\n",
                "print(f\"\\nüìÅ –ê—Ä—Ö–∏–≤: {archive_path} ({size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –°–∫–∞—á–∞—Ç—å –∞—Ä—Ö–∏–≤ —Å –º–æ–¥–µ–ª—è–º–∏\n",
                "from google.colab import files\n",
                "files.download(\"/content/tars_v3_models.tar.gz\")\n",
                "print(\"\\nüéØ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–∞—á–∞—Ç–æ!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîÑ (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ Google Drive\n",
                "\n",
                "–ï—Å–ª–∏ —Å–µ—Å—Å–∏—è –º–æ–∂–µ—Ç –æ—Ç–∫–ª—é—á–∏—Ç—å—Å—è ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –µ—Å–ª–∏ —Ö–æ—á–µ—à—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞ Google Drive\n",
                "\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "#\n",
                "# import shutil\n",
                "# drive_dir = \"/content/drive/MyDrive/TARS_models\"\n",
                "# os.makedirs(drive_dir, exist_ok=True)\n",
                "#\n",
                "# for name, src in model_files.items():\n",
                "#     if os.path.exists(os.path.join(OUTPUT_DIR, name)):\n",
                "#         shutil.copy2(os.path.join(OUTPUT_DIR, name), os.path.join(drive_dir, name))\n",
                "#         print(f\"  üíæ {name} ‚Üí Google Drive\")\n",
                "#\n",
                "# print(\"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–∞ Google Drive!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "### ‚è± –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ —Ñ–∞–∑–∞–º (T4 16GB):\n",
                "\n",
                "| –§–∞–∑–∞ | –û–ø–∏—Å–∞–Ω–∏–µ | –í—Ä–µ–º—è |\n",
                "|------|----------|-------|\n",
                "| 0 | –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π | ~2 –º–∏–Ω |\n",
                "| 1 | –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö | ~10 –º–∏–Ω |\n",
                "| 2 | –†–µ—Ñ–ª–µ–∫—Å—ã | ~2 –º–∏–Ω |\n",
                "| 3 | MinGRU LM | ~15 –º–∏–Ω |\n",
                "| 4 | Mamba-2 (4 –ø–æ–¥-—Ñ–∞–∑—ã) | ~2-4 —á–∞—Å–∞ |\n",
                "| 5 | –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è | ~15 –º–∏–Ω |\n",
                "| 7 | –í–∞–ª–∏–¥–∞—Ü–∏—è | ~1 –º–∏–Ω |\n",
                "| **–ò—Ç–æ–≥–æ** | | **~3-5 —á–∞—Å–æ–≤** |"
            ]
        }
    ]
}