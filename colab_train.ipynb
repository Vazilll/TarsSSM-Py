{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü§ñ TARS v3 ‚Äî Google Colab Full Training Pipeline\n",
                "\n",
                "**–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¢–ê–†–° v3: –º–æ–∑–≥ + –≥–æ–ª–æ—Å (STT + TTS)**\n",
                "\n",
                "### –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è:\n",
                "| # | –ú–æ–¥—É–ª—å | –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã | –í—Ä–µ–º—è (T4) |\n",
                "|---|--------|------------|------------|\n",
                "| 2 | –†–µ—Ñ–ª–µ–∫—Å—ã | MinGRU Classifier | ~2 –º–∏–Ω |\n",
                "| 3 | MinGRU LM | System 1 (–±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è) | ~30 –º–∏–Ω |\n",
                "| 4 | Mamba-2 Brain | SSD + WKV + Œ©-SSM + MoLE + WaveConsolidation | ~3-5 —á |\n",
                "| 5 | –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è | BitNet 1.58-bit | ~15 –º–∏–Ω |\n",
                "| 7 | –í–∞–ª–∏–¥–∞—Ü–∏—è | –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è | ~5 –º–∏–Ω |\n",
                "| 8 | Whisper STT | LoRA fine-tune (—Ä—É—Å—Å–∫–∏–π) | ~30 –º–∏–Ω |\n",
                "| 9 | Piper TTS | VITS fine-tune (—Ä—É—Å—Å–∫–∏–π –≥–æ–ª–æ—Å) | ~1-2 —á |\n",
                "| 10 | Voice INT8 | –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è ONNX + Whisper Boost | ~5 –º–∏–Ω |\n",
                "\n",
                "### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n",
                "1. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
                "2. –ó–∞–ø—É—Å–∫–∞–π —è—á–µ–π–∫–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É ‚ñ∂\n",
                "3. –ú–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –Ω–∞ **Google Drive**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU + –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import os\n",
                "import time\n",
                "\n",
                "print(\"=\" * 65)\n",
                "print(\"  TARS v3 ‚Äî Google Colab Full Training Pipeline\")\n",
                "print(\"=\" * 65)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    GPU_NAME = torch.cuda.get_device_name(0)\n",
                "    VRAM_GB = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
                "    print(f\"  ‚úÖ GPU:  {GPU_NAME}\")\n",
                "    print(f\"  ‚úÖ VRAM: {VRAM_GB:.1f} GB\")\n",
                "    print(f\"  ‚úÖ CUDA: {torch.version.cuda}\")\n",
                "else:\n",
                "    print(\"  ‚ùå GPU –ù–ï –û–ë–ù–ê–†–£–ñ–ï–ù!\")\n",
                "    print(\"  ‚Üí Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\")\n",
                "    raise RuntimeError(\"GPU not available\")\n",
                "\n",
                "# –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ GPU\n",
                "if VRAM_GB >= 35:     # A100\n",
                "    BATCH, ACCUM = 32, 2\n",
                "    WHISPER_BATCH = 32\n",
                "    PIPER_BATCH = 32\n",
                "    GPU_TIER = \"A100\"\n",
                "elif VRAM_GB >= 14:   # T4 / P100\n",
                "    BATCH, ACCUM = 16, 4\n",
                "    WHISPER_BATCH = 16\n",
                "    PIPER_BATCH = 16\n",
                "    GPU_TIER = \"T4\"\n",
                "else:                 # K80 / free\n",
                "    BATCH, ACCUM = 8, 8\n",
                "    WHISPER_BATCH = 8\n",
                "    PIPER_BATCH = 8\n",
                "    GPU_TIER = \"Budget\"\n",
                "\n",
                "print(f\"  ‚ö° Tier: {GPU_TIER} (batch={BATCH}√ó{ACCUM}={BATCH*ACCUM})\")\n",
                "\n",
                "# Google Drive ‚Äî –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤\n",
                "SAVE_TO_DRIVE = True\n",
                "DRIVE_DIR = \"/content/drive/MyDrive/TARS_v3_models\"\n",
                "\n",
                "if SAVE_TO_DRIVE:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
                "    print(f\"  üíæ Google Drive: {DRIVE_DIR}\")\n",
                "\n",
                "print(\"=\" * 65)\n",
                "\n",
                "T_START = time.time()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è + –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, sys, shutil\n",
                "\n",
                "REPO_URL = \"https://github.com/Vazilll/TarsSSM-Py.git\"\n",
                "WORK_DIR = \"/content/TarsSSM-Py\"\n",
                "\n",
                "if os.path.exists(os.path.join(WORK_DIR, \"mega_train.py\")):\n",
                "    print(\"‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —É–∂–µ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω\")\n",
                "    # –û–±–Ω–æ–≤–ª—è–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏\n",
                "    !cd {WORK_DIR} && git pull --ff-only 2>/dev/null || true\n",
                "else:\n",
                "    print(f\"üì• –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {REPO_URL}...\")\n",
                "    !git clone {REPO_URL} {WORK_DIR}\n",
                "\n",
                "os.chdir(WORK_DIR)\n",
                "sys.path.insert(0, WORK_DIR)\n",
                "\n",
                "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (torch —É–∂–µ –µ—Å—Ç—å –≤ Colab)\n",
                "!pip install -q einops tqdm psutil sentencepiece tokenizers \\\n",
                "    sentence-transformers datasets transformers peft jiwer \\\n",
                "    faster-whisper onnxruntime sounddevice\n",
                "\n",
                "print(f\"\\nüìÇ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}\")\n",
                "print(\"‚úÖ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã\")\n",
                "\n",
                "# –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞ Drive\n",
                "def save_to_drive(src_path, name=None):\n",
                "    \"\"\"–ö–æ–ø–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –Ω–∞ Google Drive –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.\"\"\"\n",
                "    if not SAVE_TO_DRIVE or not os.path.exists(src_path):\n",
                "        return\n",
                "    dst = os.path.join(DRIVE_DIR, name or os.path.basename(src_path))\n",
                "    shutil.copy2(src_path, dst)\n",
                "    size_mb = os.path.getsize(dst) / 1024 / 1024\n",
                "    print(f\"  üíæ ‚Üí Drive: {name or os.path.basename(src_path)} ({size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "os.makedirs(\"data\", exist_ok=True)\n",
                "\n",
                "# Wikipedia (50–∫ —Å—Ç–∞—Ç–µ–π ‚Äî –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö = –ª—É—á—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç)\n",
                "wiki_path = os.path.join(WORK_DIR, \"data\", \"wiki_ru.txt\")\n",
                "if os.path.exists(wiki_path) and os.path.getsize(wiki_path) > 100_000:\n",
                "    size_mb = os.path.getsize(wiki_path) / 1024 / 1024\n",
                "    print(f\"üìö Wikipedia: —É–∂–µ –µ—Å—Ç—å ({size_mb:.1f} MB)\")\n",
                "else:\n",
                "    print(\"üìö –°–∫–∞—á–∏–≤–∞–Ω–∏–µ Wikipedia (50 000 —Å—Ç–∞—Ç–µ–π)...\")\n",
                "    !python training/download_wiki.py --count 50000\n",
                "\n",
                "# HuggingFace datasets (–∫–æ–¥ + —á–∞—Ç + –∞–≥–µ–Ω—Ç—ã)\n",
                "hf_files = glob.glob(os.path.join(WORK_DIR, \"data\", \"hf_*.txt\"))\n",
                "if len(hf_files) >= 1:\n",
                "    total_mb = sum(os.path.getsize(f) for f in hf_files) / 1024 / 1024\n",
                "    print(f\"ü§ó HuggingFace: —É–∂–µ –µ—Å—Ç—å ({len(hf_files)} —Ñ–∞–π–ª–æ–≤, {total_mb:.0f} MB)\")\n",
                "else:\n",
                "    print(\"ü§ó –°–∫–∞—á–∏–≤–∞–Ω–∏–µ HuggingFace –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...\")\n",
                "    !python training/download_hf_dataset.py --preset all\n",
                "\n",
                "# LEANN embedding model\n",
                "print(\"üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (LEANN)...\")\n",
                "try:\n",
                "    emb_path = os.path.join(WORK_DIR, \"models\", \"embeddings\")\n",
                "    if not os.path.exists(emb_path):\n",
                "        from sentence_transformers import SentenceTransformer\n",
                "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "        model.save(emb_path)\n",
                "        print(f\"  ‚úÖ Embedding model saved: {emb_path}\")\n",
                "    else:\n",
                "        print(f\"  ‚úÖ Embedding model already exists\")\n",
                "except Exception as e:\n",
                "    print(f\"  ‚ö† Embeddings: {e}\")\n",
                "\n",
                "# LEANN ingest\n",
                "print(\"üß† –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ LEANN...\")\n",
                "try:\n",
                "    sys.path.insert(0, os.path.join(WORK_DIR, \"training\"))\n",
                "    from ingest_to_leann import ingest_all\n",
                "    ingest_all()\n",
                "    print(\"  ‚úÖ LEANN –∑–∞–ø–æ–ª–Ω–µ–Ω–∞\")\n",
                "except Exception as e:\n",
                "    print(f\"  ‚Ñπ LEANN: {e} (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ)\")\n",
                "\n",
                "# –ò—Ç–æ–≥–æ\n",
                "data_files = [f for f in glob.glob(os.path.join(WORK_DIR, \"data\", \"*\")) if os.path.isfile(f)]\n",
                "total = sum(os.path.getsize(f) for f in data_files)\n",
                "print(f\"\\nüìä –ò—Ç–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö: {total / 1024 / 1024:.0f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üß† –ú–û–ó–ì ‚Äî –§–∞–∑—ã 2-7\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –§–∞–∑–∞ 2: –†–µ—Ñ–ª–µ–∫—Å—ã (MinGRU Classifier)\n",
                "–ë—ã—Å—Ç—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: `trivial / simple / complex` ‚Üí –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥–ª—É–±–∏–Ω—É –æ–±—Ä–∞–±–æ—Ç–∫–∏.\n",
                "\n",
                "~2 –º–∏–Ω –Ω–∞ GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 65)\n",
                "print(\"  –§–∞–∑–∞ 2: –†–µ—Ñ–ª–µ–∫—Å—ã (MinGRU Classifier)\")\n",
                "print(\"=\" * 65)\n",
                "!python training/train_reflex.py --epochs 100 --lr 0.002\n",
                "save_to_drive(\"models/reflex/reflex_classifier.pt\", \"reflex.pt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –§–∞–∑–∞ 3: MinGRU Language Model (System 1)\n",
                "–ë—ã—Å—Ç—Ä—ã–π —è–∑—ã–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä ‚Äî 6 —Å–ª–æ—ë–≤ √ó 512d.\n",
                "\n",
                "–û–±—É—á–∞–µ–º—ã–µ –º–æ–¥—É–ª–∏: **MinGRU cells, embedding, LM head, Œ©-SSM context**\n",
                "\n",
                "~30 –º–∏–Ω –Ω–∞ T4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 65)\n",
                "print(\"  –§–∞–∑–∞ 3: MinGRU Language Model (System 1)\")\n",
                "print(\"=\" * 65)\n",
                "!python training/train_mingru.py \\\n",
                "    --epochs 50 \\\n",
                "    --lr 3e-3 \\\n",
                "    --dim 512 \\\n",
                "    --layers 6 \\\n",
                "    --batch 32 \\\n",
                "    --seq_len 256 \\\n",
                "    --augment\n",
                "save_to_drive(\"models/mingru_weights.pt\", \"mingru.pt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –§–∞–∑–∞ 4: Mamba-2 Brain (–æ—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)\n",
                "\n",
                "**12 —Å–ª–æ—ë–≤ √ó 768d ‚Äî –ø–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢–ê–†–° v3**\n",
                "\n",
                "–û–±—É—á–∞–µ–º—ã–µ –º–æ–¥—É–ª–∏ –ø–æ —Ñ–∞–∑–∞–º:\n",
                "- **Phase 1**: –í–°–Å ‚Äî SSD, WKV, Œ©-SSM, MoLE, WaveConsolidation, MatrixPool, embedding\n",
                "- **Phase 2**: WKV + Fusion + Œ©-SSM + MoLE + NoveltyGate (SSD frozen)\n",
                "- **Phase 3**: MoLE routing + MatrixPool + WaveMerge + WaveGate + NoveltyGate\n",
                "- **Phase 4**: WKV + RAG + Memory injection + Œ©-SSM + to/from_memory_space\n",
                "\n",
                "~3-5 —á–∞—Å–æ–≤ –Ω–∞ T4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch, os\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)} ({VRAM_GB:.0f} GB)\")\n",
                "print(f\"Batch: {BATCH} √ó {ACCUM} = {BATCH * ACCUM} effective\")\n",
                "\n",
                "# Transfer embedding MinGRU ‚Üí Mamba-2\n",
                "emb_args = \"\"\n",
                "mingru_path = os.path.join(WORK_DIR, \"models\", \"mingru_weights.pt\")\n",
                "if os.path.exists(mingru_path):\n",
                "    cp = torch.load(mingru_path, map_location='cpu', weights_only=False)\n",
                "    state = cp.get('model_state_dict', cp)\n",
                "    for k in state:\n",
                "        if 'shared_embedding' in k or 'emb.weight' in k:\n",
                "            emb_dir = os.path.join(WORK_DIR, \"models\", \"tars_v3\")\n",
                "            os.makedirs(emb_dir, exist_ok=True)\n",
                "            emb_path = os.path.join(emb_dir, \"_transfer_embedding.pt\")\n",
                "            torch.save(state[k], emb_path)\n",
                "            emb_args = f\"--pretrained_emb {emb_path}\"\n",
                "            print(f\"üîó Transfer embedding: {state[k].shape}\")\n",
                "            break\n",
                "\n",
                "BASE = f\"--d_model 768 --n_layers 12 --vocab_size 256 --batch {BATCH} --accum_steps {ACCUM} --device {device} --curriculum --label_smoothing 0.1 {emb_args}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase 1/4: Full pretrain ‚Äî –í–°–ï –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
                "print(\"=\" * 65)\n",
                "print(\"  Phase 1/4: Full pretrain (SSD + WKV + Œ©-SSM + MoLE + WaveConsolidation)\")\n",
                "print(\"=\" * 65)\n",
                "!python training/train_mamba2.py {BASE} --epochs 8 --lr 3e-4 --phase 1 --seq_len 256\n",
                "save_to_drive(\"models/mamba2/mamba2_omega.pt\", \"mamba2_phase1.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase 2/4: Fine-tune WKV + Fusion (SSD frozen)\n",
                "print(\"=\" * 65)\n",
                "print(\"  Phase 2/4: Fine-tune WKV + Fusion + Œ©-SSM + NoveltyGate\")\n",
                "print(\"=\" * 65)\n",
                "!python training/train_mamba2.py {BASE} --epochs 5 --lr 1e-4 --phase 2 --seq_len 512 --resume\n",
                "save_to_drive(\"models/mamba2/mamba2_omega.pt\", \"mamba2_phase2.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase 3/4: Fine-tune MoLE + MatrixPool + WaveConsolidation + NoveltyGate\n",
                "print(\"=\" * 65)\n",
                "print(\"  Phase 3/4: Fine-tune MoLE + MatrixPool + WaveMerge + NoveltyGate\")\n",
                "print(\"=\" * 65)\n",
                "!python training/train_mamba2.py {BASE} --epochs 3 --lr 3e-5 --phase 3 --seq_len 512 --resume\n",
                "save_to_drive(\"models/mamba2/mamba2_omega.pt\", \"mamba2_phase3.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Phase 4/4: Fine-tune WKV + RAG + Memory Injection + Œ©-SSM\n",
                "print(\"=\" * 65)\n",
                "print(\"  Phase 4/4: Fine-tune WKV + RAG + Memory + to/from_memory_space\")\n",
                "print(\"=\" * 65)\n",
                "!python training/train_mamba2.py {BASE} --epochs 3 --lr 1.5e-5 --phase 4 --seq_len 512 --resume\n",
                "save_to_drive(\"models/mamba2/mamba2_omega.pt\", \"mamba2_final.pt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –§–∞–∑–∞ 5: –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è BitNet 1.58-bit\n",
                "FP16 ‚Üí 1.58-bit (260 MB ‚Üí ~60 MB) —Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ STE.\n",
                "\n",
                "~15 –º–∏–Ω"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fp16_path = os.path.join(WORK_DIR, \"models\", \"mamba2\", \"mamba2_omega.pt\")\n",
                "\n",
                "if os.path.exists(fp16_path):\n",
                "    print(\"=\" * 65)\n",
                "    print(\"  –§–∞–∑–∞ 5: –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è BitNet 1.58-bit\")\n",
                "    print(\"=\" * 65)\n",
                "    !python training/train_mamba2.py \\\n",
                "        --d_model 768 --n_layers 12 \\\n",
                "        --batch {BATCH} --accum_steps {ACCUM} \\\n",
                "        --epochs 3 --lr 5e-5 \\\n",
                "        --phase 1 --quant \\\n",
                "        --resume --device cuda \\\n",
                "        --seq_len 256 --label_smoothing 0.1\n",
                "    save_to_drive(\"models/mamba2/mamba2_omega_158bit.pt\", \"mamba2_158bit.pt\")\n",
                "else:\n",
                "    print(\"‚ö† FP16 –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –§–∞–∑–∞ 7: –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
                "–¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ‚Äî –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–∑–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch, sys\n",
                "sys.path.insert(0, WORK_DIR)\n",
                "\n",
                "print(\"=\" * 65)\n",
                "print(\"  –§–∞–∑–∞ 7: –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–∑–≥–∞\")\n",
                "print(\"=\" * 65)\n",
                "\n",
                "try:\n",
                "    from brain.tokenizer import TarsTokenizer\n",
                "    from brain.mamba2.model import TarsMamba2LM\n",
                "\n",
                "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    tokenizer = TarsTokenizer()\n",
                "    model, ckpt = TarsMamba2LM.load_pretrained(device=device)\n",
                "    model.eval()\n",
                "\n",
                "    if ckpt is None:\n",
                "        print(\"‚ö† Trained weights not found\")\n",
                "    else:\n",
                "        params = sum(p.numel() for p in model.parameters())\n",
                "        print(f\"‚úÖ Model: {ckpt}\")\n",
                "        print(f\"‚úÖ Parameters: {params:,}\")\n",
                "        print()\n",
                "\n",
                "        for prompt in [\"–ø—Ä–∏–≤–µ—Ç\", \"–∫–∞–∫ –¥–µ–ª–∞\", \"—á—Ç–æ —Ç–∞–∫–æ–µ\", \"—Ä–∞—Å—Å–∫–∞–∂–∏\", \"–ø–æ–º–æ–≥–∏\"]:\n",
                "            tokens = tokenizer.encode(prompt)\n",
                "            input_ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
                "            with torch.no_grad():\n",
                "                logits = model(input_ids)\n",
                "            probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
                "            top5 = torch.topk(probs, 5)\n",
                "            preds = []\n",
                "            for idx, prob in zip(top5.indices.tolist(), top5.values.tolist()):\n",
                "                char = tokenizer.decode([idx])\n",
                "                preds.append(f\"'{char}'({prob:.2%})\")\n",
                "            print(f\"  '{prompt}' ‚Üí {', '.join(preds)}\")\n",
                "\n",
                "        print(\"\\n‚úÖ Brain works!\")\n",
                "    del model\n",
                "    torch.cuda.empty_cache()\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Validation error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üé§ –ì–û–õ–û–° ‚Äî –§–∞–∑—ã 8-10\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –§–∞–∑–∞ 8: Whisper STT ‚Äî –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ (LoRA)\n",
                "\n",
                "–î–æ–æ–±—É—á–µ–Ω–∏–µ **Whisper Tiny** –Ω–∞ —Ä—É—Å—Å–∫–æ–º —á–µ—Ä–µ–∑ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã.\n",
                "\n",
                "–û–±—É—á–∞–µ–º—ã–µ –º–æ–¥—É–ª–∏: **q_proj, v_proj** (LoRA rank=32)\n",
                "\n",
                "–î–∞–Ω–Ω—ã–µ: **Common Voice Russian** (10K –ø—Ä–∏–º–µ—Ä–æ–≤)\n",
                "\n",
                "~30 –º–∏–Ω –Ω–∞ T4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 65)\n",
                "print(\"  –§–∞–∑–∞ 8: Whisper Tiny LoRA Fine-tune (Russian STT)\")\n",
                "print(\"=\" * 65)\n",
                "\n",
                "# –û—á–∏—Å—Ç–∫–∞ VRAM –ø–µ—Ä–µ–¥ —Ç—è–∂—ë–ª–æ–π —Ñ–∞–∑–æ–π\n",
                "import gc, torch\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "!python training/train_whisper.py \\\n",
                "    --device cuda \\\n",
                "    --samples 10000 \\\n",
                "    --val_samples 1000 \\\n",
                "    --epochs 5 \\\n",
                "    --batch {WHISPER_BATCH} \\\n",
                "    --lr 1e-3 \\\n",
                "    --lora_r 32\n",
                "\n",
                "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
                "whisper_dir = os.path.join(WORK_DIR, \"models\", \"voice\", \"whisper_ru_lora\")\n",
                "if os.path.exists(whisper_dir):\n",
                "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ Drive\n",
                "    drive_whisper = os.path.join(DRIVE_DIR, \"whisper_ru_lora\") if SAVE_TO_DRIVE else None\n",
                "    if drive_whisper:\n",
                "        import shutil\n",
                "        if os.path.exists(drive_whisper):\n",
                "            shutil.rmtree(drive_whisper)\n",
                "        shutil.copytree(whisper_dir, drive_whisper)\n",
                "        print(f\"  üíæ ‚Üí Drive: whisper_ru_lora/\")\n",
                "    print(\"‚úÖ Whisper STT –æ–±—É—á–µ–Ω!\")\n",
                "else:\n",
                "    print(\"‚ö† Whisper LoRA –Ω–µ –Ω–∞–π–¥–µ–Ω\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –§–∞–∑–∞ 9: Piper TTS ‚Äî –°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ (VITS)\n",
                "\n",
                "Fine-tune **Piper TTS** –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∫–æ—Ä–ø—É—Å–µ RUSLAN (31—á —Ä–µ—á–∏).\n",
                "\n",
                "–û–±—É—á–∞–µ–º—ã–µ –º–æ–¥—É–ª–∏: **VITS encoder + decoder + flow**\n",
                "\n",
                "–†–µ–∑—É–ª—å—Ç–∞—Ç: ONNX-–º–æ–¥–µ–ª—å —Ä—É—Å—Å–∫–æ–≥–æ –≥–æ–ª–æ—Å–∞.\n",
                "\n",
                "~1-2 —á–∞—Å–∞ –Ω–∞ T4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 65)\n",
                "print(\"  –§–∞–∑–∞ 9: Piper TTS Fine-tune (Russian Voice)\")\n",
                "print(\"=\" * 65)\n",
                "\n",
                "# –û—á–∏—Å—Ç–∫–∞ VRAM\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Piper\n",
                "!pip install -q piper-tts piper-phonemize 2>/dev/null || echo \"piper install skipped\"\n",
                "\n",
                "!python training/train_piper.py \\\n",
                "    --epochs 1000 \\\n",
                "    --max_samples 3000 \\\n",
                "    --batch {PIPER_BATCH}\n",
                "\n",
                "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ TTS –º–æ–¥–µ–ª–∏\n",
                "piper_onnx = os.path.join(WORK_DIR, \"models\", \"voice\", \"tars_voice_ru.onnx\")\n",
                "if os.path.exists(piper_onnx):\n",
                "    save_to_drive(piper_onnx, \"tars_voice_ru.onnx\")\n",
                "    print(\"‚úÖ Piper TTS –æ–±—É—á–µ–Ω!\")\n",
                "else:\n",
                "    print(\"‚ö† Piper ONNX –Ω–µ —Å–æ–∑–¥–∞–Ω (–≤–æ–∑–º–æ–∂–Ω–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–∫–∞—á–∞–ª–∏—Å—å)\")\n",
                "    print(\"  ‚Üí –≠—Ç–æ –ù–ï –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–∑—ã\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## –§–∞–∑–∞ 10: Whisper Boost + Voice INT8 –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è\n",
                "\n",
                "1. **Whisper Boost**: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ hotwords –∏–∑ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è STT\n",
                "2. **INT8 –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è**: Whisper + Piper ONNX ‚Üí —Å–∂–∞—Ç–∏–µ ~3x\n",
                "\n",
                "~5 –º–∏–Ω"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 65)\n",
                "print(\"  –§–∞–∑–∞ 10: Whisper Boost + Voice INT8 –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è\")\n",
                "print(\"=\" * 65)\n",
                "\n",
                "# Whisper Vocabulary Boost ‚Äî hotwords + initial_prompt\n",
                "print(\"\\nüìù Whisper Boost: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∫–æ—Ä–ø—É—Å–∞...\")\n",
                "!python training/whisper_boost.py\n",
                "\n",
                "whisper_ctx = os.path.join(WORK_DIR, \"models\", \"voice\", \"whisper_context.json\")\n",
                "if os.path.exists(whisper_ctx):\n",
                "    save_to_drive(whisper_ctx, \"whisper_context.json\")\n",
                "\n",
                "# INT8 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è ONNX (–µ—Å–ª–∏ –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å)\n",
                "print(\"\\nüîß INT8 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≥–æ–ª–æ—Å–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π...\")\n",
                "!python training/quantize_voice.py\n",
                "\n",
                "print(\"\\n‚úÖ –ì–æ–ª–æ—Å–æ–≤–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à—ë–Ω!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üì¶ –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ + –°–∫–∞—á–∏–≤–∞–Ω–∏–µ\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil, json, time\n",
                "\n",
                "OUTPUT_DIR = \"/content/tars_v3_output\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(\"=\" * 65)\n",
                "print(\"  –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ tars_v3\")\n",
                "print(\"=\" * 65)\n",
                "\n",
                "# –í—Å–µ –º–æ–¥–µ–ª–∏\n",
                "model_files = {\n",
                "    # –ú–æ–∑–≥\n",
                "    \"mamba2.pt\":        os.path.join(WORK_DIR, \"models\", \"mamba2\", \"mamba2_omega.pt\"),\n",
                "    \"mamba2_158bit.pt\": os.path.join(WORK_DIR, \"models\", \"mamba2\", \"mamba2_omega_158bit.pt\"),\n",
                "    \"mingru.pt\":        os.path.join(WORK_DIR, \"models\", \"mingru_weights.pt\"),\n",
                "    \"reflex.pt\":        os.path.join(WORK_DIR, \"models\", \"reflex\", \"reflex_classifier.pt\"),\n",
                "    # –ì–æ–ª–æ—Å\n",
                "    \"tars_voice_ru.onnx\": os.path.join(WORK_DIR, \"models\", \"voice\", \"tars_voice_ru.onnx\"),\n",
                "    \"whisper_context.json\": os.path.join(WORK_DIR, \"models\", \"voice\", \"whisper_context.json\"),\n",
                "}\n",
                "\n",
                "print(\"\\nüì¶ –ú–æ–¥–µ–ª–∏:\")\n",
                "saved_models = []\n",
                "for name, src in model_files.items():\n",
                "    if os.path.exists(src):\n",
                "        dst = os.path.join(OUTPUT_DIR, name)\n",
                "        shutil.copy2(src, dst)\n",
                "        size_mb = os.path.getsize(dst) / 1024 / 1024\n",
                "        print(f\"  ‚úÖ {name} ({size_mb:.1f} MB)\")\n",
                "        saved_models.append(name)\n",
                "    else:\n",
                "        print(f\"  ‚è≠ {name} ‚Äî –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
                "\n",
                "# Whisper LoRA (—Ü–µ–ª–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è)\n",
                "whisper_lora_src = os.path.join(WORK_DIR, \"models\", \"voice\", \"whisper_ru_lora\")\n",
                "whisper_lora_dst = os.path.join(OUTPUT_DIR, \"whisper_ru_lora\")\n",
                "if os.path.exists(whisper_lora_src):\n",
                "    if os.path.exists(whisper_lora_dst):\n",
                "        shutil.rmtree(whisper_lora_dst)\n",
                "    shutil.copytree(whisper_lora_src, whisper_lora_dst)\n",
                "    print(f\"  ‚úÖ whisper_ru_lora/ (LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã)\")\n",
                "    saved_models.append(\"whisper_ru_lora/\")\n",
                "\n",
                "# Config\n",
                "config = {\n",
                "    \"version\": \"3.0\",\n",
                "    \"models\": {\n",
                "        \"mamba2\": {\n",
                "            \"params\": {\n",
                "                \"encoding\": \"cp1251\",\n",
                "                \"vocab_size\": 256,\n",
                "                \"d_model\": 768,\n",
                "                \"n_layers\": 12,\n",
                "                \"n_experts\": 8,\n",
                "                \"omega_dim\": 32,\n",
                "                \"pool_size\": 48\n",
                "            }\n",
                "        },\n",
                "        \"mingru\": {\"dim\": 512, \"layers\": 6},\n",
                "        \"voice\": {\n",
                "            \"stt\": \"whisper-tiny + LoRA (Russian)\",\n",
                "            \"tts\": \"piper-vits (Russian)\",\n",
                "            \"boost\": \"whisper_context.json\"\n",
                "        }\n",
                "    },\n",
                "    \"trained_on\": \"Google Colab\",\n",
                "    \"gpu\": GPU_NAME,\n",
                "}\n",
                "with open(os.path.join(OUTPUT_DIR, \"config.json\"), \"w\") as f:\n",
                "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "# –ê—Ä—Ö–∏–≤\n",
                "archive_path = \"/content/tars_v3_full\"\n",
                "shutil.make_archive(archive_path, 'gztar', OUTPUT_DIR)\n",
                "archive_file = archive_path + \".tar.gz\"\n",
                "size_mb = os.path.getsize(archive_file) / 1024 / 1024\n",
                "\n",
                "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ Drive\n",
                "if SAVE_TO_DRIVE:\n",
                "    shutil.copy2(archive_file, os.path.join(DRIVE_DIR, \"tars_v3_full.tar.gz\"))\n",
                "    print(f\"\\nüíæ –ê—Ä—Ö–∏–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –Ω–∞ Google Drive!\")\n",
                "\n",
                "total_time = time.time() - T_START\n",
                "hours = total_time / 3600\n",
                "\n",
                "print(f\"\\n{'=' * 65}\")\n",
                "print(f\"  –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø\")\n",
                "print(f\"{'=' * 65}\")\n",
                "print(f\"  ‚è±  –í—Ä–µ–º—è: {hours:.1f} —á–∞—Å–æ–≤ ({total_time:.0f} —Å–µ–∫)\")\n",
                "print(f\"  üì¶ –ú–æ–¥–µ–ª–∏: {len(saved_models)} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\")\n",
                "for m in saved_models:\n",
                "    print(f\"     ‚úÖ {m}\")\n",
                "print(f\"  üìÅ –ê—Ä—Ö–∏–≤: {archive_file} ({size_mb:.1f} MB)\")\n",
                "print(f\"  üíæ Drive: {DRIVE_DIR}\")\n",
                "print(f\"{'=' * 65}\")\n",
                "print(f\"\\n  üéØ –¢–ê–†–° v3 –ü–û–õ–ù–û–°–¢–¨–Æ –û–ë–£–ß–ï–ù!\")\n",
                "print(f\"  üöÄ –°–∫–æ–ø–∏—Ä—É–π—Ç–µ models/ –≤ –ø—Ä–æ–µ–∫—Ç –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python launch_tars.py\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# –°–∫–∞—á–∞—Ç—å –∞—Ä—Ö–∏–≤ —Å –í–°–ï–ú–ò –º–æ–¥–µ–ª—è–º–∏ (–º–æ–∑–≥ + –≥–æ–ª–æ—Å)\n",
                "from google.colab import files\n",
                "files.download(\"/content/tars_v3_full.tar.gz\")\n",
                "print(\"\\nüì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–∞—á–∞—Ç–æ!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "### üìä –û–±—É—á–∞–µ–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¢–ê–†–° v3:\n",
                "\n",
                "| –ú–æ–¥—É–ª—å | –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã | –§–∞–∑–∞ |\n",
                "|--------|-----------|------|\n",
                "| **Embedding** | 256 √ó 768d | Phase 1, 3 |\n",
                "| **TarsCoreBlock** (√ó12) | SSD (Mamba-2) + WKV (RWKV-7) + WuNeng Fusion | Phase 1-4 |\n",
                "| **Œ©-SSM** (√ó12) | Cayley SO(n) rotation layer | Phase 1, 2, 4 |\n",
                "| **MoLE** (√ó12) | Top-2/8 sparse expert routing | Phase 1, 3 |\n",
                "| **NoveltyGate** (√ó12+1) | Threshold-based novelty detector | Phase 2, 3, 4 |\n",
                "| **WaveConsolidation** (√ó6) | Gate + Fusion MLP + Reflex integration | Phase 1, 3 |\n",
                "| **MatrixPool** | 48 domain embedding matrices | Phase 1, 3 |\n",
                "| **RAG injection** (√ó12) | Query/Out projections | Phase 4 |\n",
                "| **Memory injection** (√ó12) | mem_query, mem_proj, mem_gate | Phase 4 |\n",
                "| **Spine** | to/from_memory_space (768‚Üî384) | Phase 4 |\n",
                "| **Reflex Classifier** | MinGRU 3-class | Phase 2 |\n",
                "| **MinGRU LM** | 6√ó512d System 1 generator | Phase 3 |\n",
                "| **Whisper STT** | LoRA (q_proj, v_proj, rank=32) | Phase 8 |\n",
                "| **Piper TTS** | VITS encoder + decoder + flow | Phase 9 |\n",
                "| **Voice boost** | Hotwords + initial_prompt | Phase 10 |"
            ]
        }
    ]
}